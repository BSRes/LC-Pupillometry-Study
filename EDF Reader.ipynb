{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set these manually:\n",
    "subject_id = \"aaaa\"\n",
    "run_number = 1"
   ],
   "id": "8d9db5fa6c005385",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Requires the files to be formatted according to the f-strings below:\n",
    "filepath = f\"data/{subject_id}/{subject_id}S{run_number}.asc\"\n",
    "log_filepath = f\"data/{subject_id}/log{subject_id}S{run_number}.txt\"\n",
    "output_filepath = f\"./data/processed_data/{subject_id}S{run_number}.csv\""
   ],
   "id": "d84201660afc26da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = f.readlines()\n",
    "\n",
    "with open(log_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "    log_data = f.readlines()"
   ],
   "id": "35ccc3d4f5767b22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Separate out the sessions, starting at adaptation and ending when the recording ends for that session.\n",
    "# Sessions get stored in a dict called \"session_data\", with keys ranging from session_1 to session_4.\n",
    "keep_line = False\n",
    "last_line = False\n",
    "session_idx = 0\n",
    "session_data = {\n",
    "    \"session_1\": [],\n",
    "    \"session_2\": [],\n",
    "    \"session_3\": [],\n",
    "    \"session_4\": [],\n",
    "}\n",
    "\n",
    "for line in raw_data:\n",
    "    reformatted_line = line\n",
    "    if \"ADAPTATION_START\" in reformatted_line:\n",
    "        keep_line = True\n",
    "        session_idx += 1\n",
    "    elif \"END\" in reformatted_line[:3]:\n",
    "        keep_line = False\n",
    "        last_line = True\n",
    "\n",
    "    if keep_line or last_line:\n",
    "        session_data[f\"session_{session_idx}\"].append(reformatted_line)\n",
    "        last_line = False"
   ],
   "id": "b1f547fbbc63bab4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a dictionary that will hold all the data which is to be categorised into either the\n",
    "# session's adaptation period or to be placed in a list for each block within that session:\n",
    "extracted_data = {}\n",
    "\n",
    "# Create an individual dictionary for each session that has separate data per event/trial:\n",
    "for idx in range(4):\n",
    "    extracted_data[f\"session_{idx+1}\"] = {\n",
    "        \"adaptation_period\": [],\n",
    "        \"adaptation_period_overspill\": [],\n",
    "        \"raw_blocks\": {}\n",
    "    }\n",
    "\n",
    "\n",
    "adaptation_end_found = False\n",
    "current_block = 0\n",
    "\n",
    "for session in extracted_data.keys():\n",
    "    adaptation_end_found = False\n",
    "    current_block = 0\n",
    "\n",
    "    for idx, line in enumerate(session_data[session]):\n",
    "        # Add the adaptation period data to its own key:\n",
    "        if not adaptation_end_found:\n",
    "            extracted_data[session][\"adaptation_period\"].append(line)\n",
    "        if \"ADAPTATION_END\" in line:\n",
    "            adaptation_end_found = True\n",
    "\n",
    "        # If the adaptation period is over, we can start adding blocks:\n",
    "        if adaptation_end_found:\n",
    "            # Get blocks:\n",
    "            if \"BLOCK_START\" in line:\n",
    "                current_block = int((line.split(\"_\")[2]))\n",
    "\n",
    "            # Create the current block's key if it doesn't already exist:\n",
    "            if current_block not in extracted_data[session][\"raw_blocks\"]:\n",
    "                # Handle if the current block is still 0, meaning it's the 'overspill' of the adaptation period:\n",
    "                if current_block == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    # This makes sure the blocks are 1-indexed, so if we want to access the first block, we do so with extracted_data[session][1]\n",
    "                    extracted_data[session][\"raw_blocks\"][current_block] = []\n",
    "\n",
    "            # Add the line to the current block's list:\n",
    "            if current_block == 0:\n",
    "                extracted_data[session][\"adaptation_period_overspill\"].append(line)\n",
    "            else:\n",
    "                extracted_data[session][\"raw_blocks\"][current_block].append(line)"
   ],
   "id": "c5ae1cae1e1e272d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Explanation of data structure:\n",
    "We now have a dictionary called `extracted_data`.\n",
    "\n",
    "Within `extracted_data` are four more dictionaries called `session_1`, `session_2`, `session_3`, and `session_4`.\n",
    "\n",
    "Each of these 'session' dictionaries contain 3 keys: `adaptation_period`, `adaptation_period_overspill`, and `raw_blocks`.\n",
    "\n",
    "## Adaptation Period:\n",
    "This is a list containing each line from the ASCII EDF file, starting from the first line containing \"ADAPTATION_START\", and ending with the first following instance of \"ADAPTATION_END\" for that session, as received by the eye-tracker in its EDF file.\n",
    "\n",
    "## Adaptation Period Overspill:\n",
    "This is also a list of lines like in the 'Adaptation Period' list, however these are any lines that are between the same \"ADAPTATION_END\" and the first instance of \"BLOCK_START_ for that session, effectively meaning it's any data logged to the EDF but wasn't officially captured during the adaptation period for whatever reason. It's usually around 11ms worth of data.\n",
    "\n",
    "## Raw Blocks:\n",
    "This is a dictionary with keys starting from 1 and continuing up to the total number of blocks for that session. For each block, there is a list containing every line recorded during that block. The lines begin with the first instance of \"BLOCK_START\" for that block, and end with the very last line before the next instance of \"BLOCK_START\", therefore including every line of the EDF file for that particular block."
   ],
   "id": "6379b8f454fc1754"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for session in extracted_data.keys():\n",
    "\n",
    "    # Creating the trial dictionaries:\n",
    "    extracted_data[session][\"blocks\"] = {}\n",
    "\n",
    "    for block_idx in extracted_data[session][\"raw_blocks\"].keys():\n",
    "\n",
    "        extracted_data[session][\"blocks\"][block_idx] = {\"trials\": {}}\n",
    "\n",
    "        new_trial = False\n",
    "        curr_trial = 0\n",
    "\n",
    "        for line in extracted_data[session][\"raw_blocks\"][block_idx]:\n",
    "\n",
    "            # Determine whether to record the line in the dict or not:\n",
    "            if \"TRIALID_VAR\" in line:\n",
    "                curr_trial = int(line.split(\"TRIALID \")[1].split(\",\")[0].split(\"_\")[2])\n",
    "                new_trial = True\n",
    "            else:\n",
    "                new_trial = False\n",
    "\n",
    "            if new_trial:\n",
    "                extracted_data[session][\"blocks\"][block_idx][\"trials\"][curr_trial] = []\n",
    "\n",
    "            if curr_trial in extracted_data[session][\"blocks\"][block_idx][\"trials\"]:\n",
    "                extracted_data[session][\"blocks\"][block_idx][\"trials\"][curr_trial].append(line)"
   ],
   "id": "ff592fe6e105bef6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# New Data Structure\n",
    "Within the dictionary called `extracted_data`, there is now a new dictionary under the key `blocks`.\n",
    "\n",
    "`blocks` is like `raw_blocks`, having the same keys indexed starting at 1, and incrementing up until all blocks for a particular session have been included.\n",
    "\n",
    "Within a given block index under `blocks` is a new dictionary called `trials`, which itself is indexed starting at 1 and going up until all trials for that block are included. Within these indexed trials is a list containing all lines associated with that trial.\n",
    "\n",
    "Thus, to access all the lines associated with the second trial in block 15 for session 2, you would call `extracted_data[\"session_2\"][\"blocks\"][15][\"trials\"][2]`."
   ],
   "id": "7dc3e577792d7262"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Making a Pandas-Compatible Dictionary:\n",
    "We now have a human-friendly dictionary in which we can access the raw line data by specifying a session, block, and trial number. However, because this line data is raw in format, and its formatting changes depending on the type of message sent to the EDF during recording, we need to process it in a way such that every line has the same formatting and can then be filtered based on what we're looking for.\n",
    "\n",
    "To achieve the filtering, I'll use Pandas, which requires every line to have the same formatting anyway. Thus, I will iterate over the `extracted_data` dict and store the reformatted data lines in a list called `data_for_pd` which will be used to create the Pandas DataFrame:"
   ],
   "id": "66f87053eeb50607"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define a function that can take in a line and convert it to the dictionary format we need for the Pandas DataFrame:\n",
    "def line_to_dict(\n",
    "        entry, session_num, block_num, trial_num, session_con, session_common_ob,\n",
    "        trial_con=None, main_stim_visibility=None, main_stim_type=None, target_status=None, att_letter=None\n",
    "):\n",
    "    formatted_line = {\n",
    "        \"Subject\": subject_id,\n",
    "        \"Run\": run_number,                                  # Defines whether it was the 1st or 2nd run of the experiment (int - 1 or 2)\n",
    "        \"Timestamp\": None,\n",
    "        \"Message Type\": None,\n",
    "        \"Session\": session_num,                             # The session number\n",
    "        \"Session Condition\": session_con,                   # Either \"Attend\" or \"Divert\"\n",
    "        \"Session Common Oddball\": session_common_ob,        # Either \"fine gabor\" or \"noise disk\"\n",
    "        \"Block\": block_num,                                 # The block number\n",
    "        \"Trial\": trial_num,                                 # The trial number\n",
    "        \"Trial Condition\": trial_con,                       # Either \"Standard\" or \"Oddball\"\n",
    "        \"Main Stimulus Visibility\": main_stim_visibility,   # A bool - True iff the main stimulus was visible at that timestamp\n",
    "        \"Main Stimulus Type\": main_stim_type,               # A string describing which of the three possible main stimuli was shown\n",
    "        \"Target Status\": target_status,                     # A bool - True iff the target attention stimulus is showing.\n",
    "        \"Attention Letter\": att_letter,                     # The letter used (can be passed as None during attend conditions\n",
    "        \"Gaze X\": None,\n",
    "        \"Gaze Y\": None,\n",
    "        \"Pupil\": None,\n",
    "    }\n",
    "\n",
    "    split_line = entry.split(\"\\t\")\n",
    "\n",
    "    # Check the first character of the message to see if it's a number. If it is, it's a line of data. Otherwise, it's a message we sent to the eyetracker.\n",
    "    if not entry[0].isnumeric():\n",
    "        formatted_line[\"Message Type\"] = \"Message\"\n",
    "\n",
    "        # Fill out the dictionary depending on the different types of messages it may receive:\n",
    "        if \"TRIALID_VAR\" in entry:\n",
    "            formatted_line[\"Timestamp\"] = int(entry.split(\"\\t\")[1].split(\" \")[0])\n",
    "            formatted_line[\"Message Type\"] = \"Trial Start\"\n",
    "\n",
    "        elif \"TRIALID\" in entry:\n",
    "            formatted_line[\"Timestamp\"] = int(entry.split(\"\\t\")[1].split(\" \")[0])\n",
    "            formatted_line[\"Message Type\"] = \"Trial Start 2\"\n",
    "\n",
    "        elif \"MAIN_STIM_ONSET\" in entry:\n",
    "            formatted_line[\"Timestamp\"] = int(entry.split(\"\\t\")[1].split(\" \")[0])\n",
    "            formatted_line[\"Message Type\"] = \"Main Stimulus Onset\"\n",
    "\n",
    "        elif \"MAIN_STIM_OFFSET\" in entry:\n",
    "            formatted_line[\"Timestamp\"] = int(entry.split(\"\\t\")[1].split(\" \")[0])\n",
    "            formatted_line[\"Message Type\"] = \"Main Stimulus Offset\"\n",
    "\n",
    "        elif \"FIXATION_STIM_ONSET\" in entry:\n",
    "            formatted_line[\"Timestamp\"] = int(entry.split(\"\\t\")[1].split(\" \")[0])\n",
    "            formatted_line[\"Message Type\"] = \"Fixation Stimulus Onset\"\n",
    "\n",
    "        elif \"INTERIM_START\" in entry:\n",
    "            formatted_line[\"Timestamp\"] = int(entry.split(\"\\t\")[1].split(\" \")[0])\n",
    "            formatted_line[\"Message Type\"] = \"Interim Onset\"\n",
    "\n",
    "        elif \"INTERIM_END\" in entry:\n",
    "            formatted_line[\"Timestamp\"] = int(entry.split(\"\\t\")[1].split(\" \")[0])\n",
    "            formatted_line[\"Message Type\"] = \"Interim Offset\"\n",
    "\n",
    "        elif \"KEY_RESPONSE\" in entry:\n",
    "            formatted_line[\"Timestamp\"] = int(entry.split(\"\\t\")[1].split(\" \")[0])\n",
    "            formatted_line[\"Message Type\"] = \"Key Response\"\n",
    "\n",
    "        elif \"EFIX\" in entry:\n",
    "            formatted_line[\"Timestamp\"] = int(entry.split(\"\\t\")[1])\n",
    "            formatted_line[\"Message Type\"] = \"Fixation Interrupted\"\n",
    "\n",
    "        elif \"SFIX\" in entry:\n",
    "            formatted_line[\"Timestamp\"] = int(entry.split(\" \")[-1].strip())\n",
    "            formatted_line[\"Message Type\"] = \"Fixation Resumed\"\n",
    "\n",
    "        elif \"SSACC\" in entry:\n",
    "            formatted_line[\"Timestamp\"] = int(entry.split(\" \")[-1].strip())\n",
    "            formatted_line[\"Message Type\"] = \"Saccade Start\"\n",
    "\n",
    "        elif \"ESACC\" in entry:\n",
    "            formatted_line[\"Timestamp\"] = int(entry.split(\"\\t\")[1])\n",
    "            formatted_line[\"Message Type\"] = \"Saccade End\"\n",
    "\n",
    "        elif \"SBLINK\" in entry:\n",
    "            formatted_line[\"Timestamp\"] = int(entry.split(\" \")[2].strip()) - 1 # These messages are always ahead by 1\n",
    "            formatted_line[\"Message Type\"] = \"Blink Start\"\n",
    "\n",
    "        elif \"EBLINK\" in entry:\n",
    "            formatted_line[\"Timestamp\"] = int(entry.split(\"\\t\")[1].split(\" \")[0]) + 1 # These messages are always behind by 1\n",
    "            formatted_line[\"Message Type\"] = \"Blink End\"\n",
    "\n",
    "        elif \"END\" in entry:\n",
    "            formatted_line[\"Timestamp\"] = int(entry.split(\" \")[0].replace(\"END\", \"\").strip())\n",
    "            formatted_line[\"Message Type\"] = \"Session End\"\n",
    "\n",
    "        else:\n",
    "            print(f\"Unhandled message type:\\n{entry}\\n\")\n",
    "\n",
    "    # Fill out the dictionary assuming that the entry is a typical line of data:\n",
    "    else:\n",
    "        formatted_line[\"Message Type\"] = \"Data\"\n",
    "        formatted_line[\"Timestamp\"] = int(split_line[0])\n",
    "        if float(split_line[3].strip()) > 0: # Handles blinks\n",
    "            formatted_line[\"Gaze X\"] = split_line[1].strip()\n",
    "            formatted_line[\"Gaze Y\"] = split_line[2].strip()\n",
    "            formatted_line[\"Pupil\"] = split_line[3].strip()\n",
    "\n",
    "    return formatted_line\n",
    "\n"
   ],
   "id": "65a064d1715739be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Additional line processing:\n",
    "The code below handles the basic extraction of data from each line, such as the timestamp of the message, and the basic formatting. However, because each line of data doesn't necessarily contain information that we want to persist through each line (for instance, a typical line of data doesn't tell us whether the main stimulus condition is a standard or oddball for that trial), we need to process this outside of the function as we iterate over each line, then pass that information into the function along with the line."
   ],
   "id": "8e1934f7cc238883"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_for_pd = []\n",
    "target_is_on = False # Necessary for the trail processing in the 2nd loop of trial data (see below).\n",
    "curr_letter = None\n",
    "\n",
    "# This debug list is used to display the order in which the sessions, blocks, and trials were processed, useful for debugging.\n",
    "# The list is used instead of printing because Jupyter's buffer is overloaded by the rapid processing steps, causing printed debug messages to become jumbled.\n",
    "debug_log = []\n",
    "\n",
    "# Get the order of the behavioural conditions ('attend' or 'divert' conditions) from the log:\n",
    "for log_line in log_data:\n",
    "    if \"Session Condition Order:\" in log_line:\n",
    "        con_order = log_line.split(\"[\")[-1].replace(\"]\", \"\").replace(\"'\", \"\").strip().split(\", \")\n",
    "        break\n",
    "\n",
    "# Get the order of the common oddballs from the log:\n",
    "for log_line in log_data:\n",
    "    if \"Common Oddball Order:\" in log_line:\n",
    "        oddball_order = log_line.split(\"[\")[-1].replace(\"]\", \"\").replace(\"'\", \"\").strip().split(\", \")\n",
    "        break\n",
    "\n",
    "\n",
    "# Iterate over each session, and store the session number and the condition in a variable to pass on to the function:\n",
    "for session_name, session_condition, session_common_oddball in zip(extracted_data.keys(), con_order, oddball_order):\n",
    "    session = int(session_name[-1])\n",
    "\n",
    "    # DEBUG:\n",
    "    debug_log.append(f\"Session: {session}\")\n",
    "\n",
    "    for block in extracted_data[session_name][\"blocks\"]: # Iterate over the blocks, storing the block number to pass on.\n",
    "\n",
    "        # DEBUG:\n",
    "        debug_log.append(f\"Session: {session}\\tBlock: {block}\")\n",
    "\n",
    "        # This is the first loop through the trial data - it finds the information it needs to pass into the 2nd loop through the data:\n",
    "        for trial, trial_data in extracted_data[session_name][\"blocks\"][block][\"trials\"].items():\n",
    "\n",
    "            # DEBUG:\n",
    "            debug_log.append(f\"Session: {session}\\tBlock: {block}\\tTrial: {trial}\")\n",
    "\n",
    "            # Create a dict to store the relevant info, including onset and offset indexes for the relevant lines, and info about the trial:\n",
    "            trial_info = {\n",
    "                \"main_stim_onset_idx\": None,\n",
    "                \"main_stim_offset_idx\": None,\n",
    "                \"main_stim_type\": None,\n",
    "                \"fixation_stim_onset_idxs\": [],\n",
    "                \"fixation_stim_offset_idxs\": [],\n",
    "                \"fixation_types\": [],\n",
    "                \"fixation_letters\": [],\n",
    "                \"trial_condition\": None,\n",
    "            }\n",
    "\n",
    "            for data_line_idx, data_entry in enumerate(trial_data):\n",
    "                if \"MAIN_STIM_ONSET\" in data_entry:\n",
    "                    trial_info[\"main_stim_onset_idx\"] = data_line_idx\n",
    "                    trial_info[\"main_stim_type\"] = data_entry.split(\"Stim: \")[-1].strip()\n",
    "\n",
    "                elif \"MAIN_STIM_OFFSET\" in data_entry:\n",
    "                    trial_info[\"main_stim_offset_idx\"] = data_line_idx\n",
    "\n",
    "                elif \"FIXATION_STIM_ONSET\" in data_entry:\n",
    "                    if len(trial_info[\"fixation_stim_onset_idxs\"]) > 0: # If not the first fixation stim presentation, add an offset time for the prior one:\n",
    "                        trial_info[\"fixation_stim_offset_idxs\"].append(data_line_idx - 1)\n",
    "                    trial_info[\"fixation_stim_onset_idxs\"].append(data_line_idx)\n",
    "                    attention_type, attention_letter = data_entry.split(\"Attention_type: \")[-1].split(\", Letter: \")\n",
    "                    attention_letter = attention_letter.replace(\",FrameN: 0\", \"\").strip()\n",
    "                    trial_info[\"fixation_types\"].append(attention_type)\n",
    "                    trial_info[\"fixation_letters\"].append(attention_letter)\n",
    "\n",
    "                elif \"INTERIM_END\" in data_entry:\n",
    "                    trial_info[\"fixation_stim_offset_idxs\"].append(data_line_idx - 1)\n",
    "\n",
    "                elif \"TRIALID_VAR\" in data_entry:\n",
    "                    trial_condition_letter = data_entry.split(\"Type: \")[-1].strip()\n",
    "                    if trial_condition_letter == \"s\":\n",
    "                        trial_info[\"trial_condition\"] = \"Standard\"\n",
    "                    elif trial_condition_letter == \"o\":\n",
    "                        trial_info[\"trial_condition\"] = \"Oddball\"\n",
    "                    else:\n",
    "                        trial_info[\"trial_condition\"] = f\"Error: Got {trial_condition_letter}\"\n",
    "\n",
    "\n",
    "\n",
    "            # Loop through the same trial again, this time with the trial info contained in the trial_info dict:\n",
    "            for data_line_idx, data_entry in enumerate(trial_data):\n",
    "\n",
    "                # Check if the main stimulus was visible when this data line was written:\n",
    "                if trial_info[\"main_stim_onset_idx\"] <= data_line_idx < trial_info[\"main_stim_offset_idx\"]:\n",
    "                    main_stim_is_visible = True\n",
    "                else:\n",
    "                    main_stim_is_visible = False\n",
    "\n",
    "\n",
    "                # Find the fixation index period where the current data entry will fall,\n",
    "                # and check if the target was showing, and what letter was showing (if any):\n",
    "                if data_line_idx < trial_info[\"fixation_stim_onset_idxs\"][0]:\n",
    "                    # This handles the first few lines of each trial where there is noting pertaining to the actual on-screen stimuli.\n",
    "                    # In this event, the previous info pertatining to the attention stimulus should be correct, except in the very first trial\n",
    "                    # of an experiment. In that case, there will be no stimulus info. target_is_on is therefore defined as false, and letter as None\n",
    "                    # when they're first initialised.\n",
    "                    pass # This code was included for readability, but could be removed later if needed by swapping the operator in the if statement\n",
    "                else:\n",
    "                    for fix_onset, fix_offset, fix_letter, fix_status  in zip(\n",
    "                            trial_info[\"fixation_stim_onset_idxs\"],\n",
    "                            trial_info[\"fixation_stim_offset_idxs\"],\n",
    "                            trial_info[\"fixation_letters\"],\n",
    "                            trial_info[\"fixation_types\"]\n",
    "                    ):\n",
    "                        if fix_onset < data_line_idx <= fix_offset:\n",
    "                            # Define the letter during this period:\n",
    "                            if fix_letter == \"None\":\n",
    "                                curr_letter = None\n",
    "                            else:\n",
    "                                curr_letter = fix_letter\n",
    "\n",
    "                            # Define the status of the attention stimulus (was it a target or standard attention stimulus?):\n",
    "                            if fix_status == \"Normal\":\n",
    "                                target_is_on = False\n",
    "                            elif fix_status == \"Target\":\n",
    "                                target_is_on = True\n",
    "                            else:\n",
    "                                print(f\"ERROR: Unexpected target value: {fix_status}\")\n",
    "                            break\n",
    "\n",
    "\n",
    "                line_as_dict = line_to_dict(\n",
    "                    entry=data_entry,\n",
    "                    session_num=session,\n",
    "                    block_num=block,\n",
    "                    trial_num=trial,\n",
    "                    session_con=session_condition,\n",
    "                    session_common_ob=session_common_oddball,\n",
    "                    trial_con=trial_info[\"trial_condition\"],\n",
    "                    main_stim_visibility=main_stim_is_visible,\n",
    "                    main_stim_type=trial_info[\"main_stim_type\"],\n",
    "                    target_status=target_is_on,\n",
    "                    att_letter=curr_letter\n",
    "                )\n",
    "\n",
    "                data_for_pd.append(line_as_dict)"
   ],
   "id": "678e0e08855b3fdb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = pd.DataFrame(data_for_pd)",
   "id": "130bf04da51bf55d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.to_csv(output_filepath)",
   "id": "b94e42016f99403a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for line in debug_log:\n",
    "    print(line)"
   ],
   "id": "b5fbf1df166b57f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Calculate baseline periods for the first trial of each session\n",
    "These are needed so that the first trial of each session can have a baseline from which to normalise the pupil data. All other trials can use the respective prior trial's last 200ms to calculate these from, but we need to use the adaptation period for the first one."
   ],
   "id": "a5a76f45ea69fd26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "baselines = {}\n",
    "for session in [1, 2, 3, 4]:\n",
    "    print(f\"Calculating Baselines for Session: {session}\")\n",
    "\n",
    "    baseline_period = extracted_data[f\"session_{session}\"][\"adaptation_period\"][-501:-1]\n",
    "\n",
    "    baseline_measurements = []\n",
    "    for entry in baseline_period:\n",
    "        if not entry[0].isnumeric():\n",
    "            pass\n",
    "        else:\n",
    "            baseline_measurements.append(float(entry.split(\"\\t\")[3].strip()))\n",
    "    ses_baseline = np.array(baseline_measurements).mean()\n",
    "\n",
    "    baselines[f\"Session {session}\"] = ses_baseline\n",
    "    print(f\"Baseline taken from {len(baseline_measurements)} entries\")\n",
    "    print(f\"Baseline: {ses_baseline}\\n\")\n",
    "\n",
    "os.makedirs(f\"data/processed_data/baselines/{subject_id}\", exist_ok=True)\n",
    "\n",
    "baseline_filename = f\"data/processed_data/baselines/{subject_id}/run_{run_number}_baselines.pkl\"\n",
    "with open(baseline_filename, 'wb') as f: # 'wb' for write binary\n",
    "    pickle.dump(baselines, f)"
   ],
   "id": "9212ec506fc7e26b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Create a dictionary of final blinks and saccade pass on to the analysis workbook\n",
    "This is necessary because we're not passing along the adaptation phase information in the .csv as it's not of interest to our analysis.\n",
    "However, a blink or saccade could have started during this window of time, then end during the first trial.\n",
    "If it ends during the first trial and we don't have the start message, then we will have start and end times that are incorrectly paired together.\n",
    "Instead, we'll process this data here instead and pass it on."
   ],
   "id": "9953ea46891d7997"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "blinks_and_saccades = {\n",
    "    \"session_1\": {\"blinks\": None, \"saccades\": None},\n",
    "    \"session_2\": {\"blinks\": None, \"saccades\": None},\n",
    "    \"session_3\": {\"blinks\": None, \"saccades\": None},\n",
    "    \"session_4\": {\"blinks\": None, \"saccades\": None},\n",
    "}\n",
    "for session in [1, 2, 3, 4]:\n",
    "    blink_data = []\n",
    "    saccade_data = []\n",
    "    for entry in extracted_data[f\"session_{session}\"][\"adaptation_period\"]:\n",
    "        if not entry[0].isnumeric():\n",
    "            if \"SSACC\" in entry:\n",
    "                saccade_data.append((\"Start\", int(entry.split(' ')[-1].strip())))\n",
    "\n",
    "            elif \"ESACC\" in entry:\n",
    "                saccade_data.append((\"End\", int(entry.split('\\t')[1])))\n",
    "\n",
    "            elif \"SBLINK\" in entry:\n",
    "                blink_data.append((\"Start\", int(entry.split(\" \")[2].strip()) - 1))\n",
    "\n",
    "            elif \"EBLINK\" in entry:\n",
    "                blink_data.append((\"End\", int(entry.split(\"\\t\")[1].split(\" \")[0]) + 1))\n",
    "\n",
    "    for entry in extracted_data[f\"session_{session}\"][\"adaptation_period_overspill\"]:\n",
    "        if not entry[0].isnumeric():\n",
    "            if \"SSACC\" in entry:\n",
    "                saccade_data.append((\"Start\", int(entry.split(' ')[-1].strip())))\n",
    "\n",
    "            elif \"ESACC\" in entry:\n",
    "                saccade_data.append((\"End\", int(entry.split('\\t')[1])))\n",
    "\n",
    "            elif \"SBLINK\" in entry:\n",
    "                blink_data.append((\"Start\", int(entry.split(\" \")[2].strip()) - 1))\n",
    "\n",
    "            elif \"EBLINK\" in entry:\n",
    "                blink_data.append((\"End\", int(entry.split(\"\\t\")[1].split(\" \")[0]) + 1))\n",
    "\n",
    "    if len(saccade_data) > 0: # Check if the list is populated so we don't get indexing errors:\n",
    "        if saccade_data[-1][0] == \"Start\":\n",
    "            blinks_and_saccades[f\"session_{session}\"][\"saccades\"] = saccade_data[-1][-1] # Add the data if the final saccade started but didn't end\n",
    "    if len(blink_data)> 0:\n",
    "        if blink_data[-1][0] == \"Start\":\n",
    "            blinks_and_saccades[f\"session_{session}\"][\"blinks\"] = blink_data[-1][-1]\n",
    "\n",
    "print(blinks_and_saccades)\n",
    "\n",
    "os.makedirs(f\"data/processed_data/final_blinks_and_saccades/{subject_id}\", exist_ok=True)\n",
    "\n",
    "blinks_and_saccades_filename = f\"data/processed_data/final_blinks_and_saccades/{subject_id}/run_{run_number}.pkl\"\n",
    "with open(blinks_and_saccades_filename, 'wb') as f: # 'wb' for write binary\n",
    "    pickle.dump(blinks_and_saccades, f)"
   ],
   "id": "11411b1431fb54e3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
