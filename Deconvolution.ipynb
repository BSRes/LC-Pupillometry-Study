{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Phase 1: Setup and Data Loading",
   "id": "ac961c58f5ef2d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Step 1.1: Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm # For GLM\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os # For file/directory operations if needed later"
   ],
   "id": "d554243433e71080",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# --- Step 1.2: Configuration ---\n",
    "PARTICIPANT_ID = \"dddd\"  # Example: \"aaaa\"\n",
    "RUN = 1                 # Example: 1\n",
    "PLOT_REGRESSORS = True\n",
    "regressor_session = 1 # The session for which regressors will be inspected if PLOT_REGRESSORS is set to True\n",
    "\n",
    "# Path to the deconvolution input CSV\n",
    "# This file should contain all original samples within continuous recording blocks (sessions),\n",
    "# with flag\n",
    "# s for trial rejection, but no trials physically dropped if it creates discontinuities.\n",
    "# It should also NOT have undergone trial-specific baselining.\n",
    "DECONV_INPUT_FILENAME = f\"{RUN}_deconvolution_input.csv\"\n",
    "input_data_path = f\"./data/fully_preprocessed_data/{PARTICIPANT_ID}/{DECONV_INPUT_FILENAME}\"\n",
    "message_data_path = f\"./data/fully_preprocessed_data/{PARTICIPANT_ID}/{RUN}_messages.csv\"\n",
    "\n",
    "image_dir = f\"./data/deconv_figs/{PARTICIPANT_ID}/\"\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "pupil_signal_column_name = 'Pupil Filtered'  # This should be the filtered, interpolated data\n",
    "\n",
    "# Columns required from the input CSV\n",
    "# TODO: Make sure all events are flagged - go to the pre-processing step and see what details have been left out of the deconvolution.csv output\n",
    "required_cols = [\n",
    "    'Subject', 'Run', 'Session', 'Block', 'Trial', 'Timestamp',\n",
    "    pupil_signal_column_name,\n",
    "    'Session Condition',      # e.g., 'attend', 'divert'\n",
    "    'Trial Condition',        # e.g., 'Standard', 'Oddball'\n",
    "    'Main Stimulus Type',     # e.g., 'coarse_gabor', 'fine_gabor', 'noise_disk'\n",
    "    'Session Common Oddball', # Name of the common oddball type for that session\n",
    "    'Trial Main Stim Onset',  # Timestamp of main visual stimulus onset for the current trial\n",
    "    'Trial_Rejected_Least_Strict', # The flag indicating if a trial met rejection criteria\n",
    "    'Blink On Main Stim',\n",
    "    'Saccade On Main Stim',\n",
    "    'Exclude Trial',          # Flag for trials with un-interpolatable NaNs in Pupil Int\n",
    "    'Interpolation PC',\n",
    "    # Columns needed for Nuisance Event Regressors\n",
    "    'Target Status',          # Boolean: True if an attention task target is on screen\n",
    "    'Attention Letter',       # The letter shown (for divert) or NaN (for attend)\n",
    "    'Blink',                  # Boolean: True if current sample is within a padded blink period\n",
    "    'Saccade'                 # Boolean: True if current sample is within a padded saccade period\n",
    "]\n",
    "\n",
    "print(f\"Attempting to load: {input_data_path}\")\n",
    "\n",
    "# --- Step 1.3: Load Data ---\n",
    "# messages df:\n",
    "try:\n",
    "    messages_df = pd.read_csv(message_data_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at {message_data_path}\")\n",
    "    print(\"Please ensure the 'Data Pre-processing.ipynb' notebook has been run with DECONVOLUTION_OUTPUT=True,\")\n",
    "    print(f\"and the file '{RUN}_messages.csv' exists in the correct directory.\")\n",
    "    raise  # Stop execution\n",
    "except ValueError as e:\n",
    "    print(f\"ERROR during data loading or column check: {e}\")\n",
    "    print(\"This might be due to missing columns in the CSV or an issue with the file path.\")\n",
    "    raise  # Stop execution\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during data loading: {e}\")\n",
    "    raise\n",
    "\n",
    "# data df:\n",
    "try:\n",
    "    raw_df = pd.read_csv(input_data_path, usecols=lambda col_name: col_name in required_cols)   # Passing cols as list would raise ValueError if col is missing.\n",
    "                                                                                                # Passing via lambda simply doesn't load missing cols\n",
    "    print(f\"\\nSuccessfully loaded data for Participant {PARTICIPANT_ID}, Run {RUN}.\")\n",
    "    print(f\"Shape of loaded DataFrame (raw_df): {raw_df.shape}\")\n",
    "    original_sample_count = raw_df.shape[0] # Used in the downsampling code to check that the downsampled number of samples is the expected value by comparing the orig number / the downsampling factor\n",
    "\n",
    "    # Basic check for pupil signal column\n",
    "    if pupil_signal_column_name not in raw_df.columns:\n",
    "        raise ValueError(f\"Pupil signal column '{pupil_signal_column_name}' not found in loaded data.\")\n",
    "    if 'Trial Main Stim Onset' not in raw_df.columns:\n",
    "        raise ValueError(f\"'Trial Main Stim Onset' column not found. It's critical for event alignment.\")\n",
    "    if 'Trial_Rejected_Least_Strict' not in raw_df.columns:\n",
    "        print(f\"WARNING: 'Trial_Rejected_Least_Strict' column not found.\\n\\tWill assume all trials are good for event selection. Ensure this is intended.\")\n",
    "        raw_df['Trial_Rejected_Least_Strict'] = False # Add it as False if missing, for safety\n",
    "\n",
    "    # Convert relevant columns to appropriate types if necessary (e.g., for merging or boolean checks)\n",
    "    raw_df['Trial_Rejected_Least_Strict'] = raw_df['Trial_Rejected_Least_Strict'].astype(bool)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at {input_data_path}\")\n",
    "    print(\"Please ensure the 'Data Pre-processing.ipynb' notebook has been run with DECONVOLUTION_OUTPUT=True,\")\n",
    "    print(f\"and the file '{DECONV_INPUT_FILENAME}' exists in the correct directory.\")\n",
    "    raise  # Stop execution\n",
    "except ValueError as e:\n",
    "    print(f\"ERROR during data loading or column check: {e}\")\n",
    "    print(\"This might be due to missing columns in the CSV or an issue with the file path.\")\n",
    "    raise  # Stop execution\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during data loading: {e}\")\n",
    "    raise\n",
    "\n",
    "# Verify that 'Trial_Rejected_Least_Strict' has both True and False values if expected\n",
    "if 'Trial_Rejected_Least_Strict' in raw_df.columns:\n",
    "    print(f\"\\nValue counts for 'Trial_Rejected_Least_Strict':\")\n",
    "    print(raw_df['Trial_Rejected_Least_Strict'].value_counts(dropna=False))\n",
    "\n",
    "# Display a snippet of the loaded data\n",
    "print(\"\\nSnippet of loaded data:\")\n",
    "raw_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Setup for the Finite Impulse Response Model and the Downsampling:",
   "id": "134e46658a3309bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Step 1.4: Define FIR Model Parameters ---\n",
    "# These parameters define how we'll model the shape of the pupil response.\n",
    "fir_window_start_ms = 0    # When to start estimating responses from an event onset\n",
    "fir_window_end_ms = 3000   # Estimate response up to 3000ms after the event onset\n",
    "fir_bin_width_ms = 200     # Each FIR beta will represent average activity over a fir_bin_width_ms millisecond bin\n",
    "\n",
    "# Calculate the number of FIR bins/regressors per event type\n",
    "if fir_bin_width_ms <= 0:\n",
    "    raise ValueError(\"fir_bin_width_ms must be positive.\")\n",
    "num_fir_bins = int((fir_window_end_ms - fir_window_start_ms) / fir_bin_width_ms)\n",
    "\n",
    "if num_fir_bins <= 0:\n",
    "    raise ValueError(\"Number of FIR bins is not positive. Check fir_window_start_ms, fir_window_end_ms, and fir_bin_width_ms.\")\n",
    "\n",
    "print(f\"--- FIR Model Parameters ---\")\n",
    "print(f\"  Response window: {fir_window_start_ms}ms to {fir_window_end_ms}ms post-event\")\n",
    "print(f\"  Bin width: {fir_bin_width_ms}ms\")\n",
    "print(f\"  Number of FIR bins per event type: {num_fir_bins}\")\n",
    "\n",
    "# This will be the sampling period of the data *as it goes into the GLM*:\n",
    "initial_sampling_period_ms = 1 # Placeholder, assuming 1000Hz initially.\n",
    "# The actual sampling_period_ms used by run_fir_glm will be set after potential downsampling.\n",
    "\n",
    "\n",
    "\n",
    "# --- Step 1.5: Define Downsampling Parameters ---\n",
    "DO_DOWNSAMPLING = True  # Set to False to run on original 1000Hz data (slower)\n",
    "target_sr_hz = 100      # Target sampling rate in Hz (e.g., 100Hz for 10ms resolution)\n",
    "\n",
    "if DO_DOWNSAMPLING:\n",
    "    if target_sr_hz <= 0:\n",
    "        raise ValueError(\"target_sr_hz must be positive.\")\n",
    "    # Calculate the new sampling period after downsampling\n",
    "    new_sampling_period_ms = int(1000 / target_sr_hz)\n",
    "\n",
    "    # We need the original sampling rate to calculate the downsample factor:\n",
    "    original_sr_hz = 1000 # Hz\n",
    "\n",
    "    if original_sr_hz % target_sr_hz != 0:\n",
    "        print(f\"Warning: Original sampling rate ({original_sr_hz}Hz) is not an integer multiple of the target rate ({target_sr_hz}Hz). \"\n",
    "              \"Downsampling might result in slightly uneven binning or require careful handling.\")\n",
    "\n",
    "    downsample_factor = int(original_sr_hz / target_sr_hz)\n",
    "\n",
    "    if downsample_factor <= 1 and original_sr_hz > target_sr_hz : # Should only happen if target_sr is >= original_sr incorrectly\n",
    "        print(f\"Warning: Downsample factor is {downsample_factor}. Check original_sr_hz and target_sr_hz. Will effectively not downsample if factor is 1.\")\n",
    "        # If factor is 1 then effectively no downsampling, but we'll still use new_sampling_period_ms which should match initial_sampling_period_ms:\n",
    "        actually_downsampling = False\n",
    "    elif downsample_factor <=1 and original_sr_hz <= target_sr_hz:\n",
    "        print(f\"Target sampling rate ({target_sr_hz}Hz) is >= original ({original_sr_hz}Hz). No downsampling will be performed.\")\n",
    "        DO_DOWNSAMPLING = False # Override if no actual downsampling needed\n",
    "        actually_downsampling = False\n",
    "        new_sampling_period_ms = initial_sampling_period_ms # Revert to initial\n",
    "        downsample_factor = 1\n",
    "    else:\n",
    "        actually_downsampling = True\n",
    "\n",
    "    print(f\"\\n--- Downsampling Parameters ---\")\n",
    "    if actually_downsampling:\n",
    "        print(f\"  Downsampling ENABLED.\")\n",
    "        print(f\"  Original sampling rate: {original_sr_hz}Hz (assumed)\")\n",
    "        print(f\"  Target sampling rate: {target_sr_hz}Hz\")\n",
    "        print(f\"  Downsample factor: {downsample_factor}\")\n",
    "        print(f\"  New sampling period for GLM: {new_sampling_period_ms}ms\")\n",
    "    else:\n",
    "        if DO_DOWNSAMPLING: # DO_DOWNSAMPLING was true but factor ended up <=1\n",
    "             print(f\"  Downsampling was enabled, but factor is {downsample_factor}. Effective sampling rate will be {original_sr_hz}Hz.\")\n",
    "             new_sampling_period_ms = initial_sampling_period_ms\n",
    "        else: # DO_DOWNSAMPLING was false from the start\n",
    "            print(f\"  Downsampling DISABLED.\")\n",
    "            new_sampling_period_ms = initial_sampling_period_ms # Use initial if no downsampling\n",
    "            downsample_factor = 1\n",
    "            print(f\"  Sampling period for GLM: {new_sampling_period_ms}ms\")\n",
    "else:\n",
    "    print(f\"\\n--- Downsampling Parameters ---\")\n",
    "    print(f\"  Downsampling DISABLED.\")\n",
    "    new_sampling_period_ms = initial_sampling_period_ms # Use initial if no downsampling\n",
    "    downsample_factor = 1\n",
    "    print(f\"  Sampling period for GLM: {new_sampling_period_ms}ms\")\n",
    "\n",
    "# Store the effective sampling period to be used by the GLM and FIR construction\n",
    "effective_sampling_period_ms_for_glm = new_sampling_period_ms\n",
    "\n",
    "# Update FIR parameters dictionary that will be passed around\n",
    "# Note: fir_bin_width_ms and num_fir_bins are defined based on desired *model resolution*,\n",
    "# not directly by the sampling rate of the data, but the sampling rate determines\n",
    "# how many actual data points fall into each conceptual FIR bin's influence or represent a lag.\n",
    "fir_parameters_for_glm = {\n",
    "    'fir_window_start_ms': fir_window_start_ms,\n",
    "    'fir_window_end_ms': fir_window_end_ms,\n",
    "    'fir_bin_width_ms': fir_bin_width_ms, # This is the width of the conceptual bin in ms\n",
    "    'num_fir_bins': num_fir_bins,\n",
    "    'sampling_period_ms': effective_sampling_period_ms_for_glm # This is critical for run_fir_glm\n",
    "}\n",
    "\n",
    "print(f\"\\nEffective sampling period for GLM construction: {fir_parameters_for_glm['sampling_period_ms']}ms\")"
   ],
   "id": "714c27983021cb6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Define the GLM function:\n",
    "This builds the design matrix and fits the OLS model for a single continuous segment of data."
   ],
   "id": "3848b2de82ab945c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pupil_Deconvolution.ipynb\n",
    "\n",
    "# ... (Previous cells: Imports, Config, Data Loading, FIR/Downsample Params - Steps 1.1 to 1.5) ...\n",
    "\n",
    "# --- Step 2.1: Define Function to Create Design Matrix and Run GLM ---\n",
    "def run_fir_glm(pupil_data_segment, event_onsets_dict, fir_params, pupil_signal_col, segment_name=\"segment\"):\n",
    "    \"\"\"\n",
    "    Creates FIR design matrix and runs GLM for a continuous pupil data segment.\n",
    "\n",
    "    Args:\n",
    "        pupil_data_segment (pd.DataFrame): DataFrame with 'Timestamp' and `pupil_signal_col`.\n",
    "                                           MUST be a continuous recording segment.\n",
    "                                           Its index should be a simple RangeIndex if reset.\n",
    "        event_onsets_dict (dict): Dict where keys are event_names (e.g., 'Std_Attend')\n",
    "                                  and values are numpy arrays of onset timestamps (in original ms)\n",
    "                                  for that event.\n",
    "        fir_params (dict): Containing 'fir_window_start_ms', 'fir_window_end_ms',\n",
    "                           'fir_bin_width_ms', 'num_fir_bins', 'sampling_period_ms'.\n",
    "                           'sampling_period_ms' here is the period of pupil_data_segment.\n",
    "        pupil_signal_col (str): Name of the column in pupil_data_segment containing the pupil signal.\n",
    "        segment_name (str): Identifier for logging/printing.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Beta coefficients from the GLM, or None if error.\n",
    "    \"\"\"\n",
    "    _pupil_timestamps = pupil_data_segment['Timestamp'].values # Timestamps of the (potentially downsampled) data\n",
    "    _y_pupil_signal = pupil_data_segment[pupil_signal_col].values\n",
    "\n",
    "    if len(_pupil_timestamps) == 0:\n",
    "        print(f\"  GLM_FUNC: No pupil data in {segment_name}. Skipping GLM.\")\n",
    "        return None\n",
    "\n",
    "    # Initialize design matrix using the index of pupil_data_segment\n",
    "    design_matrix_fir = pd.DataFrame(index=pupil_data_segment.index)\n",
    "\n",
    "    for event_name, onsets_for_event_type_ms in event_onsets_dict.items():\n",
    "        if len(onsets_for_event_type_ms) == 0:\n",
    "            # print(f\"  GLM_FUNC: No onsets for {event_name} in {segment_name}.\")\n",
    "            continue # Skip if no onsets for this event type\n",
    "\n",
    "        for i_bin in range(fir_params['num_fir_bins']):\n",
    "            # current_lag_ms is the start of the conceptual bin in ms from event onset\n",
    "            current_lag_ms = fir_params['fir_window_start_ms'] + (i_bin * fir_params['fir_bin_width_ms'])\n",
    "            regressor_name = f'{event_name}_fir_lag{i_bin:02d}_{current_lag_ms}ms'\n",
    "            regressor_column = np.zeros(len(_pupil_timestamps))\n",
    "\n",
    "            # For each onset (in original ms), find its corresponding time point in _pupil_timestamps\n",
    "            # and then add the lag.\n",
    "            for onset_ts_ms in onsets_for_event_type_ms:\n",
    "                # Target time for this impulse: original onset + current lag\n",
    "                target_impulse_time_ms = onset_ts_ms + current_lag_ms\n",
    "\n",
    "                # Find the index in _pupil_timestamps that is closest to target_impulse_time_ms\n",
    "                # This correctly aligns events to the (potentially downsampled) timeline\n",
    "                time_diff = np.abs(_pupil_timestamps - target_impulse_time_ms)\n",
    "                idx_of_impulse_in_segment = np.argmin(time_diff)\n",
    "\n",
    "                # Check if the closest timestamp is reasonably close to the target impulse time\n",
    "                # (e.g., within half of the segment's sampling period)\n",
    "                if time_diff[idx_of_impulse_in_segment] <= (fir_params['sampling_period_ms'] / 2.0):\n",
    "                    if 0 <= idx_of_impulse_in_segment < len(regressor_column): # Boundary check\n",
    "                        regressor_column[idx_of_impulse_in_segment] += 1 # Use +=1 if multiple events could map to same sample\n",
    "\n",
    "            design_matrix_fir[regressor_name] = regressor_column\n",
    "\n",
    "    if 'Blink' in pupil_data_segment.columns:\n",
    "        design_matrix_fir['Blink_Boxcar'] = pupil_data_segment['Blink'].astype(int)\n",
    "    else:\n",
    "        print(f\"  GLM_FUNC: Warning - 'Blink' column not found in pupil_data_segment for {segment_name}.\")\n",
    "\n",
    "    if 'Saccade' in pupil_data_segment.columns:\n",
    "        design_matrix_fir['Saccade_Boxcar'] = pupil_data_segment['Saccade'].astype(int)\n",
    "    else:\n",
    "        print(f\"  GLM_FUNC: Warning - 'Saccade' column not found in pupil_data_segment for {segment_name}.\")\n",
    "\n",
    "         # demeaned linear drift (center around zero)\n",
    "    drift_regressor = np.linspace(-0.5, 0.5, len(pupil_data_segment))\n",
    "    design_matrix_fir['Linear_Drift'] = drift_regressor\n",
    "\n",
    "    if design_matrix_fir.empty and 'Linear_Drift' not in design_matrix_fir.columns : # Check if only drift was added to an empty df\n",
    "        # This case means no event FIR regressors were created and no blink/saccade columns\n",
    "        # If we only have drift and const, it's not very useful for event-related analysis\n",
    "        print(f\"  GLM_FUNC: Design matrix effectively empty (only drift/intercept possible) for {segment_name}. Skipping GLM.\")\n",
    "        return None\n",
    "\n",
    "    # Add intercept (baseline)\n",
    "    # Ensure 'const' is not already a column if this function is called multiple times on subsets\n",
    "    if 'const' not in design_matrix_fir.columns:\n",
    "        design_matrix_fir = sm.add_constant(design_matrix_fir, prepend=True, has_constant='skip')\n",
    "\n",
    "    # Prepare Y and X for OLS:\n",
    "    # Remove rows where Y is NaN (e.g. un-interpolated blinks)\n",
    "    # OLS will do this by default, but it's good to be explicit and align X\n",
    "    valid_y_mask = ~np.isnan(_y_pupil_signal)\n",
    "    y_glm = _y_pupil_signal[valid_y_mask]\n",
    "\n",
    "    if len(y_glm) == 0:\n",
    "        print(f\"  GLM_FUNC: No valid (non-NaN) pupil data points in y_glm for {segment_name}. Skipping GLM.\")\n",
    "        return None\n",
    "\n",
    "    X_glm = design_matrix_fir.loc[valid_y_mask, :] # Use .loc to align with valid_y_mask from original index\n",
    "\n",
    "    # Drop columns from X_glm that are all zero (can happen if no events fall into certain lags for this segment)\n",
    "    # Also drop columns that might have become all NaN if something went very wrong (shouldn't happen)\n",
    "    X_glm = X_glm.loc[:, (X_glm != 0).any(axis=0) & ~X_glm.isna().all(axis=0)]\n",
    "\n",
    "    # Ensure 'const' is still there if other columns were dropped, and X_glm is not empty\n",
    "    if 'const' not in X_glm.columns and not X_glm.empty:\n",
    "        X_glm = sm.add_constant(X_glm, prepend=True, has_constant='skip')\n",
    "\n",
    "    # More robust check for empty/trivial X_glm\n",
    "    is_trivial_X = X_glm.empty\n",
    "    if not is_trivial_X and 'const' in X_glm.columns: # If const is there\n",
    "        if X_glm.shape[1] == 1: # Only const column\n",
    "            is_trivial_X = True\n",
    "        elif X_glm.drop(columns=['const'], errors='ignore').empty: # Only const, and other columns were all zero/NaN\n",
    "             is_trivial_X = True\n",
    "\n",
    "    if is_trivial_X or len(y_glm) < X_glm.shape[1]:\n",
    "        # ... (rest of insufficient data check as before) ...\n",
    "        print(f\"  GLM_FUNC: Not enough data or valid regressors to fit GLM for {segment_name}. \"\\\n",
    "              f\"Y shape: {y_glm.shape}, X shape after cleanup: {X_glm.shape}. Skipping.\")\n",
    "        if not X_glm.empty: print(f\"  GLM_FUNC: X_glm columns: {X_glm.columns.tolist()}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        model = sm.OLS(y_glm, X_glm.astype(float)) # Ensure X_glm is float\n",
    "        results = model.fit()\n",
    "\n",
    "        r_squared = results.rsquared\n",
    "        adj_r_squared = results.rsquared_adj\n",
    "        r_values = (r_squared, adj_r_squared)\n",
    "        print(f\"  GLM_FUNC: Fit for {segment_name} - R-squared: {r_squared:.3f}, Adjusted R-squared: {adj_r_squared:.3f}\")\n",
    "\n",
    "        if PLOT_REGRESSORS:\n",
    "            if segment_name == f\"P{PARTICIPANT_ID}_R{RUN}_S{regressor_session}\": # Or any session of interest\n",
    "                print(f\"\\n--- Residuals for {segment_name} ---\")\n",
    "                plt.figure(figsize=(15, 5))\n",
    "                plt.plot(results.resid) # results.resid are the residuals\n",
    "                plt.title(f\"Residuals for {segment_name}\")\n",
    "                plt.xlabel(\"Sample Index (within segment)\")\n",
    "                plt.ylabel(\"Residual Pupil Value\")\n",
    "                resid_output_path = os.path.join(image_dir, f\"residuals_R{RUN}_S{regressor_session}.png\")\n",
    "                plt.savefig(resid_output_path, dpi=300, bbox_inches='tight')\n",
    "                plt.show()\n",
    "\n",
    "        return results.params, r_values # Return the beta coefficients\n",
    "\n",
    "    except np.linalg.LinAlgError as e:\n",
    "        print(f\"  GLM_FUNC: LinAlgError during GLM fitting for {segment_name}: {e}. Check for perfect multicollinearity.\")\n",
    "        # print(f\"  X_glm dtypes: {X_glm.dtypes}\")\n",
    "        # print(f\"  Unique values in X_glm columns:\\n {X_glm.nunique()}\")\n",
    "        # from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "        # if 'const' in X_glm.columns and X_glm.shape[1] > 2:\n",
    "        #     vif_X = X_glm.drop(columns=['const'])\n",
    "        #     vif_data = pd.DataFrame()\n",
    "        #     vif_data[\"feature\"] = vif_X.columns\n",
    "        #     vif_data[\"VIF\"] = [variance_inflation_factor(vif_X.values, i) for i in range(vif_X.shape[1])]\n",
    "        #     print(vif_data[vif_data['VIF'] > 10]) # Print features with high VIF\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"  GLM_FUNC: Unexpected error during GLM fitting for {segment_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"--- run_fir_glm function defined ---\")"
   ],
   "id": "ca090fa789352676",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2.2 - Mark different events in the timeseries for FIR model construction\n",
    "This crucial step prepares the specific event markers that will define the Finite Impulse Response (FIR) regressors in our General Linear Model (GLM). The GLM attempts to explain the _entire continuous pupil signal_ for a session. However, we strategically choose which types of events and which instances of those events contribute to building the specific FIR predictors we are interested in (e.g., the response to a \"Standard\" stimulus).\n",
    "### Key Actions:\n",
    "- *Filter for \"Good\" Trials:* We first identify all trials within the current session that have not been flagged for rejection by our earlier preprocessing criteria (e.g., Trial_Rejected_Least_Strict == False). This ensures that only data from high-quality trials will inform the estimation of our primary event-related responses.\n",
    "- *Extract Onset Timestamps:* For each type of event we want to model (e.g., \"Standard stimulus in attend condition,\" \"Oddball stimulus in divert condition,\" \"Attention Task Target,\" \"Keypress\"), we extract the precise onset Timestamp (in original milliseconds). These onsets are sourced only from the \"good\" trials identified above.\n",
    "### Impact on the GLM:\n",
    "- *Selective FIR Regressor Construction:* The FIR regressors for our main experimental events (like \"Standard_attend\") are built only using the onsets from these \"good\" instances.\n",
    "*Model Explains All Data, But Event Betas Reflect \"Good\" Instances:* The GLM still uses the entire continuous pupil time series of the session as the data to be explained (the y_pupil_signal).\n",
    "    - The beta coefficients estimated for the FIR regressors (e.g., for Standard_attend_fir_lag_200ms) will therefore reflect the average pupil response at that lag, specifically following the \"good\" instances of \"Standard_attend\" events.\n",
    "    - The pupil data during \"bad\" or unmodeled periods is accounted for by other components of the GLM, such as the intercept (baseline), any nuisance event regressors (like blinks, saccades, or explicitly modeled \"bad trial noise\" if you chose to add such a regressor), and the model's error term.\n",
    "- *Effective \"Rejection\" for Specific Event Responses:* By not including event markers from \"bad\" trials when building the FIR regressors for our main conditions of interest, we are effectively ensuring that those \"bad\" trial instances do not directly influence the estimated average response shape (the betas) for those specific conditions. The model doesn't try to fit a \"Standard_attend\" response shape using data from a trial where, for instance, a blink obscured the standard stimulus.\n",
    "\n",
    "In essence, this step precisely curates the set of event occurrences that will define and shape the estimated pupillary responses for each experimental condition we care about, while the GLM still considers the full observed pupil dynamics of the session.\n"
   ],
   "id": "a39b9766ffab8424"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_betas_list = [] # To store DataFrames of betas from each successful GLM run\n",
    "\n",
    "# Group by 'Session' because each session is a continuous recording block.\n",
    "# The GLM should be run on continuous data.\n",
    "# raw_df should contain all original samples for the run, with flags, not with rows dropped.\n",
    "# If raw_df had rows dropped earlier, this groupby might yield discontinuous segments if\n",
    "# a session was broken by dropped trials. This assumes raw_df is continuous per session.\n",
    "\n",
    "print(f\"\\n--- Starting GLM Processing for Participant {PARTICIPANT_ID}, Run {RUN} ---\")\n",
    "if 'Session' not in raw_df.columns:\n",
    "    raise ValueError(\"'Session' column not found in raw_df. Cannot group by session.\")\n",
    "\n",
    "n_sample_count = 0 # Counts the num of samples produced in the downsampling - checked against original downsampling to ensure the factor used is correct\n",
    "\n",
    "# Define trial identifiers, useful for merging message onsets with good trial flags\n",
    "trial_id_cols = ['Subject', 'Run', 'Session', 'Block', 'Trial']\n",
    "\n",
    "model_r_values = []\n",
    "session_orders = []\n",
    "for session_id, session_data_continuous in raw_df.groupby('Session'):\n",
    "    segment_name = f\"P{PARTICIPANT_ID}_R{RUN}_S{session_id}\"\n",
    "    print(f\"\\nProcessing {segment_name}...\")\n",
    "\n",
    "    # --- Step 2.2.1: Current Session Data ---\n",
    "    current_session_condition_str = session_data_continuous['Session Condition'].iloc[0]\n",
    "    session_orders.append(current_session_condition_str)\n",
    "\n",
    "    # --- Step 2.2.2: Downsample (Optional) ---\n",
    "    current_fir_params_for_glm = fir_parameters_for_glm.copy() # Use the globally defined effective params\n",
    "\n",
    "    if DO_DOWNSAMPLING and downsample_factor > 1:\n",
    "        print(f\"  Downsampling {segment_name} from {original_sr_hz}Hz to {target_sr_hz}Hz (factor {downsample_factor})...\")\n",
    "        # Ensure sorted by Timestamp (groupby should preserve order from sorted raw_df, but belt-and-suspenders)\n",
    "        session_data_sorted = session_data_continuous.sort_values('Timestamp').reset_index(drop=True)\n",
    "        group_idx_ds = session_data_sorted.index // downsample_factor\n",
    "        agg_dict_ds = {pupil_signal_column_name: 'mean', 'Timestamp': 'first'}\n",
    "        static_cols_for_ds = [\n",
    "            'Subject', 'Run', 'Session', 'Block', 'Trial',\n",
    "            'Session Condition', 'Trial Condition', 'Main Stimulus Type',\n",
    "            'Session Common Oddball', 'Trial Main Stim Onset',\n",
    "            'Trial_Rejected_Least_Strict', 'Blink', 'Saccade', # Keep Blink/Saccade for boxcar regressors\n",
    "            'Target Status', 'Attention Letter' # Needed for nuisance onset logic if done on downsampled\n",
    "        ]\n",
    "        for col in static_cols_for_ds:\n",
    "            if col in session_data_sorted.columns:\n",
    "                agg_dict_ds[col] = 'first'\n",
    "        session_data_for_glm = session_data_sorted.groupby(group_idx_ds).agg(agg_dict_ds).reset_index(drop=True)\n",
    "        print(f\"  Downsampled data shape: {session_data_for_glm.shape}\")\n",
    "        n_sample_count += session_data_for_glm.shape[0]\n",
    "    else:\n",
    "        session_data_for_glm = session_data_continuous.reset_index(drop=True)\n",
    "        print(f\"  No downsampling for {segment_name}. Using original resolution.\")\n",
    "\n",
    "    if session_data_for_glm.empty:\n",
    "        print(f\"  {segment_name} is empty after potential downsampling. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # --- Step 2.2.3: Identify \"Good\" Event Onsets for this Session ---\n",
    "    event_onsets_this_session = {}\n",
    "\n",
    "    # Get \"good\" trial identifiers from the continuous session data (before downsampling for flags)\n",
    "    good_trial_flags_df = session_data_continuous[\n",
    "        session_data_continuous['Trial_Rejected_Least_Strict'] == False\n",
    "    ][trial_id_cols].drop_duplicates()\n",
    "\n",
    "    if good_trial_flags_df.empty:\n",
    "        print(f\"  {segment_name}: No 'good' trials found based on Trial_Rejected_Least_Strict flag. \"\n",
    "              \"No event-related FIR regressors will be created for main/task events.\")\n",
    "    # else:\n",
    "    #    print(f\"  {segment_name}: Found {len(good_trial_flags_df)} good trials for event onset extraction.\")\n",
    "\n",
    "\n",
    "    # --- Main Stimulus Onsets (from \"good\" trials only) ---\n",
    "    # Filter session_data_continuous for good trials first, then extract onsets\n",
    "    data_for_main_event_onsets = pd.merge(session_data_continuous, good_trial_flags_df, on=trial_id_cols, how='inner')\n",
    "\n",
    "    if not data_for_main_event_onsets.empty:\n",
    "        std_onsets = data_for_main_event_onsets[\n",
    "            data_for_main_event_onsets['Trial Condition'] == 'Standard'\n",
    "        ]['Trial Main Stim Onset'].drop_duplicates().values\n",
    "        event_onsets_this_session[f'Standard_{current_session_condition_str}'] = std_onsets\n",
    "\n",
    "        oddball_trials_for_onsets = data_for_main_event_onsets[\n",
    "            data_for_main_event_onsets['Trial Condition'] == 'Oddball'\n",
    "        ].drop_duplicates(subset=['Block', 'Trial']) # Unique oddball trials from good trials\n",
    "\n",
    "        common_odd_onsets = []\n",
    "        rare_odd_onsets = []\n",
    "        if not oddball_trials_for_onsets.empty:\n",
    "            session_common_oddball_type = oddball_trials_for_onsets['Session Common Oddball'].iloc[0]\n",
    "            for _, trial_row in oddball_trials_for_onsets.iterrows():\n",
    "                if trial_row['Main Stimulus Type'] == session_common_oddball_type:\n",
    "                    common_odd_onsets.append(trial_row['Trial Main Stim Onset'])\n",
    "                else:\n",
    "                    rare_odd_onsets.append(trial_row['Trial Main Stim Onset'])\n",
    "        event_onsets_this_session[f'CommOdd_{current_session_condition_str}'] = np.array(common_odd_onsets)\n",
    "        event_onsets_this_session[f'RareOdd_{current_session_condition_str}'] = np.array(rare_odd_onsets)\n",
    "    else: # No good trials, so initialize main event types with empty arrays\n",
    "        event_onsets_this_session[f'Standard_{current_session_condition_str}'] = np.array([])\n",
    "        event_onsets_this_session[f'CommOdd_{current_session_condition_str}'] = np.array([])\n",
    "        event_onsets_this_session[f'RareOdd_{current_session_condition_str}'] = np.array([])\n",
    "\n",
    "\n",
    "    # --- Nuisance Event Onsets (from \"good\" trials using messages_df) ---\n",
    "    # Filter messages_df for the current session first\n",
    "    current_session_messages_df = messages_df[\n",
    "        (messages_df['Subject'] == PARTICIPANT_ID) &\n",
    "        (messages_df['Run'] == RUN) &\n",
    "        (messages_df['Session'] == session_id)\n",
    "    ].copy()\n",
    "\n",
    "    # Merge with good_trial_flags_df to get only messages from good trials\n",
    "    messages_from_good_trials = pd.merge(current_session_messages_df, good_trial_flags_df, on=trial_id_cols, how='inner')\n",
    "\n",
    "    if not messages_from_good_trials.empty:\n",
    "        # Keypress Onsets\n",
    "        keypress_mask = (messages_from_good_trials['Message Type'] == 'Key Response')\n",
    "        keypress_onsets = messages_from_good_trials.loc[keypress_mask, 'Timestamp'].values\n",
    "        event_onsets_this_session[f'Keypress_{current_session_condition_str}'] = keypress_onsets\n",
    "\n",
    "        # Attention Task Related Onsets from 'Fixation Stimulus Onset' messages\n",
    "        fix_stim_onset_msgs = messages_from_good_trials[\n",
    "            messages_from_good_trials['Message Type'] == 'Fixation Stimulus Onset'\n",
    "        ]\n",
    "\n",
    "        if not fix_stim_onset_msgs.empty:\n",
    "            # Attention Targets (color change or 'X')\n",
    "            # 'Target Status' in messages_df for 'Fixation Stimulus Onset' message should indicate if it's a target\n",
    "            att_target_mask = (fix_stim_onset_msgs['Target Status'] == True)\n",
    "            att_target_onsets = fix_stim_onset_msgs.loc[att_target_mask, 'Timestamp'].values\n",
    "            event_onsets_this_session[f'AttTarget_{current_session_condition_str}'] = att_target_onsets\n",
    "\n",
    "            # Divert Condition: Distractor Letter Onsets (non-'X' letters)\n",
    "            if current_session_condition_str == 'divert':\n",
    "                # Distractors are Fixation Stim Onsets that are NOT targets\n",
    "                # AND Attention Letter is not NaN (it shouldn't be for divert) and not 'X'\n",
    "                distractor_mask = (\n",
    "                    (fix_stim_onset_msgs['Target Status'] == False) &\n",
    "                    (fix_stim_onset_msgs['Attention Letter'].notna()) &\n",
    "                    (fix_stim_onset_msgs['Attention Letter'] != 'X')\n",
    "                )\n",
    "                distractor_onsets = fix_stim_onset_msgs.loc[distractor_mask, 'Timestamp'].values\n",
    "                event_onsets_this_session['Distractor_divert_Onset'] = distractor_onsets\n",
    "    else: # No messages from good trials, initialize nuisance with empty arrays\n",
    "        event_onsets_this_session[f'Keypress_{current_session_condition_str}'] = np.array([])\n",
    "        event_onsets_this_session[f'AttTarget_{current_session_condition_str}'] = np.array([])\n",
    "        if current_session_condition_str == 'divert':\n",
    "            event_onsets_this_session['Distractor_divert_Onset'] = np.array([])\n",
    "\n",
    "\n",
    "    # --- Sanity check event counts ---\n",
    "    print(f\"  Event counts for {segment_name} (from good trials):\")\n",
    "    for ev, ons in event_onsets_this_session.items():\n",
    "        if isinstance(ons, np.ndarray): print(f\"    {ev}: {len(ons)}\")\n",
    "\n",
    "    # --- Step 2.2.4: Call `run_fir_glm` ---\n",
    "    # Pass the (potentially downsampled) session_data_for_glm.\n",
    "    # Blink and Saccade columns for boxcar regressors should exist in session_data_for_glm\n",
    "    # if they were in static_cols_for_ds during downsampling.\n",
    "\n",
    "    session_betas, r_vals = run_fir_glm(\n",
    "        pupil_data_segment=session_data_for_glm,\n",
    "        event_onsets_dict=event_onsets_this_session,\n",
    "        fir_params=current_fir_params_for_glm,\n",
    "        pupil_signal_col=pupil_signal_column_name,\n",
    "        segment_name=segment_name\n",
    "    )\n",
    "    model_r_values.append(r_vals)\n",
    "\n",
    "    # --- Step 2.2.5: Store Betas ---\n",
    "    if session_betas is not None and not session_betas.empty:\n",
    "        betas_formatted_df = session_betas.reset_index()\n",
    "        betas_formatted_df.columns = ['Regressor', 'Beta']\n",
    "        betas_formatted_df['Subject'] = PARTICIPANT_ID\n",
    "        betas_formatted_df['Run'] = RUN\n",
    "        betas_formatted_df['Session'] = session_id\n",
    "        betas_formatted_df['Session_Processed_Condition'] = current_session_condition_str\n",
    "        all_betas_list.append(betas_formatted_df)\n",
    "        print(f\"  Successfully processed and stored betas for {segment_name}.\")\n",
    "    else:\n",
    "        print(f\"  No betas returned or betas were empty for {segment_name}.\")\n",
    "\n",
    "print(f\"\\nTotal downsampled samples across sessions: {n_sample_count}. Original sample number: {original_sample_count}. \"\n",
    "      f\"\\nClosest int to factor used: {int(round(original_sample_count/n_sample_count, 0))} - Expected factor: {downsample_factor}.\")\n",
    "\n",
    "print(f\"\\n--- GLM Processing Complete for Participant {PARTICIPANT_ID}, Run {RUN} ---\")"
   ],
   "id": "598e8a75be309cc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check the betas have been successfully produced for each of the 4 sessions:\n",
    "print(f\"Length of all_betas_list: {len(all_betas_list)}\")\n",
    "\n",
    "for idx, df in enumerate(all_betas_list):\n",
    "    print(f\"\\nSession {idx+1} df in all_betas_list:\")\n",
    "    print(f\"\\tShape of df: {df.shape}\")\n",
    "    print(f\"\\tCols in df: {df.columns}\")\n",
    "    print(f\"\\tSegment of df: {df.head(5)}\")\n",
    "    print(\"-\"*100)\n"
   ],
   "id": "67e9ba9ad105879a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Concatenate Betas into a single DF and Parse Regressor Names\n",
    "## Step 3.1: Concatenate Betas (pd.concat)\n",
    "- It first checks if all_betas_list (which should contain one DataFrame of betas per session) is empty. If it is, it prints a message and creates an empty final_betas_df to prevent errors in subsequent cells.\n",
    "- If all_betas_list is not empty, pd.concat(all_betas_list, ignore_index=True) stacks all the individual session-beta DataFrames into one long DataFrame called final_betas_df.\n",
    "- ignore_index=True ensures that the resulting DataFrame has a clean, new RangeIndex.\n",
    "- It prints the shape and head/tail of this concatenated DataFrame.\n",
    "\n",
    "## Step 3.2: Parse Regressor Names (parse_regressor_name function)\n",
    "- This is a crucial step to make the final_betas_df more usable for plotting and analysis.\n",
    "- The parse_regressor_name function is designed to take the Regressor column (which contains strings like Standard_attend_fir_lag00_0ms or const).\n",
    "- *For FIR regressors:*\n",
    "    - It splits the string to extract the base Event_Type (e.g., Standard_attend).\n",
    "    - It extracts the Lag_Index (e.g., 00 -> 0). This is useful for ordered plotting if names aren't perfectly sortable by ms alone.\n",
    "    - It extracts the Lag_ms (e.g., 0ms -> 0). This will be our x-axis for plotting PRFs.\n",
    "    - It sets Is_FIR to True.\n",
    "- For *non-FIR regressors (like const):*\n",
    "    - It sets Event_Type to the regressor name itself.\n",
    "    - Lag_Index and Lag_ms are set to np.nan.\n",
    "    - Is_FIR is set to False.\n",
    "- The function now takes the whole Series of regressor names and returns a DataFrame with the new parsed columns, which is then joined back to final_betas_df. This is generally more efficient than using .apply(pd.Series) for row-wise operations that create multiple new columns.\n",
    "- Finally, it prints snippets and unique event types to help verify the parsing.\n"
   ],
   "id": "7202bdfa30bb9678"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Step 3.1: Concatenate Betas from all GLM runs ---\n",
    "\n",
    "if not all_betas_list: # Check if the list is empty\n",
    "    print(\"The list 'all_betas_list' is empty. No betas were collected from GLM runs.\")\n",
    "    print(\"Please check the output of the previous processing loop (Step 2.2) for errors or warnings.\")\n",
    "    raise RuntimeError(\"No betas collected, cannot proceed with Phase 3.\")\n",
    "else:\n",
    "    try:\n",
    "        final_betas_df = pd.concat(all_betas_list, ignore_index=True)\n",
    "        print(\"\\n--- Step 3.1: All Betas Concatenated ---\")\n",
    "        print(f\"Shape of final_betas_df: {final_betas_df.shape}\")\n",
    "        print(\"Snippet of final_betas_df (head):\")\n",
    "        print(final_betas_df.head())\n",
    "        print(\"\\nSnippet of final_betas_df (tail):\")\n",
    "        print(final_betas_df.tail())\n",
    "    except Exception as e:\n",
    "        print(f\"Error during concatenation of betas: {e}\")\n",
    "        final_betas_df = pd.DataFrame() # Ensure it exists but is empty on error\n",
    "\n",
    "\n",
    "# --- Step 3.2: Parse Regressor Names to Extract Event Type and Lag Information ---\n",
    "if not final_betas_df.empty:\n",
    "    def parse_regressor_name(reg_name_series):\n",
    "        \"\"\"\n",
    "        Parses a Series of regressor names to extract Event_Type, Lag_Index, Lag_ms, and Is_FIR.\n",
    "        Handles 'const' and other non-FIR regressors gracefully.\n",
    "        \"\"\"\n",
    "        # Initialize lists to store parsed components\n",
    "        event_types = []\n",
    "        lag_indices = []\n",
    "        lag_ms_values = []\n",
    "        is_fir_flags = []\n",
    "\n",
    "        for reg_name in reg_name_series:\n",
    "            if isinstance(reg_name, str) and '_fir_lag' in reg_name:\n",
    "                try:\n",
    "                    parts = reg_name.split('_fir_lag')\n",
    "                    event_type = parts[0]\n",
    "                    lag_info = parts[1].split('_') # e.g., \"00_0ms\" -> [\"00\", \"0ms\"]\n",
    "                    lag_idx = int(lag_info[0])\n",
    "                    lag_ms = int(lag_info[1].replace('ms', ''))\n",
    "\n",
    "                    event_types.append(event_type)\n",
    "                    lag_indices.append(lag_idx)\n",
    "                    lag_ms_values.append(lag_ms)\n",
    "                    is_fir_flags.append(True)\n",
    "                except Exception as e: # Catch any parsing errors for specific strings\n",
    "                    # print(f\"Warning: Could not parse FIR regressor name '{reg_name}': {e}\")\n",
    "                    event_types.append(reg_name) # Keep original name as event type\n",
    "                    lag_indices.append(np.nan)\n",
    "                    lag_ms_values.append(np.nan)\n",
    "                    is_fir_flags.append(False) # Mark as not a successfully parsed FIR\n",
    "            else:\n",
    "                # Not an FIR regressor (e.g., 'const' or other nuisance)\n",
    "                event_types.append(reg_name) # Use the regressor name itself as the event type\n",
    "                lag_indices.append(np.nan)\n",
    "                lag_ms_values.append(np.nan)\n",
    "                is_fir_flags.append(False)\n",
    "\n",
    "        return pd.DataFrame({\n",
    "            'Event_Type': event_types,\n",
    "            'Lag_Index': lag_indices,\n",
    "            'Lag_ms': lag_ms_values,\n",
    "            'Is_FIR': is_fir_flags\n",
    "        })\n",
    "\n",
    "    # Apply the parsing function. It returns a DataFrame, so we join it.\n",
    "    parsed_info_df = parse_regressor_name(final_betas_df['Regressor'])\n",
    "    final_betas_df = final_betas_df.join(parsed_info_df)\n",
    "\n",
    "    print(\"\\n--- Step 3.2: Final Betas DF with Parsed Regressor Info ---\")\n",
    "    print(f\"Shape after parsing: {final_betas_df.shape}\")\n",
    "    print(\"Snippet of final_betas_df with new columns (head):\")\n",
    "    print(final_betas_df.head())\n",
    "\n",
    "    # Verify: Show unique event types and some FIR betas\n",
    "    print(\"\\nUnique parsed Event_Types:\")\n",
    "    print(final_betas_df['Event_Type'].unique())\n",
    "\n",
    "    print(\"\\nExample of FIR betas (first few rows where Is_FIR is True):\")\n",
    "    print(final_betas_df[final_betas_df['Is_FIR'] == True].head())\n",
    "\n",
    "    print(\"\\nExample of non-FIR betas (e.g., const):\")\n",
    "    print(final_betas_df[final_betas_df['Is_FIR'] == False].head())\n",
    "else:\n",
    "    if 'final_betas_df' not in locals(): # Check if it was even defined\n",
    "         print(\"Variable 'final_betas_df' was not created. Skipping parsing.\")\n",
    "    else: # It was defined but is empty\n",
    "         print(\"'final_betas_df' is empty. Skipping parsing.\")"
   ],
   "id": "8832b2475902a512",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualize Estimated Pupil Response Functions (PRFs)",
   "id": "fc6a7460ceca51a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filter out non-FIR regressors (like 'const') for these plots\n",
    "fir_betas_to_plot_df = final_betas_df[final_betas_df['Is_FIR'] == True].copy()\n",
    "# Ensure Lag_ms is numeric for plotting\n",
    "fir_betas_to_plot_df['Lag_ms'] = pd.to_numeric(fir_betas_to_plot_df['Lag_ms'], errors='coerce')\n",
    "fir_betas_to_plot_df.dropna(subset=['Lag_ms'], inplace=True) # Drop if any Lag_ms became NaN\n",
    "\n",
    "# Group the event types for better interpretability:\n",
    "unique_event_types = sorted(fir_betas_to_plot_df['Event_Type'].unique())\n",
    "# You might want a custom order:\n",
    "desired_event_order = [et for et in ['Standard_attend', 'CommOdd_attend', 'RareOdd_attend',\n",
    "                                     'Standard_divert', 'CommOdd_divert', 'RareOdd_divert'] if et in unique_event_types]\n",
    "\n",
    "# --- Plot 1: Averaged PRF for each Event_Type across all sessions for this Subject/Run ---\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.lineplot(\n",
    "    data=fir_betas_to_plot_df,\n",
    "    x='Lag_ms',\n",
    "    y='Beta',\n",
    "    hue='Event_Type',\n",
    "    hue_order=desired_event_order,\n",
    "    errorbar='se', # Standard error across sessions (since we have multiple sessions per Event_Type)\n",
    "    linewidth=1.5\n",
    ")\n",
    "\n",
    "# Format the R2 / AdjR2 values into a string\n",
    "r2_text = \"R2 / AdjR2\\n\"\n",
    "used_cons = []\n",
    "for session_con, (r2, adj_r2) in (zip(session_orders, model_r_values)):\n",
    "    if session_con not in used_cons:\n",
    "        i=1\n",
    "    else:\n",
    "        i=2\n",
    "    r2_text += f\"{session_con.title()} {i}: {r2:.3f} / {adj_r2:.3f}\\n\"\n",
    "    used_cons.append(session_con)\n",
    "\n",
    "# Add R2 / AdjR2 text box\n",
    "plt.text(\n",
    "    x=1.07, y=0.5, s=r2_text,\n",
    "    transform=plt.gca().transAxes,  # position relative to axes\n",
    "    fontsize=12,\n",
    "    verticalalignment='center',\n",
    "    bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgrey', alpha=0.8)\n",
    ")\n",
    "\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=1.5, label='Baseline (0 Beta)')\n",
    "plt.axvline(0, color='green', linestyle=':', linewidth=1, label='Event Onset (Lag 0ms)')\n",
    "\n",
    "plt.title(f'Estimated PRFs (Averaged Across Sessions)\\nSubject {PARTICIPANT_ID}, Run {RUN}', fontsize=16)\n",
    "plt.xlabel('Time Lag from Event Onset (ms)', fontsize=14)\n",
    "plt.ylabel('Beta Coefficient (Change in Pupil Signal)', fontsize=14)\n",
    "plt.legend(title='Event Type', bbox_to_anchor=(1.05, 1), loc='upper left', title_fontsize='13', fontsize='11')\n",
    "plt.grid(True, linestyle=':', alpha=0.6)\n",
    "plt.tight_layout(rect=[0, 0, 0.82, 1]) # Adjust layout to make space for legend (may need tuning)\n",
    "\n",
    "combined_betas_output_path = os.path.join(image_dir, f\"combo_R{RUN}.png\")\n",
    "plt.savefig(combined_betas_output_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 2: PRFs Faceted by original Session_Processed_Condition ---\n",
    "\n",
    "def get_base_event_and_attention(event_type_str):\n",
    "    \"\"\"\n",
    "    Parses an event type string to extract its base event name and attention condition.\n",
    "\n",
    "    Assumes the event type string is formatted like \"BaseEvent_attentioncondition\"\n",
    "    (e.g., \"Standard_attend\", \"CommOdd_divert\"). If the input string does not\n",
    "    contain \"_attend\" or \"_divert\", or is not a string, it attempts to handle\n",
    "    it gracefully by returning the original string as the base event and NaN or\n",
    "    'unknown' for the attention condition.\n",
    "\n",
    "    This function is typically used with pandas .apply() on a Series of event type strings.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    event_type_str : str or any\n",
    "        The string representing the event type, potentially including an attention\n",
    "        condition suffix like \"_attend\" or \"_divert\". If not a string, it's\n",
    "        returned as the 'Base_Event'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.Series\n",
    "        A pandas Series with two elements:\n",
    "        - 'Base_Event': The extracted base name of the event (e.g., \"Standard\", \"CommOdd\").\n",
    "        - 'Attention_From_Event': The extracted attention condition (\"attend\", \"divert\",\n",
    "                                   or \"unknown\"/np.nan if not parsable).\n",
    "    \"\"\"\n",
    "    if not isinstance(event_type_str, str):\n",
    "        return pd.Series({'Base_Event': event_type_str, 'Attention_From_Event': np.nan})\n",
    "\n",
    "    if '_attend' in event_type_str:\n",
    "        return pd.Series({'Base_Event': event_type_str.replace('_attend', ''), 'Attention_From_Event': 'attend'})\n",
    "\n",
    "    elif '_divert' in event_type_str:\n",
    "        return pd.Series({'Base_Event': event_type_str.replace('_divert', ''), 'Attention_From_Event': 'divert'})\n",
    "\n",
    "    else: # Should not happen with current naming\n",
    "        return pd.Series({'Base_Event': event_type_str, 'Attention_From_Event': 'unknown'})\n",
    "\n",
    "\n",
    "\n",
    "parsed_event_components = fir_betas_to_plot_df['Event_Type'].apply(get_base_event_and_attention)\n",
    "fir_betas_to_plot_df = fir_betas_to_plot_df.join(parsed_event_components)\n",
    "\n",
    "attention_conditions = sorted(fir_betas_to_plot_df['Attention_From_Event'].unique())\n",
    "if set(attention_conditions) == {'attend', 'divert'}:\n",
    "    color_palette = {\"attend\": \"green\", \"divert\": \"purple\"}\n",
    "elif \"attend\" in attention_conditions and \"unknown\" in attention_conditions and len(attention_conditions) == 2: # Handle if only one real condition exists\n",
    "    color_palette = {\"attend\": \"green\", \"unknown\": \"grey\"}\n",
    "elif \"divert\" in attention_conditions and \"unknown\" in attention_conditions and len(attention_conditions) == 2:\n",
    "    color_palette = {\"divert\": \"purple\", \"unknown\": \"grey\"}\n",
    "else: # Default if more/other conditions\n",
    "    color_palette = sns.color_palette(n_colors=len(attention_conditions))\n",
    "\n",
    "if 'Base_Event' in fir_betas_to_plot_df.columns and 'Attention_From_Event' in fir_betas_to_plot_df.columns:\n",
    "    plt.close('all')\n",
    "    plt.figure(figsize=(12, 10)) # Adjust if too many facets or for single column\n",
    "    g = sns.FacetGrid(\n",
    "        data=fir_betas_to_plot_df,\n",
    "        row='Base_Event', # One row per base event type\n",
    "        #col='Attention_From_Event', # One col per attention type\n",
    "        height=4, aspect=2.5,\n",
    "        sharey=True, # Keep y-axis consistent for comparison\n",
    "        margin_titles=True\n",
    "    )\n",
    "    # Map the lineplot, but tell it NOT to create a legend on each facet.\n",
    "    # The `hue` and `palette` are passed here directly to lineplot.\n",
    "    g.map_dataframe(\n",
    "        sns.lineplot,\n",
    "        x='Lag_ms',\n",
    "        y='Beta',\n",
    "        hue='Attention_From_Event', # Lineplot handles the hue\n",
    "        hue_order=attention_conditions,\n",
    "        palette=color_palette,\n",
    "        errorbar='se',\n",
    "        linewidth=1.5,\n",
    "        legend=False # IMPORTANT: Prevent lineplot from drawing its own legend per facet\n",
    "    )\n",
    "\n",
    "    # Map the reference lines (these should not generate legend entries by default if no label is given)\n",
    "    g.map(plt.axhline, y=0, color='black', linestyle='--', linewidth=1)\n",
    "    g.map(plt.axvline, x=0, color='grey', linestyle=':', linewidth=1)\n",
    "\n",
    "    g.set_axis_labels('Time Lag from Event Onset (ms)', 'Beta Coefficient (Change in Pupil Signal)')\n",
    "    g.set_titles(row_template=\"{row_name}\", col_template=\"{col_name}\")\n",
    "\n",
    "    # --- Create a single, consolidated legend for the figure ---\n",
    "    # We need to get handles and labels. Since lineplot created them based on hue,\n",
    "    # we can usually grab them from the first axes that has plotted data.\n",
    "    # However, a more robust way with FacetGrid is to make dummy plots for legend handles.\n",
    "\n",
    "    handles = []\n",
    "    labels = []\n",
    "    for i, condition_name in enumerate(attention_conditions):\n",
    "        handles.append(plt.Line2D([0], [0], color=color_palette[condition_name] if isinstance(color_palette, dict) else color_palette[i], lw=1.5))\n",
    "        labels.append(condition_name)\n",
    "\n",
    "    if handles: # Only add legend if there are handles\n",
    "        g.fig.legend(\n",
    "            handles,\n",
    "            labels,\n",
    "            title=\"Attention Condition\",\n",
    "            loc='upper right', # Experiment with location\n",
    "            title_fontsize='13',\n",
    "            fontsize='11'\n",
    "        )\n",
    "\n",
    "    plt.suptitle(f'Estimated PRFs by Base Event and Attention\\nSubject {PARTICIPANT_ID}, Run {RUN}', y=1.04, fontsize=16)\n",
    "\n",
    "    # Adjust layout to accommodate suptitle and potentially the external legend\n",
    "    # If legend is outside, you may need to reduce the 'right' parameter in rect\n",
    "    condition_betas_output_path = os.path.join(image_dir, f\"conditions_R{RUN}.png\")\n",
    "    plt.savefig(condition_betas_output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.tight_layout() # Example: rect=[0,0,0.85,0.97] if legend is far right\n",
    "    plt.show()"
   ],
   "id": "3d74a81f1d523b49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Summarise the betas over chosen epoch for analyses",
   "id": "2b8ec090f378d307"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Step 3.4: Summarize Betas for 2nd Level Analysis ---\n",
    "\n",
    "if 'final_betas_df' not in locals() or final_betas_df.empty:\n",
    "    raise ValueError(\"DataFrame 'final_betas_df' is not available or is empty. Cannot summarize betas.\")\n",
    "else:\n",
    "    # Filter for FIR betas only, as 'const' and boxcars don't have lags to average over in this way\n",
    "    fir_betas_for_summary_df = final_betas_df[final_betas_df['Is_FIR'] == True].copy()\n",
    "\n",
    "    print(\"\\n--- Summarizing FIR Betas into Analysis Windows ---\")\n",
    "\n",
    "    # --- 1. Cognitive Window ---\n",
    "    cog_window_start_ms = 1250\n",
    "    cog_window_end_ms = 3000 # Inclusive of the start of the bin at 3000ms if fir_bin_width allows\n",
    "                              # Our last FIR bin starts at 2800ms for a 3000ms window and 200ms bin width.\n",
    "                              # So we should filter Lag_ms <= (cog_window_end_ms - fir_bin_width_ms)\n",
    "                              # or more simply, Lag_ms < cog_window_end_ms if lags are start of bin\n",
    "                              # Let's ensure we capture the bin starting at 2800ms.\n",
    "                              # If fir_bin_width_ms is 200, the bin starting at 2800ms covers 2800-2999ms.\n",
    "                              # We need to be careful how `cog_window_end_ms` is used with `Lag_ms`.\n",
    "                              # `Lag_ms` is the START of the FIR bin.\n",
    "                              # So, we want lags where:\n",
    "                              # Lag_ms >= cog_window_start_ms\n",
    "                              # AND Lag_ms < cog_window_end_ms (if window end is exclusive of the start of the next bin)\n",
    "                              # OR Lag_ms <= (cog_window_end_ms - fir_bin_width_ms) if window end is start of last bin\n",
    "\n",
    "    print(f\"Cognitive Window: {cog_window_start_ms}ms to {cog_window_end_ms}ms (lags included if lag_start >= start AND lag_start < end)\")\n",
    "\n",
    "    betas_in_cog_window = fir_betas_for_summary_df[\n",
    "        (fir_betas_for_summary_df['Lag_ms'] >= cog_window_start_ms) &\n",
    "        (fir_betas_for_summary_df['Lag_ms'] < cog_window_end_ms) # Captures bins starting up to, but not including, 3000ms\n",
    "    ]\n",
    "\n",
    "    if betas_in_cog_window.empty:\n",
    "        print(\"Warning: No FIR betas found within the specified Cognitive Window.\")\n",
    "        summarized_betas_cognitive = pd.DataFrame()\n",
    "    else:\n",
    "        # Group by original identifiers and the parsed Event_Type, then mean of Beta\n",
    "        summarized_betas_cognitive = betas_in_cog_window.groupby(\n",
    "            ['Subject', 'Run', 'Session', 'Session_Processed_Condition', 'Event_Type']\n",
    "        )['Beta'].mean().reset_index()\n",
    "        summarized_betas_cognitive.rename(columns={'Beta': f'Mean_Beta_Cognitive_{cog_window_start_ms}-{cog_window_end_ms}ms'}, inplace=True)\n",
    "\n",
    "    # --- 2. PLR Window (for secondary analysis) ---\n",
    "    plr_window_start_ms = 200\n",
    "    plr_window_end_ms = 1200 # Captures bins starting up to, but not including, 1200ms\n",
    "\n",
    "    print(f\"\\nPLR Window: {plr_window_start_ms}ms to {plr_window_end_ms}ms (lags included if lag_start >= start AND lag_start < end)\")\n",
    "\n",
    "    betas_in_plr_window = fir_betas_for_summary_df[\n",
    "        (fir_betas_for_summary_df['Lag_ms'] >= plr_window_start_ms) &\n",
    "        (fir_betas_for_summary_df['Lag_ms'] < plr_window_end_ms)\n",
    "    ]\n",
    "\n",
    "    if betas_in_plr_window.empty:\n",
    "        print(\"Warning: No FIR betas found within the specified PLR Window.\")\n",
    "        summarized_betas_plr = pd.DataFrame()\n",
    "    else:\n",
    "        summarized_betas_plr = betas_in_plr_window.groupby(\n",
    "            ['Subject', 'Run', 'Session', 'Session_Processed_Condition', 'Event_Type']\n",
    "        )['Beta'].mean().reset_index()\n",
    "        summarized_betas_plr.rename(columns={'Beta': f'Mean_Beta_PLR_{plr_window_start_ms}-{plr_window_end_ms}ms'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Display unique event types in the cognitive summary to check\n",
    "if 'summarized_betas_cognitive' in locals() and not summarized_betas_cognitive.empty:\n",
    "    print(\"\\nUnique Event_Types in Cognitive Summary:\")\n",
    "    print(summarized_betas_cognitive['Event_Type'].unique())"
   ],
   "id": "af8e4ac12912aa2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\nSummarized Betas (Cognitive Window):\")\n",
    "summarized_betas_cognitive"
   ],
   "id": "92dba46eaf2e2ecb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\nSummarized Betas (PLR Window):\")\n",
    "div = summarized_betas_plr[summarized_betas_plr[\"Session_Processed_Condition\"] == \"divert\"]\n",
    "div[div.Event_Type == \"CommOdd_divert\"]"
   ],
   "id": "7a5ddb6f9dab9f3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "output_dir = f\"./data/decon_betas_summaries/{PARTICIPANT_ID}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- Save Cognitive Window Betas ---\n",
    "if 'summarized_betas_cognitive' in locals() and not summarized_betas_cognitive.empty:\n",
    "    cog_filename = f\"{PARTICIPANT_ID}_R{RUN}_betas_cognitive_window.csv\"\n",
    "    cog_filepath = os.path.join(output_dir, cog_filename)\n",
    "    try:\n",
    "        summarized_betas_cognitive.to_csv(cog_filepath, index=False)\n",
    "        print(f\"\\nSuccessfully saved summarized COGNITIVE window betas to: {cog_filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving cognitive window betas to {cog_filepath}: {e}\")\n",
    "else:\n",
    "    print(\"\\n'summarized_betas_cognitive' DataFrame not found or is empty. Nothing to save for cognitive window.\")\n",
    "\n",
    "# --- Save PLR Window Betas ---\n",
    "if 'summarized_betas_plr' in locals() and not summarized_betas_plr.empty:\n",
    "    plr_filename = f\"{PARTICIPANT_ID}_R{RUN}_betas_plr_window.csv\"\n",
    "    plr_filepath = os.path.join(output_dir, plr_filename)\n",
    "    try:\n",
    "        summarized_betas_plr.to_csv(plr_filepath, index=False)\n",
    "        print(f\"Successfully saved summarized PLR window betas to: {plr_filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving PLR window betas to {plr_filepath}: {e}\")\n",
    "else:\n",
    "    print(\"\\n'summarized_betas_plr' DataFrame not found or is empty. Nothing to save for PLR window.\")\n",
    "\n",
    "print(\"\\n--- End of processing for this participant/run ---\")"
   ],
   "id": "d722dd624458baae",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
