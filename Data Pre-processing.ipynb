{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import preprocessing_config as config\n",
    "import random\n",
    "from scipy.signal import butter, filtfilt"
   ],
   "id": "9c85197cdcbb78a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "PARTICIPANT_ID = \"ffff\"\n",
    "participant_id = PARTICIPANT_ID\n",
    "run = 1\n",
    "\n",
    "# Set to True to produce data for deconvolution rather than using a running baseline method:\n",
    "DECONVOLUTION_OUTPUT = True\n",
    "\n",
    "PRODUCE_BASELINES = not DECONVOLUTION_OUTPUT"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load Data:\n",
    "test_df = pd.read_csv(f\"./data/processed_data/{participant_id}S{run}.csv\", index_col=0, low_memory=False)"
   ],
   "id": "7b81c68a7b9d1b06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a test_df that only contains the metric data - This is necessary to remove some of the overlapping timestamps due to the inclusion of message data:\n",
    "test_metrics_df = test_df[test_df[\"Message Type\"] == \"Data\"].copy()\n",
    "\n",
    "# Create a test_df that only contains the messages:\n",
    "test_messages_df = test_df[test_df[\"Message Type\"] != \"Data\"].copy()\n",
    "\n",
    "# Delete the original dfs as they are no longer useful\n",
    "del test_df"
   ],
   "id": "5097dd4074d4ff4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_metrics_df.size",
   "id": "2b60091dbbf37529",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Ensure all timepoints are covered without breaks.\n",
    "The eye-tracker, despite being set to 1000Hz, may have skipped an entry or two. We should check this, and interpolate the data if it did.\n",
    "\n",
    "However, there are some occasions where missing data is expected. For instance, each run should have 3 series of missing values representing the time of non-recording between the 4 sessions of a run. As such, we need to investigate these manually.\n",
    "\n",
    "### CHECK THE OUTPUTS OF THE CELL BELOW:\n",
    "The output will tell you whether there is missing data that is not explained by the design of the experiment itself (There will always be some missing data, but some of it is expected, such as during breaks and adaptation periods)."
   ],
   "id": "143185c5cbd7752e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "min_ts = test_metrics_df['Timestamp'].min()\n",
    "max_ts = test_metrics_df['Timestamp'].max()\n",
    "\n",
    "expected_timestamps = pd.Series(range(min_ts, max_ts + 1), name=\"ExpectedTimestamp\")\n",
    "actual_timestamps = test_metrics_df['Timestamp']\n",
    "\n",
    "missing_mask = ~expected_timestamps.isin(actual_timestamps)\n",
    "missing_timestamps = expected_timestamps[missing_mask]\n",
    "\n",
    "columns_of_interest = [\"Timestamp\", \"Message Type\", \"Block\", \"Trial\"] # Used for checking the session_end series are present (see above note)\n",
    "\n",
    "if not missing_timestamps.empty:\n",
    "    print(f\"Total individual missing timestamps: {len(missing_timestamps)}\")\n",
    "\n",
    "    # --- Find consecutive series of missing timestamps ---\n",
    "\n",
    "    # 1. Calculate the difference between consecutive missing timestamps\n",
    "    #    A 'break' in a consecutive series of missing items occurs when diff > 1\n",
    "    #    The first item in missing_timestamps always starts a new series (or is a series of 1)\n",
    "    diffs = missing_timestamps.diff()\n",
    "\n",
    "    # 2. Identify the start of each new series of missing timestamps\n",
    "    #    A new series starts where the diff is not 1 (or for the very first missing timestamp)\n",
    "    #    We use .ne(1) which means \"not equal to 1\". For the first NaN, NaN.ne(1) is True.\n",
    "    series_starts_mask = diffs.ne(1) # True where a new series begins\n",
    "    # Alternatively, and perhaps more explicitly for the first element:\n",
    "\n",
    "    # 3. Create a series ID by taking the cumulative sum of these 'new series' markers\n",
    "    series_id = series_starts_mask.cumsum()\n",
    "\n",
    "    # 4. Group the missing timestamps by these series IDs\n",
    "    #    And aggregate to get the start, end, and count of each series\n",
    "    missing_series_summary = missing_timestamps.groupby(series_id).agg(\n",
    "        start_ts='first',\n",
    "        end_ts='last',\n",
    "        count='size'\n",
    "    )\n",
    "\n",
    "    # 5. Filter for series that have 2 or more consecutive missing values\n",
    "    consecutive_missing_series = missing_series_summary[missing_series_summary['count'] >= 2]\n",
    "\n",
    "    series_val_manual_checks = [] # Holds any series vals that are not caused by session ends, therefore needing manual checks\n",
    "\n",
    "    if not consecutive_missing_series.empty:\n",
    "        print(\"\\nReport of Consecutive Missing Timestamp Series (length >= 2):\")\n",
    "        for index, row in consecutive_missing_series.iterrows(): # series_id is the index here\n",
    "            print(f\"  - Missing from {row['start_ts']} to {row['end_ts']} (Count: {row['count']})\")\n",
    "        print(f\"\\nTotal number of such series: {len(consecutive_missing_series)}\")\n",
    "\n",
    "        print(\"\\n--- Detailed Analysis of Missing Series ---\")\n",
    "\n",
    "        for index, row in consecutive_missing_series.iterrows():\n",
    "            print(f\"\\nAnalyzing missing series from {row['start_ts']} to {row['end_ts']} (Count: {row['count']})\")\n",
    "            series_is_expected = True\n",
    "\n",
    "            # --- Check the event IMMEDIATELY BEFORE the missing series starts ---\n",
    "            # The missing series starts at row['start_ts'], so we look at row['start_ts'] - 1\n",
    "            timestamp_at_start_of_gap = row['start_ts']\n",
    "            start_event_df = test_messages_df[test_messages_df[\"Timestamp\"] == timestamp_at_start_of_gap]\n",
    "\n",
    "            if not start_event_df.empty:\n",
    "                # Get the message type of the single event before the gap\n",
    "                starting_message_type = start_event_df['Message Type'].iloc[0]\n",
    "                print(f\"  Event at the start of the gap (at {timestamp_at_start_of_gap}): '{starting_message_type}'\")\n",
    "                if starting_message_type == \"Session End\":\n",
    "                    print(\"    Context: Starts at a 'Session End'. This is expected, as it follows a break and adaptation period of no recording.\")\n",
    "                else:\n",
    "                    print(f\"    MANUAL CHECK REQUIRED - The gap did not begin when a session ended.\")\n",
    "                    series_is_expected = False\n",
    "            else:\n",
    "                print(f\"  MANUAL CHECK REQUIRED - The gap did not begin when a session ended: No corresponding event found in test_messages_df.\")\n",
    "                series_is_expected = False\n",
    "\n",
    "            # --- Check the event IMMEDIATELY AFTER the missing series ends ---\n",
    "            # The missing series ends at row['end_ts'], so we look at row['end_ts'] + 1,\n",
    "            # which should be the first trial of a session if the gap was due to a session ending.\n",
    "            timestamp_after_gap = row['end_ts'] + 1\n",
    "            event_after_df = test_messages_df[test_messages_df[\"Timestamp\"] == timestamp_after_gap][columns_of_interest].head(1) # Some timestamps have multiple messages\n",
    "\n",
    "            if not event_after_df.empty:\n",
    "                # Extract scalar values since we know it's a single row DataFrame now\n",
    "                trial_val = event_after_df[\"Trial\"].item() # .item() extracts the single value\n",
    "                block_val = event_after_df[\"Block\"].item()\n",
    "                message_type_after = event_after_df[\"Message Type\"].item()\n",
    "\n",
    "                print(f\"  Event after gap (at {timestamp_after_gap}): Type='{message_type_after}', Block={block_val}, Trial={trial_val}\")\n",
    "\n",
    "                if trial_val == 1 and block_val == 1: # Assuming Trial 1, Block 1 is the start of a session\n",
    "                    print(\"    Context: Ending point of gap is before a 'Session Start' (Block 1, Trial 1). This is expected.\")\n",
    "                else:\n",
    "                    print(f\"    MANUAL CHECK REQUIRED - Ending point of gap is before an event that is NOT Block 1, Trial 1 \"\n",
    "                          f\"(Block={block_val}, Trial={trial_val}).\")\n",
    "                    series_is_expected = False\n",
    "\n",
    "            else:\n",
    "                print(f\"  MANUAL CHECK REQUIRED - The gap didn't end at the start of a new session, so it needs to be investigated manually.\")\n",
    "                series_is_expected = False\n",
    "\n",
    "            if not series_is_expected:\n",
    "                series_val_manual_checks.append((row['start_ts'], row['end_ts']))\n",
    "\n",
    "            print(\"-\" * 40) # Separator for each series\n",
    "\n",
    "    else:\n",
    "        print(\"\\nNo consecutive missing timestamp series (length >= 2) found.\")\n",
    "\n",
    "    # Optional: Report single missing timestamps if you want to differentiate\n",
    "    single_missing = missing_series_summary[missing_series_summary['count'] == 1]\n",
    "    if not single_missing.empty and not consecutive_missing_series.empty:\n",
    "        print(f\"\\nAdditionally, there are {len(single_missing)} single isolated missing timestamps.\")\n",
    "    elif not single_missing.empty:\n",
    "         print(f\"\\nFound {len(single_missing)} single isolated missing timestamps (no consecutive series >= 2).\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"No missing integer timestamps found in the expected 1ms interval.\")\n",
    "\n",
    "single_val_manual_checks = []\n",
    "for index, row in single_missing.iterrows():\n",
    "    timestamp = row[\"start_ts\"]\n",
    "    message_type = test_messages_df[test_messages_df[\"Timestamp\"] == timestamp+1][\"Message Type\"].values[0]\n",
    "    trial_num = test_messages_df[test_messages_df[\"Timestamp\"] == timestamp+1][\"Trial\"].values[0]\n",
    "    if not (message_type == \"Trial Start\" and trial_num == 1):\n",
    "        print(f\"MANUAL CHECK REQUIRED - Missing timestamp ({timestamp}) is not caused by one block ending and another beginning\")\n",
    "        single_val_manual_checks.append(timestamp)\n",
    "\n",
    "# Report a summary of the single missing value checks:\n",
    "print(\"\\n\"*2)\n",
    "print(\"=\" * 40)\n",
    "print(\"SUMMARY:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if len(series_val_manual_checks) > 0:\n",
    "    print(f\"Series Missing Timestamps REQUIRE MANUAL CHECKS:\")\n",
    "    for timestamps in series_val_manual_checks:\n",
    "        print(f\"{timestamps[0]} - {timestamps[1]}\")\n",
    "else:\n",
    "    print(\"All series missing timestamps are EXPECTED - they are caused by the recording being paused during breaks and interim periods between sessions.\")\n",
    "\n",
    "if len(single_val_manual_checks) > 0:\n",
    "    print(f\"Single Missing Timestamps REQUIRE MANUAL CHECKS:\")\n",
    "    for timestamp in single_val_manual_checks:\n",
    "        print(timestamp)\n",
    "else:\n",
    "    print(\"All single missing timestamps are EXPECTED - they are caused by delays between a block's offset and the next block's onset.\")\n",
    "\n",
    "# Delete dfs that are no longer useful to preserve memory:\n",
    "del [start_event_df, event_after_df]\n"
   ],
   "id": "79e7f72ffc0aa577",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Flag trials where blinks cover the main stimulus for later dropping\n",
    "Flag any trials via a new column in which the blink covers the main stimulus presentation period, while adding another bool column to mark any samples containing a blink for later interpolation (after pre-processing)."
   ],
   "id": "a612ef6a06ab4af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Before dropping any trials, create a backup df for later comparison after all pre-processing steps:\n",
    "backup_df = test_metrics_df.copy()"
   ],
   "id": "5ee93304af91d0f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load in the adaptation period blinks and saccades for this run.\n",
    "# This allows us to check if there was a blink or saccade started that didn't end, bleeding into one of the sessions below:\n",
    "with open(f\"data/processed_data/final_blinks_and_saccades/{PARTICIPANT_ID}/run_{run}.pkl\", 'rb') as f: # 'rb' for read binary\n",
    "        loaded_dict = pickle.load(f)"
   ],
   "id": "c07347ad92250fc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Locate the starting and ending points of a blink - These are samples where there should be no recorded data:\n",
    "blink_periods = []\n",
    "for session in [1, 2, 3, 4]:\n",
    "    session_df = test_messages_df[test_messages_df[\"Session\"] == session]\n",
    "\n",
    "    # Get the timestamps of the blinks, both when they begin and when they end:\n",
    "    blink_starts = session_df[session_df[\"Message Type\"] == \"Blink Start\"][\"Timestamp\"].values\n",
    "    blink_ends = session_df[session_df[\"Message Type\"] == \"Blink End\"][\"Timestamp\"].values\n",
    "\n",
    "    # Add the blink padding, configured in preprocessing_config.py:\n",
    "    blink_starts -= config.BLINK_PRE_PADDING\n",
    "    blink_ends += config.BLINK_POST_PADDING\n",
    "\n",
    "    # Check if there's a hidden blink start that happened during the adaptation period ran over into the session data:\n",
    "    if loaded_dict[f\"session_{session}\"][\"blinks\"] is not None:\n",
    "        blink_starts = np.insert(blink_starts, 0, loaded_dict[f\"session_{session}\"][\"blinks\"]) # Add the value to the front of the numpy array\n",
    "\n",
    "    # Add a tuple containing the paired start and end times for each blink:\n",
    "    for start_time, end_time in zip(blink_starts, blink_ends):\n",
    "        blink_periods.append((int(start_time), int(end_time)))\n"
   ],
   "id": "f1f2522650b01f41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a 'Blink' column which tells us whether a blink was occurring at that timestamp:\n",
    "test_metrics_df[\"Blink\"] = False # Initialise the column with all False values.\n",
    "\n",
    "for blink_start_ts, blink_end_ts in blink_periods:\n",
    "    # Create a boolean mask for the current blink period:\n",
    "    is_during_blink_mask = (test_metrics_df['Timestamp'] >= blink_start_ts) & (test_metrics_df['Timestamp'] <= blink_end_ts)\n",
    "\n",
    "    # Use .loc to set 'Blink' to True for rows matching the mask\n",
    "    test_metrics_df.loc[is_during_blink_mask, \"Blink\"] = True"
   ],
   "id": "a130210fb6a18c56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filter the database to identify any rows containing blinks where the main stimulus was visible:\n",
    "contaminated_df = test_metrics_df[(test_metrics_df[\"Blink\"] == True) & (test_metrics_df[\"Main Stimulus Visibility\"] == True)]\n",
    "\n",
    "# Filter the resultant list of contaminated rows such that there's only 1 row per unique trial:\n",
    "trial_identifier_cols = [\"Run\", \"Session\", \"Block\", \"Trial\"]\n",
    "contaminated_trials_df = contaminated_df[trial_identifier_cols].drop_duplicates()\n",
    "contaminated_trials_list = [tuple(row) for row in contaminated_trials_df.itertuples(index=False)]\n",
    "\n",
    "# Convert the resultant dataframe of contaminated trials to a list of tuples in the format: (run, session, block, trial)\n",
    "print(f\"Number of blink-contaminated trials: {len(contaminated_trials_list)}\")\n",
    "\n",
    "blink_trials_removed = contaminated_trials_list\n",
    "contaminated_trials_list"
   ],
   "id": "1f88e41a8fc543e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Flag the blink-contaminated trials:\n",
    "\n",
    "# Perform a left merge with an indicator column called '_merge'. This will be 'both' if a match was found (i.e., the trial is in contaminated_trials_df), 'left_only' otherwise:\n",
    "merged_blink_df = pd.merge(\n",
    "    test_metrics_df,      # Your main data DataFrame at this stage\n",
    "    contaminated_trials_df, # DataFrame of trials contaminated by blinks during main stim\n",
    "    on=trial_identifier_cols,\n",
    "    how='left',           # Keep all rows from test_metrics_df\n",
    "    indicator=True        # Adds a column named '_merge'\n",
    ")\n",
    "\n",
    "# Create the 'Blink_During_Main_Stim' flag column based on the '_merge' column.\n",
    "# If '_merge' is 'both', it means the trial was found in contaminated_trials_df.\n",
    "merged_blink_df[\"Blink On Main Stim\"] = merged_blink_df['_merge'] == 'both'\n",
    "\n",
    "# Keep all rows and drop the temporary '_merge' column. The flag \"Blink On Main Stim\" is now part of the DataFrame.\n",
    "test_metrics_df = merged_blink_df.drop(columns=['_merge'])\n",
    "\n",
    "# Output to verify\n",
    "print(f\"Flagged {test_metrics_df[\"Blink On Main Stim\"].sum()} samples belonging to blink-contaminated trials (during main stimulus).\")\n",
    "print(f\"Number of unique trials flagged for blink during main stim: {test_metrics_df[test_metrics_df[\"Blink On Main Stim\"] == True][trial_identifier_cols].drop_duplicates().shape[0]}\")\n",
    "\n",
    "# Remove the unneeded dfs:\n",
    "del [contaminated_df, contaminated_trials_df, merged_blink_df]"
   ],
   "id": "1920b157662df66a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Flag trials where saccades cover the main stimulus for later dropping\n",
    "Flag any trials via a new column in which the saccade covers the main stimulus presentation period, while adding another bool column to mark any samples containing a saccade for later interpolation (after pre-processing)."
   ],
   "id": "a32e47aae34b7cdc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Locate the starting and ending points of a saccade.\n",
    "# These typically have data associated with them, but plotting the data usually shows artefacts in the timeseries where the pupil size\n",
    "# forms a valley. As such, they need identifying:\n",
    "saccade_periods = []\n",
    "for session in [1, 2, 3, 4]:\n",
    "    session_df = test_messages_df[test_messages_df[\"Session\"] == session]\n",
    "\n",
    "    # Get the timestamps of the saccades, both when they begin and when they end:\n",
    "    saccade_starts = session_df[session_df[\"Message Type\"] == \"Saccade Start\"][\"Timestamp\"].values\n",
    "    saccade_ends = session_df[session_df[\"Message Type\"] == \"Saccade End\"][\"Timestamp\"].values\n",
    "\n",
    "    # Add the saccade padding, configured in preprocessing_config.py:\n",
    "    saccade_starts -= config.SACCADE_PRE_PADDING\n",
    "    saccade_ends += config.SACCADE_POST_PADDING\n",
    "\n",
    "    # Check if there's a hidden saccade start that happened during the adaptation period ran over into the session data:\n",
    "    if loaded_dict[f\"session_{session}\"][\"saccades\"] is not None:\n",
    "        saccade_starts = np.insert(saccade_starts, 0, loaded_dict[f\"session_{session}\"][\"saccades\"]) # Add the value to the front of the numpy array\n",
    "\n",
    "    # Add a tuple containing the paired start and end times for each blink:\n",
    "    for start_time, end_time in zip(saccade_starts, saccade_ends):\n",
    "        saccade_periods.append((int(start_time), int(end_time)))"
   ],
   "id": "6ede312279ea1ff9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a 'Saccade' column which tells us whether a saccade was occurring at that timestamp:\n",
    "test_metrics_df[\"Saccade\"] = False # Initialise the column with all False values.\n",
    "\n",
    "for saccade_start_ts, saccade_end_ts in saccade_periods:\n",
    "    # Create a boolean mask for the current saccade period:\n",
    "    is_during_saccade_mask = (test_metrics_df['Timestamp'] >= saccade_start_ts) & (test_metrics_df['Timestamp'] <= saccade_end_ts)\n",
    "\n",
    "    # Use .loc to set 'Saccade' to True for rows matching the mask\n",
    "    test_metrics_df.loc[is_during_saccade_mask, \"Saccade\"] = True"
   ],
   "id": "afd63eb189c4303a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filter the database to identify any rows containing saccades where the main stimulus was visible:\n",
    "contaminated_df = test_metrics_df[(test_metrics_df[\"Saccade\"] == True) & (test_metrics_df[\"Main Stimulus Visibility\"] == True)]\n",
    "\n",
    "# Filter the resultant list of contaminated rows such that there's only 1 row per unique trial:\n",
    "trial_identifier_cols = [\"Run\", \"Session\", \"Block\", \"Trial\"]\n",
    "contaminated_trials_df = contaminated_df[trial_identifier_cols].drop_duplicates()\n",
    "contaminated_trials_list = [tuple(row) for row in contaminated_trials_df.itertuples(index=False)]\n",
    "\n",
    "# Convert the resultant dataframe of contaminated trials to a list of tuples in the format: (run, session, block, trial)\n",
    "print(f\"Number of saccade-contaminated trials: {len(contaminated_trials_list)}\")\n",
    "saccade_trials_removed = contaminated_trials_list\n",
    "contaminated_trials_list"
   ],
   "id": "d9ef40ab80481475",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Flag the saccade-contaminated trials:\n",
    "\n",
    "# Perform a left merge with an indicator column called '_merge'. This will be 'both' if a match was found (i.e., the trial is in contaminated_trials_df), 'left_only' otherwise:\n",
    "merged_saccade_df = pd.merge(\n",
    "    test_metrics_df,      # Your main data DataFrame at this stage\n",
    "    contaminated_trials_df, # DataFrame of trials contaminated by blinks during main stim\n",
    "    on=trial_identifier_cols,\n",
    "    how='left',           # Keep all rows from test_metrics_df\n",
    "    indicator=True        # Adds a column named '_merge'\n",
    ")\n",
    "\n",
    "# Create the 'Saccade_During_Main_Stim' flag column based on the '_merge' column.\n",
    "# If '_merge' is 'both', it means the trial was found in contaminated_trials_df.\n",
    "merged_saccade_df[\"Saccade On Main Stim\"] = merged_saccade_df['_merge'] == 'both'\n",
    "\n",
    "# Keep all rows and drop the temporary '_merge' column. The flag 'Blink_During_Main_Stim' is now part of the DataFrame.\n",
    "test_metrics_df = merged_saccade_df.drop(columns=['_merge'])\n",
    "\n",
    "# Output to verify\n",
    "print(f\"Flagged {test_metrics_df[\"Saccade On Main Stim\"].sum()} samples belonging to saccade-contaminated trials (during main stimulus).\")\n",
    "print(f\"Number of unique trials flagged for saccade during main stim: {test_metrics_df[test_metrics_df[\"Saccade On Main Stim\"] == True][trial_identifier_cols].drop_duplicates().shape[0]}\")\n",
    "\n",
    "# Remove the unneeded dfs:\n",
    "del [contaminated_df, contaminated_trials_df, merged_saccade_df]"
   ],
   "id": "d0c439818fa3b2ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# test_metrics_df[test_metrics_df[\"Saccade On Main Stim\"] == True].drop_duplicates(subset=trial_identifier_cols, keep=\"first\")",
   "id": "973880a52a2b94ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Create a 'Since Target' column for behavioural task-relevance filtering\n",
    "This allows me to screen trials later that may be confounded due to a target appearing within the period of interest, thereby affecting the pupil (potentially due to the LC's responsiveness to task-relevant stimuli).\n",
    "\n",
    "I'll decide whether to overwrite these values based on settings in preprocessing_config.py"
   ],
   "id": "a16fdd9f4eb0ed07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a temp column containing the timestamps of rows where the target was showing\n",
    "test_metrics_df[\"Target Timestamps Only\"] = test_metrics_df['Timestamp'].where(test_metrics_df['Target Status'] == True)\n",
    "\n",
    "# Each session will begin without a target having appeared, so we want these values to be NaN. For the rest, we want to forward fill the column:\n",
    "test_metrics_df[\"Last Target Timestamp\"] = test_metrics_df.groupby(\"Session\")[\"Target Timestamps Only\"].ffill()\n",
    "\n",
    "# Calculate the \"Time Since Target\" Column:\n",
    "test_metrics_df[\"Time Since Target\"] = test_metrics_df[\"Timestamp\"] - test_metrics_df[\"Last Target Timestamp\"]\n",
    "\n",
    "# Delete the temp columns:\n",
    "del test_metrics_df[\"Target Timestamps Only\"]\n",
    "del test_metrics_df[\"Last Target Timestamp\"]"
   ],
   "id": "59fd26184c33cba9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pre-Processing According to Kret & Sjak-Shie (2019)\n",
    "## Step 1: Prepare the Raw Data - Remove Blinks, Saccades, and Identified Artifacts/Confounds\n",
    "This involves handling the blinks, saccades, behavioural responses, etc that were identified above.\n",
    "\n",
    "We handle them by setting their respective samples' pupil data to NaN for now to later interpolate over:"
   ],
   "id": "b514549d392d218f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# To avoid the copy issue with Pandas, we'll set the condition separately, then use it with loc to set these vals to NaN:\n",
    "cleaned_df = test_metrics_df.copy()\n",
    "\n",
    "if not DECONVOLUTION_OUTPUT:\n",
    "    condition = (cleaned_df[\"Blink\"] == True) | (cleaned_df[\"Saccade\"] == True)\n",
    "    cleaned_df.loc[condition, \"Pupil\"] = np.nan"
   ],
   "id": "89d9a7118470b2e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Visual Check for outliers\n",
    "The plots below show the timeseries of all the trials in a session, with lines marking 3SD from the mean.\n",
    "\n",
    "There should be a reduction in large 'spikes' that extend towards the 3SD lines when we compare the cleaned version against the original. Make sure there's no obvious spikes remaining. If there are, then you need to inspect the data to see why this is:"
   ],
   "id": "f26f831601fa0796"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(12, 12), sharey=True)\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for row, df, name in zip((0, 1), (test_metrics_df, cleaned_df), (\"Original\", \"Cleaned\")):\n",
    "    for col in range(4):\n",
    "        run_data = df[df[\"Session\"] == col+1][[\"Pupil\", \"Timestamp\"]]\n",
    "        axes[col][row].plot(run_data[\"Timestamp\"], run_data[\"Pupil\"])\n",
    "        axes[col][row].set_title(f\"{name} - Session {col+1}\")\n",
    "        if session % 2 == 0:\n",
    "            axes[col][row].set_ylabel(\"Pupil\")\n",
    "\n",
    "        # Plot ref lines:\n",
    "        mean = run_data[\"Pupil\"].mean()\n",
    "        std = run_data[\"Pupil\"].std()\n",
    "\n",
    "        axes[col][row].axhline(y=mean, xmin=0, xmax=1, color='black', linestyle='--')\n",
    "        axes[col][row].axhline(y=mean+(std*3), xmin=0, xmax=1, color='red', linestyle='--')\n",
    "        axes[col][row].axhline(y=mean-(std*3), xmin=0, xmax=1, color='red', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "ed59ecbcd917c0f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2.1: Reject dilation speed outliers\n",
    "By calculating how quickly the pupil changes its area between its prior and next samples, we can detect artifacts in the pupil data.\n",
    "\n",
    "Such artifacts could be from partial eyelid closure, a slight head movement, etc.\n",
    "\n",
    "This filter detects extremely rapid changes - i.e. noise that is characterised by high-velocity change:"
   ],
   "id": "c50fb6c191fdb674"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Get the data for the PREVIOUS and NEXT samples:\n",
    "prev_pupil = cleaned_df[\"Pupil\"].shift(1)\n",
    "prev_time = cleaned_df[\"Timestamp\"].shift(1)\n",
    "next_pupil = cleaned_df[\"Pupil\"].shift(-1)\n",
    "next_time = cleaned_df[\"Timestamp\"].shift(-1)\n",
    "\n",
    "# Calculate the differences with the PREVIOUS and NEXT samples:\n",
    "delta_pupil_backward = cleaned_df[\"Pupil\"] - prev_pupil\n",
    "delta_time_backward = cleaned_df[\"Timestamp\"] - prev_time\n",
    "velocity_backward = (delta_pupil_backward / delta_time_backward).abs()\n",
    "\n",
    "delta_pupil_forward = next_pupil - cleaned_df[\"Pupil\"]\n",
    "delta_time_forward = next_time - cleaned_df[\"Timestamp\"]\n",
    "velocity_forward = (delta_pupil_forward / delta_time_forward).abs()\n",
    "\n",
    "# Store the resultant normalised dilation speeds for each sample:\n",
    "cleaned_df[\"Dilation Speed\"] = np.maximum(velocity_backward, velocity_forward)"
   ],
   "id": "ed4323786ce05a49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate the median absolute deviation (MAD):\n",
    "mad = (cleaned_df[\"Dilation Speed\"] - cleaned_df[\"Dilation Speed\"].median()).abs().median()"
   ],
   "id": "af300d81b3355e6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Explore possible constants to set n to as a threshold for rejecting samples based on the speed of dilation:\n",
    "possible_ns = []\n",
    "for possible_n in range(5, 25):\n",
    "    test_threshold = cleaned_df[\"Dilation Speed\"].median() + (possible_n * mad)\n",
    "    test_outliers = cleaned_df[cleaned_df[\"Dilation Speed\"] > test_threshold][\"Subject\"].count()\n",
    "    total_samples = cleaned_df[\"Dilation Speed\"].count()\n",
    "\n",
    "    pc_removed = ((test_outliers / total_samples) * 100).round(2)\n",
    "    if pc_removed < 1 and test_outliers >= 1:\n",
    "        possible_ns.append((possible_n, test_outliers, pc_removed))\n",
    "for possible_n, test_outliers, pc_removed in possible_ns:\n",
    "    print(f\"n = {possible_n}:\\t\\tOutliers detected: {test_outliers}\\t\\tPercentage of total:{pc_removed}\")"
   ],
   "id": "b9e79eecfb94308",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define a constant to multiply with MAD to determine the threshold for accepting/rejecting a sample. n depends on the data:\n",
    "n = 14\n",
    "\n",
    "d_speed_threshold = cleaned_df[\"Dilation Speed\"].median() + (n * mad)\n",
    "total_samples = cleaned_df[\"Dilation Speed\"].count()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 6), sharey=True)\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for session, ax in enumerate(axes_flat):\n",
    "    run_data = cleaned_df[cleaned_df[\"Session\"] == session+1][[\"Dilation Speed\", \"Timestamp\"]]\n",
    "    ax.scatter(run_data[\"Timestamp\"], run_data[\"Dilation Speed\"], s=1, color=\"black\", alpha=0.15)\n",
    "\n",
    "    ax.axhline(y=cleaned_df[\"Dilation Speed\"].median(), xmin=0, xmax=1, color='magenta', linestyle='-', label=\"Median\")\n",
    "    ax.axhline(y=d_speed_threshold, xmin=0, xmax=1, color='red', linestyle='-', label=\"Chosen Threshold\")\n",
    "\n",
    "    ax.legend(loc=\"upper right\")\n",
    "\n",
    "    ax.set_title(f\"Session {session+1}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 6), sharey=True)\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for session, ax in enumerate(axes_flat):\n",
    "    run_data = cleaned_df[cleaned_df[\"Session\"] == session+1][[\"Dilation Speed\", \"Timestamp\"]]\n",
    "    ax.hist(run_data[\"Dilation Speed\"], color=\"black\")\n",
    "\n",
    "    ax.axvline(x=cleaned_df[\"Dilation Speed\"].median(), color='magenta', linestyle='-', label=\"Median\")\n",
    "    ax.axvline(x=d_speed_threshold, color='red', linestyle='-', label=\"Chosen Threshold\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "\n",
    "    ax.set_title(f\"Session {session+1}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "45ae07ae288d576f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the raw pupil data matching some of these samples to see if they are obvious outliers:\n",
    "session = cleaned_df[cleaned_df[\"Session\"] == random.choice([1,2,3,4])]\n",
    "\n",
    "num_of_plots = 10\n",
    "window_period = 300\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=num_of_plots//2, figsize=(12, 8))\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for ax in axes_flat:\n",
    "    # Choose a random sample where the dilation speed is flagged as an outlier - get its timestamp:\n",
    "    selection = random.choice(session[session[\"Dilation Speed\"] >= d_speed_threshold][\"Timestamp\"].values)\n",
    "\n",
    "    # Get a series that's 25ms either side of the selection:\n",
    "    window = session[(session[\"Timestamp\"] >= selection - (window_period/2)) & (session[\"Timestamp\"] <= selection + (window_period/2))]\n",
    "\n",
    "    ax.plot(window[\"Timestamp\"], window[\"Pupil\"], color=\"black\")\n",
    "\n",
    "    selection_pupil_data = window[window[\"Timestamp\"] == selection][\"Pupil\"].values\n",
    "    ax.scatter(selection, selection_pupil_data, color=\"red\", s=100)\n",
    "\n",
    "    difference_in_minmax = window[\"Pupil\"].max() - window[\"Pupil\"].min()\n",
    "    ax.set_title(f\"TS:{selection}\\nRange={difference_in_minmax}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(f\"Outliers detected with a threshold of {d_speed_threshold} based on an n-value of {n}:\\n{cleaned_df[cleaned_df[\"Dilation Speed\"] > d_speed_threshold][\"Subject\"].count()}\")\n"
   ],
   "id": "5740ce54a20f1452",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Once happy with the threshold selected, move on to this cell.\n",
    "# This takes any sample with a dilation speed above this threshold and sets its pupil data to NaN:\n",
    "condition = cleaned_df[\"Dilation Speed\"] > d_speed_threshold\n",
    "cleaned_df.loc[condition, \"Pupil\"] = np.nan\n",
    "\n",
    "print(f\"Outliers detected with a threshold of {d_speed_threshold} after pruning:\\n{cleaned_df[condition & cleaned_df['Pupil'].notna()]['Pupil'].count()}\")"
   ],
   "id": "8f2ac77b39fd0540",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2.2: Perform a multi-pass deviation filter on the dilation speed data:\n",
    "This is very similar to the first dilation speed calculation and rejection we did, but this time we're widening the window of time that we're comparing against to measure the velocity of change in pupil diameter.\n",
    "\n",
    "This effectively means that we're looking for datapoints that vary considerably from an abstract 'smoothed' trend of pupil dilation. Effectively, we're screening for sustained noise - not rapidly corrected noise, potentially caused by things like the eyetracker temporarily losing signal, or a prolonged change in head position before correction.\n",
    "\n",
    "Note: The paper calls this a \"trend-line deviation\" rather than a filter.\n",
    "\n",
    "### Bug Fix:\n",
    "Running this on the continuous dataframe ~(i.e. with all sessions at once) causes the trendline to deviate from the data in the timespan between sessions, thus the first second or two of data between sessions will typically get flagged and deleted. Thus, we instead need to feed it in one session at a time."
   ],
   "id": "b30d81808bb2e86d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Multiplier for MAD (similar to 'n' for dilation speed)\n",
    "# This determines how many MADs away from the median deviation a point must be to be an outlier.\n",
    "# Need to adjust based on the outcome of the plot at the end:\n",
    "n_trend_dev = n # Set it higher to be marginally less aggressive while still trying target those prolonged spikes that remain\n",
    "\n",
    "# Number of passes for the filter. The paper suggests this can help \"reintroduce\" valid samples if a previous pass's trend line\n",
    "# was skewed by other outliers. A small number is good:\n",
    "num_passes_trend = 3\n",
    "\n",
    "# Maximum number of consecutive NaNs to interpolate (linearly) when generating the trend line.\n",
    "# This helps the smoothing function operate on more continuous data.\n",
    "# The interpolated trend line is supposed to have SMALL gaps filled, but not blinks or saccades necessarily - so don't worry\n",
    "# that blinks and saccade padding likely exceed the number below; this isn't intended to handle these gaps.\n",
    "interpolation_limit_for_trend_gen = 50\n",
    "\n",
    "# Smoothing window for the trend line (rolling median).\n",
    "# The window size should represent a duration, e.g. 151-251ms (Should be odd):\n",
    "smoothing_window_samples_for_trend = 201 # Odd number of ms for an odd window size in samples\n"
   ],
   "id": "5df902f3351bb1fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Keep track of NaNs before this specific filter\n",
    "nans_before_trend_filter = cleaned_df[\"Pupil\"].isna().sum()\n",
    "print(f\"NaNs before trend-line deviation filter: {nans_before_trend_filter}\")\n",
    "\n",
    "# Take a snapshot of the pupil data before the multi-pass filtering for plotting the comparison later:\n",
    "before_pass_ts, before_pass_pupil = cleaned_df[\"Timestamp\"], cleaned_df[\"Pupil\"].ffill()\n",
    "\n",
    "# Take another snapshot to later compare the effects of the multipass filter:\n",
    "pupil_data_before_session_filtering = cleaned_df[\"Pupil\"].copy()"
   ],
   "id": "182639dd2eb2aa10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Store the trend lines for later plotting:\n",
    "all_session_trend_lines = {}\n",
    "\n",
    "for session_id in cleaned_df[\"Session\"].unique():\n",
    "    print(f\"\\n--- Processing Session: {session_id} ---\")\n",
    "\n",
    "    # Get the slice of the main DataFrame for the current session\n",
    "    session_indices = cleaned_df[cleaned_df[\"Session\"] == session_id].index\n",
    "\n",
    "    # Store the trend line from the last effective pass for this session\n",
    "    last_effective_trend_line_for_session = None\n",
    "\n",
    "    for pass_num in range(1, num_passes_trend + 1):\n",
    "        print(f\"\\nPass {pass_num} (Session {session_id}):\")\n",
    "\n",
    "        # Create a temporary version of the pupil data from this session for trend line generation.\n",
    "        # It takes the current state of pupil data for this session from cleaned_df.\n",
    "        pupil_for_trend_generation_session = cleaned_df.loc[session_indices, \"Pupil\"].copy()\n",
    "\n",
    "        # Interpolate small gaps. This does NOT change the NaNs in cleaned_df[\"Pupil\"] yet.\n",
    "        interpolated_pupil_for_trend_session = pupil_for_trend_generation_session.interpolate(\n",
    "            method='linear',\n",
    "            limit_direction='both',\n",
    "            limit=interpolation_limit_for_trend_gen\n",
    "        )\n",
    "        # If gaps are too large for 'limit', they might still be NaN.\n",
    "        # Rolling median can handle some NaNs, but filling edges is good.\n",
    "        # Fill any remaining NaNs (e.g. at ends or very large gaps) just for the trend.\n",
    "        interpolated_pupil_for_trend_session = interpolated_pupil_for_trend_session.bfill().ffill()\n",
    "\n",
    "        # Smooth the interpolated data to get the trend line:\n",
    "        trend_line_session = interpolated_pupil_for_trend_session.rolling(\n",
    "            window=smoothing_window_samples_for_trend,\n",
    "            center=True,\n",
    "            min_periods=1\n",
    "        ).median()\n",
    "\n",
    "        # Rolling operations can create NaNs at the very start/end if min_periods isn't 1 or window is large, so we fill them:\n",
    "        trend_line_session = trend_line_session.bfill().ffill()\n",
    "\n",
    "        # Update the 'last effective trend line' in each pass\n",
    "        last_effective_trend_line_for_session = trend_line_session.copy()\n",
    "\n",
    "        # Calculate absolute deviations of the current session's actual pupil data from this session's trend line:\n",
    "        deviations_session = (cleaned_df.loc[session_indices, \"Pupil\"] - trend_line_session).abs()\n",
    "\n",
    "        # Identify outliers based on these deviations using MAD (Eqs. 2 and 3 from paper).\n",
    "        # Only consider deviations where the session's pupil data is NOT already NaN.\n",
    "        valid_deviations_session = deviations_session.dropna()\n",
    "\n",
    "        median_dev_session = valid_deviations_session.median()\n",
    "        mad_dev_session = (valid_deviations_session - median_dev_session).abs().median()\n",
    "\n",
    "        deviation_threshold_session = median_dev_session + (n_trend_dev * mad_dev_session)\n",
    "\n",
    "        print(f\"Session {session_id}, Pass {pass_num} - Median Dev: {median_dev_session:.4f}, MAD Dev: {mad_dev_session:.4f}, Threshold: {deviation_threshold_session:.4f}\")\n",
    "\n",
    "\n",
    "        # Identify samples within this session in cleaned_df that exceed this threshold.\n",
    "        # The condition must be on indices belonging to the current session.\n",
    "        # deviations_session is already indexed by session_indices.\n",
    "        outlier_condition_this_session = (deviations_session > deviation_threshold_session) & \\\n",
    "                                         (cleaned_df.loc[session_indices, \"Pupil\"].notna())\n",
    "\n",
    "        # Get the actual global indices from outlier_condition_this_session (which is a boolean Series on session_indices)\n",
    "        global_indices_to_nan_this_pass = outlier_condition_this_session[outlier_condition_this_session].index\n",
    "\n",
    "        num_outliers_this_pass = len(global_indices_to_nan_this_pass)\n",
    "        print(f\"Session {session_id}, Pass {pass_num} - Outliers identified: {num_outliers_this_pass}\")\n",
    "\n",
    "        if num_outliers_this_pass == 0:\n",
    "            print(f\"Session {session_id}, Pass {pass_num} - No new trend deviation outliers found. Stopping passes for this session.\")\n",
    "            break # Optimization: if no outliers found, subsequent passes likely won't change much\n",
    "\n",
    "        # Set these newly identified outliers in the main cleaned_df to NaN using the global indices:\n",
    "        cleaned_df.loc[global_indices_to_nan_this_pass, \"Pupil\"] = np.nan\n",
    "\n",
    "    # After all passes for a session (or early exit), store the last computed trend line for this session\n",
    "    if last_effective_trend_line_for_session is not None:\n",
    "        all_session_trend_lines[session_id] = last_effective_trend_line_for_session\n",
    "    else:\n",
    "        print(f\"Warning: No effective trend line was computed for session {session_id}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Session-wise Trend-Line Deviation Filtering Complete ---\")\n",
    "nans_after_session_filtering = cleaned_df[\"Pupil\"].isna().sum()\n",
    "\n",
    "print(f\"NaNs before session-wise trend-line filter: {nans_before_trend_filter}\")\n",
    "print(f\"NaNs after session-wise trend-line filter: {nans_after_session_filtering}\")\n",
    "print(f\"Samples marked NaN by this session-wise filter: {nans_after_session_filtering - nans_before_trend_filter}\")\n",
    "\n",
    "# For plotting the final trend line (concatenated from all sessions):\n",
    "if all_session_trend_lines:\n",
    "    sorted_session_ids = sorted(all_session_trend_lines.keys())\n",
    "    final_concatenated_trend_line = pd.concat(\n",
    "        [all_session_trend_lines[sid] for sid in sorted_session_ids]\n",
    "    )\n",
    "else:\n",
    "    print(\"Warning: No session trend lines were stored.\")\n",
    "    final_concatenated_trend_line = pd.Series(index=cleaned_df.index, dtype=float)"
   ],
   "id": "856b468b82933e0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd # Ensure pandas is imported if not already\n",
    "\n",
    "# --- Plot 1: Pupil Data Before Filter vs. Final (Concatenated) Trend Line ---\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# 'pupil_data_before_session_filtering' is the state of pupil data *before* the session-wise filter.\n",
    "# We need to ffill it for continuous plotting, similar to how 'before_pass_pupil' was created previously.\n",
    "# The timestamps for this should align with cleaned_df[\"Timestamp\"].\n",
    "pupil_before_plot = pupil_data_before_session_filtering.ffill() # ffill for plotting continuity\n",
    "\n",
    "plt.plot(cleaned_df[\"Timestamp\"], pupil_before_plot,\n",
    "         label='Pupil Data Before Session-wise Trend Filter (NaNs ffilled for plot)', color=\"cyan\", alpha=0.7, zorder=1, linewidth=1.5)\n",
    "\n",
    "# 'final_concatenated_trend_line' is the trend line from the session-wise processing\n",
    "plt.plot(cleaned_df[\"Timestamp\"], final_concatenated_trend_line,\n",
    "         label='Final Concatenated Trend Line (Last Pass per Session)', color='red', alpha=0.7, zorder=2)\n",
    "\n",
    "plt.xlabel(\"Timestamp\")\n",
    "plt.ylabel(\"Pupil Size\")\n",
    "plt.title(\"Pupil Data Before Session-wise Trend Filter vs. Final Concatenated Trend Line\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True, linestyle=':', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Plot 2: Pupil Data Before vs. After Session-wise Trend Filter ---\n",
    "#    (No ffill, showing actual NaNs as gaps)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Pupil data *before* this specific session-wise filter (shows original NaNs for this stage)\n",
    "plt.plot(cleaned_df[\"Timestamp\"], pupil_data_before_session_filtering, label=\"Before\", color=\"green\")\n",
    "\n",
    "# Plot 2: Pupil data *after* this specific session-wise filter (shows NaNs added by this filter as well)\n",
    "plt.plot(cleaned_df[\"Timestamp\"], cleaned_df[\"Pupil\"], label=\"After\", color=\"red\")\n",
    "\n",
    "plt.xlabel(\"Timestamp\")\n",
    "plt.ylabel(\"Pupil Size\")\n",
    "plt.title(\"Pupil Data: Before vs. After Session-wise Trend Filter\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True, linestyle=':', alpha=0.7)\n",
    "plt.show()"
   ],
   "id": "a222bee097525e61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Visualization of Randomly Sampled Changes from Session-Wise Multi-Pass Trend Filter ---\n",
    "\n",
    "# Number of random segments to plot\n",
    "num_plots_to_sample = 5 # Adjust as needed\n",
    "# Time window around the identified NaN'd point (in milliseconds)\n",
    "# I'll keep your 50ms here, but consider increasing it to e.g., 250ms or 500ms for better context.\n",
    "plot_window_ms = 50\n",
    "\n",
    "# Ensure pupil_data_before_session_filtering and final_concatenated_trend_line exist\n",
    "if 'pupil_data_before_session_filtering' not in locals() or \\\n",
    "   'final_concatenated_trend_line' not in locals():\n",
    "    print(\"CRITICAL ERROR: 'pupil_data_before_session_filtering' or 'final_concatenated_trend_line' not found.\")\n",
    "    print(\"Please ensure you have run the session-wise trend filtering cell completely.\")\n",
    "    # Consider raising an error or exiting if these are missing.\n",
    "    # For now, this script might fail or produce incorrect plots if they are missing.\n",
    "    # raise NameError(\"Required data for plotting is missing. Rerun previous cells.\")\n",
    "\n",
    "\n",
    "# Candidate indices are points that were valid *before* this session-wise filter\n",
    "# but are NaN in cleaned_df *after* it.\n",
    "candidate_indices = cleaned_df[\n",
    "    pupil_data_before_session_filtering.notna() & cleaned_df[\"Pupil\"].isna()\n",
    "].index\n",
    "\n",
    "print(f\"\\n--- Plotting {num_plots_to_sample} Segments Randomly Sampled from those NaN'd by Session-Wise Filter ---\")\n",
    "if len(candidate_indices) == 0:\n",
    "    print(\"No points found that were specifically NaN'd by the session-wise multi-pass filter.\")\n",
    "else:\n",
    "    if len(candidate_indices) < num_plots_to_sample:\n",
    "        print(f\"Warning: Fewer than {num_plots_to_sample} samples were NaN'd. Plotting all {len(candidate_indices)}.\")\n",
    "        sampled_indices = candidate_indices\n",
    "    else:\n",
    "        sampled_indices = random.sample(list(candidate_indices), num_plots_to_sample)\n",
    "\n",
    "    fig_mp_session, axes_mp_session = plt.subplots(nrows=num_plots_to_sample, ncols=1,\n",
    "                                                   figsize=(15, 5 * num_plots_to_sample), sharex=False)\n",
    "    if num_plots_to_sample == 1: # Make axes_mp_session iterable if only one plot\n",
    "        axes_mp_session = [axes_mp_session]\n",
    "\n",
    "    for i, random_idx in enumerate(sampled_indices):\n",
    "        ax = axes_mp_session[i]\n",
    "        center_timestamp = cleaned_df.loc[random_idx, \"Timestamp\"]\n",
    "\n",
    "        min_ts = center_timestamp - plot_window_ms\n",
    "        max_ts = center_timestamp + plot_window_ms\n",
    "\n",
    "        # Get window indices from the main cleaned_df based on timestamps\n",
    "        window_df_indices = cleaned_df[\n",
    "            (cleaned_df[\"Timestamp\"] >= min_ts) & (cleaned_df[\"Timestamp\"] <= max_ts)\n",
    "        ].index\n",
    "\n",
    "        # Ensure we have some data in the window\n",
    "        if window_df_indices.empty:\n",
    "            print(f\"Warning: Window around index {random_idx} (Timestamp {center_timestamp}) is empty. Skipping plot.\")\n",
    "            ax.set_title(f\"Empty window around Timestamp: {center_timestamp:.0f} ms (Index: {random_idx})\")\n",
    "            ax.text(0.5, 0.5, \"No data in window\", ha='center', va='center')\n",
    "            continue\n",
    "\n",
    "        timestamps_window = cleaned_df.loc[window_df_indices, \"Timestamp\"]\n",
    "\n",
    "        # Data *before* this specific session-wise multi-pass filter\n",
    "        pupil_before_filter_window = pupil_data_before_session_filtering.loc[window_df_indices]\n",
    "\n",
    "        # Data *after* this filter (has the new NaNs)\n",
    "        pupil_after_filter_window = cleaned_df.loc[window_df_indices, \"Pupil\"]\n",
    "\n",
    "        # Trend line from the session-wise processing for this window\n",
    "        # Use final_concatenated_trend_line which should align with cleaned_df's index\n",
    "        trend_line_window = final_concatenated_trend_line.loc[window_df_indices]\n",
    "\n",
    "        # Plot 1: Pupil data *before* this session-wise filter (showing actual NaNs at this stage)\n",
    "        # Using a solid line for \"before\" and points for \"after\" can help differentiate\n",
    "        ax.plot(timestamps_window, pupil_before_filter_window,\n",
    "                label='Pupil BEFORE session-wise filter', color='green', alpha=0.7, linestyle='-', marker='o', markersize=3)\n",
    "\n",
    "        # Plot 2: Pupil data *after* this filter (with new NaNs shown as gaps)\n",
    "        ax.plot(timestamps_window, pupil_after_filter_window,\n",
    "                label='Pupil AFTER session-wise filter', color='black', alpha=0.9, marker='.', markersize=20, linestyle='None')\n",
    "\n",
    "        # Plot 3: The trend line used in the last pass (for the relevant session)\n",
    "        ax.plot(timestamps_window, trend_line_window,\n",
    "                label='Final Concatenated Trend Line', color='red', linestyle='--', alpha=0.8)\n",
    "\n",
    "        # Highlight the specific point that was selected (which is now NaN)\n",
    "        original_value_at_idx = pupil_data_before_session_filtering.loc[random_idx] # Value before this filter\n",
    "        if pd.notna(original_value_at_idx):\n",
    "             ax.scatter(center_timestamp, original_value_at_idx,\n",
    "                       color='magenta', s=100, zorder=5,\n",
    "                       label=f'Original value of this NaN point: {original_value_at_idx:.2f}')\n",
    "\n",
    "        # Calculate deviation for the specific point from the relevant trend line\n",
    "        deviation_at_point = abs(original_value_at_idx - final_concatenated_trend_line.loc[random_idx]) \\\n",
    "                             if pd.notna(original_value_at_idx) and pd.notna(final_concatenated_trend_line.loc[random_idx]) \\\n",
    "                             else float('nan')\n",
    "\n",
    "        ax.set_title(f\"Segment NaN'd by Session-Wise Filter (Index: {random_idx})\\nTS: {center_timestamp:.0f} ms | Original Dev: {deviation_at_point:.2f}\")\n",
    "        ax.set_xlabel(\"Timestamp (ms)\")\n",
    "        ax.set_ylabel(\"Pupil Size\")\n",
    "        ax.legend(loc='best')\n",
    "        ax.grid(True, linestyle=':', alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "67b89b985ff45e93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Visualization of Changes from Session-Wise Multi-Pass Trend Filter ---\n",
    "# --- AND Comparison with Data that PASSED the filter ---\n",
    "\n",
    "# Number of random segments to plot for EACH category (NaN'd and Passed)\n",
    "num_plots_to_sample = 5 # Adjust as needed\n",
    "# Time window around the identified point (in milliseconds)\n",
    "plot_window_ms = 500  # Plot before and after the point by this amount\n",
    "\n",
    "# Ensure necessary variables from the session-wise filtering are available\n",
    "if 'pupil_data_before_session_filtering' not in locals() or \\\n",
    "   'final_concatenated_trend_line' not in locals() or \\\n",
    "   'cleaned_df' not in locals():\n",
    "    print(\"CRITICAL ERROR: Required data ('pupil_data_before_session_filtering', \"\n",
    "          \"'final_concatenated_trend_line', 'cleaned_df') not found.\")\n",
    "    print(\"Please ensure you have run the session-wise trend filtering cell completely.\")\n",
    "    # Consider raising an error. For now, the script might fail later.\n",
    "    # raise NameError(\"Required data for plotting is missing. Rerun previous cells.\")\n",
    "\n",
    "\n",
    "# --- Plotting Segments NaN'd by the Session-Wise Multi-Pass Filter ---\n",
    "\n",
    "# Candidate indices are points that were valid before the session-wise filter\n",
    "# but are NaN in cleaned_df after it.\n",
    "candidate_indices_nand = cleaned_df[\n",
    "    pupil_data_before_session_filtering.notna() & cleaned_df[\"Pupil\"].isna()\n",
    "].index\n",
    "\n",
    "print(f\"\\n--- Plotting {num_plots_to_sample} Segments NaN'd by Session-Wise Multi-Pass Filter ---\")\n",
    "if len(candidate_indices_nand) == 0:\n",
    "    print(\"No points found that were specifically NaN'd by the session-wise multi-pass filter.\")\n",
    "else:\n",
    "    if len(candidate_indices_nand) < num_plots_to_sample:\n",
    "        print(f\"Warning: Fewer than {num_plots_to_sample} samples were NaN'd. Plotting all {len(candidate_indices_nand)}.\")\n",
    "        sampled_indices_nand = candidate_indices_nand\n",
    "    else:\n",
    "        sampled_indices_nand = random.sample(list(candidate_indices_nand), num_plots_to_sample)\n",
    "\n",
    "    fig_nand, axes_nand = plt.subplots(nrows=num_plots_to_sample, ncols=1,\n",
    "                                       figsize=(15, 5 * num_plots_to_sample), sharex=False)\n",
    "    if num_plots_to_sample == 1: # Make axes_nand iterable if only one plot\n",
    "        axes_nand = [axes_nand]\n",
    "\n",
    "    for i, random_idx in enumerate(sampled_indices_nand):\n",
    "        ax = axes_nand[i]\n",
    "        center_timestamp = cleaned_df.loc[random_idx, \"Timestamp\"]\n",
    "        min_ts = center_timestamp - plot_window_ms\n",
    "        max_ts = center_timestamp + plot_window_ms\n",
    "\n",
    "        window_df_indices = cleaned_df[\n",
    "            (cleaned_df[\"Timestamp\"] >= min_ts) & (cleaned_df[\"Timestamp\"] <= max_ts)\n",
    "        ].index\n",
    "\n",
    "        if window_df_indices.empty:\n",
    "            ax.set_title(f\"Empty window for NaN'd segment (Index: {random_idx}) TS: {center_timestamp:.0f} ms\")\n",
    "            ax.text(0.5, 0.5, \"No data in window\", ha='center', va='center')\n",
    "            continue\n",
    "\n",
    "        timestamps_window = cleaned_df.loc[window_df_indices, \"Timestamp\"]\n",
    "\n",
    "        # Pupil data *before* this specific session-wise multi-pass filter step\n",
    "        pupil_before_filter_window = pupil_data_before_session_filtering.loc[window_df_indices]\n",
    "\n",
    "        # Pupil data *after* this filter (has the new NaNs)\n",
    "        pupil_after_filter_window = cleaned_df.loc[window_df_indices, \"Pupil\"]\n",
    "\n",
    "        # Trend line from the session-wise processing for this window\n",
    "        trend_line_window = final_concatenated_trend_line.loc[window_df_indices]\n",
    "\n",
    "        ax.plot(timestamps_window, pupil_before_filter_window,\n",
    "                label='Pupil BEFORE session-wise filter', color='green', alpha=0.7, linestyle='-', markersize=3)\n",
    "        ax.plot(timestamps_window, pupil_after_filter_window,\n",
    "                label='Pupil AFTER session-wise filter (NaNs shown)', color='black', alpha=0.9, marker='.', markersize=5, linestyle='None')\n",
    "        ax.plot(timestamps_window, trend_line_window,\n",
    "                label='Final Concatenated Trend Line', color='red', linestyle='--', alpha=0.8)\n",
    "\n",
    "        original_value_at_idx = pupil_data_before_session_filtering.loc[random_idx]\n",
    "        if pd.notna(original_value_at_idx):\n",
    "             ax.scatter(center_timestamp, original_value_at_idx,\n",
    "                       color='magenta', s=100, zorder=5, label=f'Original: {original_value_at_idx:.2f}')\n",
    "\n",
    "        deviation_at_point = abs(original_value_at_idx - final_concatenated_trend_line.loc[random_idx]) \\\n",
    "                             if pd.notna(original_value_at_idx) and pd.notna(final_concatenated_trend_line.loc[random_idx]) \\\n",
    "                             else float('nan')\n",
    "        ax.set_title(f\"NaN'd Segment (Index: {random_idx}) TS: {center_timestamp:.0f} ms\\nOriginal Dev: {deviation_at_point:.2f}\")\n",
    "        ax.set_xlabel(\"Timestamp (ms)\")\n",
    "        ax.set_ylabel(\"Pupil Size\")\n",
    "        ax.legend(loc='best')\n",
    "        ax.grid(True, linestyle=':', alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- Plotting Segments that PASSED the Session-Wise Multi-Pass Filter ---\n",
    "\n",
    "# Candidate indices are those where pupil data is NOT NaN in the *final* cleaned_df\n",
    "# And where final_concatenated_trend_line was also computed (not NaN)\n",
    "candidate_indices_passed = cleaned_df[\n",
    "    cleaned_df[\"Pupil\"].notna() & final_concatenated_trend_line.notna()\n",
    "].index\n",
    "\n",
    "print(f\"\\n--- Plotting {num_plots_to_sample} Segments that PASSED All Filters (Session-Wise) ---\")\n",
    "if len(candidate_indices_passed) == 0:\n",
    "    print(\"No data points found that passed all filters.\")\n",
    "else:\n",
    "    if len(candidate_indices_passed) < num_plots_to_sample:\n",
    "        print(f\"Warning: Fewer than {num_plots_to_sample} samples passed. Plotting all {len(candidate_indices_passed)}.\")\n",
    "        sampled_indices_passed = candidate_indices_passed\n",
    "    else:\n",
    "        # Simpler random sampling for passed data; can add edge buffer if issues arise\n",
    "        sampled_indices_passed = random.sample(list(candidate_indices_passed), num_plots_to_sample)\n",
    "\n",
    "\n",
    "    fig_passed, axes_passed = plt.subplots(nrows=num_plots_to_sample, ncols=1,\n",
    "                                           figsize=(15, 5 * num_plots_to_sample), sharex=False)\n",
    "    if num_plots_to_sample == 1: # Make axes_passed iterable if only one plot\n",
    "        axes_passed = [axes_passed]\n",
    "\n",
    "    for i, random_idx in enumerate(sampled_indices_passed):\n",
    "        ax = axes_passed[i]\n",
    "        center_timestamp = cleaned_df.loc[random_idx, \"Timestamp\"]\n",
    "        min_ts = center_timestamp - plot_window_ms\n",
    "        max_ts = center_timestamp + plot_window_ms\n",
    "\n",
    "        window_df_indices = cleaned_df[\n",
    "            (cleaned_df[\"Timestamp\"] >= min_ts) & (cleaned_df[\"Timestamp\"] <= max_ts)\n",
    "        ].index\n",
    "\n",
    "        if window_df_indices.empty:\n",
    "            ax.set_title(f\"Empty window for Passed segment (Index: {random_idx}) TS: {center_timestamp:.0f} ms\")\n",
    "            ax.text(0.5, 0.5, \"No data in window\", ha='center', va='center')\n",
    "            continue\n",
    "\n",
    "        timestamps_window = cleaned_df.loc[window_df_indices, \"Timestamp\"]\n",
    "        pupil_data_window = cleaned_df.loc[window_df_indices, \"Pupil\"] # Final \"good\" pupil data\n",
    "        trend_line_window = final_concatenated_trend_line.loc[window_df_indices]\n",
    "\n",
    "        ax.plot(timestamps_window, pupil_data_window,\n",
    "                label='Pupil Data (Passed Filter)', color='blue', alpha=0.9, marker='.', markersize=3, linestyle='-')\n",
    "        ax.plot(timestamps_window, trend_line_window,\n",
    "                label='Final Concatenated Trend Line', color='red', linestyle='--', alpha=0.8)\n",
    "\n",
    "        value_at_idx = cleaned_df.loc[random_idx, \"Pupil\"]\n",
    "        ax.scatter(center_timestamp, value_at_idx,\n",
    "                   color='cyan', s=100, zorder=5, label=f'Selected Point: {value_at_idx:.2f}')\n",
    "\n",
    "        deviation_at_point = abs(value_at_idx - final_concatenated_trend_line.loc[random_idx]) \\\n",
    "                             if pd.notna(value_at_idx) and pd.notna(final_concatenated_trend_line.loc[random_idx]) \\\n",
    "                             else float('nan')\n",
    "        ax.set_title(f\"Passed Segment (Index: {random_idx}) TS: {center_timestamp:.0f} ms\\nActual Dev: {deviation_at_point:.2f}\")\n",
    "        ax.set_xlabel(\"Timestamp (ms)\")\n",
    "        ax.set_ylabel(\"Pupil Size\")\n",
    "        ax.legend(loc='best')\n",
    "        ax.grid(True, linestyle=':', alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "5cd8fba6e78277",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2.3: Identifying Islands (Sparsity Filter):\n",
    "Identifies 'small islands' of potentially valid but noisy-looking data surrounded by large gaps (such as between blinks and saccades) which can be indicative of noise, thereby throwing off interpolation going forward."
   ],
   "id": "a40c7b6233959f28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Parameters for identifying small islands ---\n",
    "# These likely need fine-tuning:\n",
    "MAX_ISLAND_DURATION_SAMPLES = 50  # e.g. islands of 15ms or less\n",
    "MIN_SURROUNDING_GAP_SAMPLES = 40 # e.g. surrounded by gaps of at least 75ms on each side\n",
    "\n",
    "pupil_series_for_island_removal = cleaned_df[\"Pupil\"].copy()\n",
    "\n",
    "# --- Helper function to get block information ---\n",
    "def get_block_info(series):\n",
    "    \"\"\"Identifies contiguous blocks of NaN and non-NaN data.\"\"\"\n",
    "    is_nan_series = series.isna()\n",
    "    # Find changes from NaN to non-NaN or vice-versa\n",
    "    change_points = is_nan_series.diff().ne(0) # For each value, diff subtracts the prior value in the series, .ne(0) then returns True if the result == 0.\n",
    "    # Assign a unique ID to each block\n",
    "    block_ids = change_points.cumsum() # Goes over the bool series, giving each an index that increases when a True value is seen.\n",
    "\n",
    "    blocks = []\n",
    "    for block_id, group in series.groupby(block_ids):\n",
    "        blocks.append({\n",
    "            \"id\": block_id,\n",
    "            \"is_nan_block\": group.isna().all(), # True if it's a NaN block\n",
    "            \"start_index\": group.index[0],\n",
    "            \"end_index\": group.index[-1],\n",
    "            \"duration_samples\": len(group)\n",
    "        })\n",
    "    return pd.DataFrame(blocks)\n",
    "\n",
    "\n",
    "nans_before_island_removal = pupil_series_for_island_removal.isna().sum()\n",
    "print(f\"NaNs before island removal: {nans_before_island_removal}\")\n",
    "\n",
    "# Apply per session to handle session boundaries correctly:\n",
    "indices_to_nan_from_islands = []\n",
    "for session_id in cleaned_df[\"Session\"].unique():\n",
    "    session_mask = (cleaned_df[\"Session\"] == session_id)\n",
    "    session_pupil_data = pupil_series_for_island_removal[session_mask]\n",
    "\n",
    "    # Get blocks for this session\n",
    "    session_blocks_df = get_block_info(session_pupil_data) # So we've effectively got a dataframe describing subseries that are defined either\n",
    "                                                           # as consisting either of single and isolated or continuous data points/NaN values.\n",
    "\n",
    "    # Iterate through the blocks to find small data islands surrounded by large NaN gaps\n",
    "    for i, current_block in session_blocks_df.iterrows():\n",
    "        if not current_block[\"is_nan_block\"]:  # If it's a data island\n",
    "            if current_block[\"duration_samples\"] <= MAX_ISLAND_DURATION_SAMPLES:\n",
    "                # This is a \"small\" island. Now check its neighbors.\n",
    "\n",
    "                # Check preceeding NaN block\n",
    "                preceding_nan_gap_sufficient = False\n",
    "                if i > 0: # If there is a preceding block, i.e we're not at the first index\n",
    "                    prev_block = session_blocks_df.iloc[i-1]\n",
    "                    if prev_block[\"is_nan_block\"] and prev_block[\"duration_samples\"] >= MIN_SURROUNDING_GAP_SAMPLES:\n",
    "                        preceding_nan_gap_sufficient = True\n",
    "                else:\n",
    "                    # If it's the first block of the session and it's a small data island,\n",
    "                    # consider the \"gap\" before it as infinitely large.\n",
    "                    preceding_nan_gap_sufficient = True\n",
    "\n",
    "                # Check the succeeding NaN block\n",
    "                succeeding_nan_gap_sufficient = False\n",
    "                if i < len(session_blocks_df) - 1: # If there is a succeeding block (-1 because of 0-indexing)\n",
    "                    next_block = session_blocks_df.iloc[i+1]\n",
    "                    if next_block[\"is_nan_block\"] and next_block[\"duration_samples\"] >= MIN_SURROUNDING_GAP_SAMPLES:\n",
    "                        succeeding_nan_gap_sufficient = True\n",
    "                else:\n",
    "                    # If it's the last block of the session and it's a small data island,\n",
    "                    # consider the \"gap\" after it as infinitely large.\n",
    "                    succeeding_nan_gap_sufficient = True\n",
    "\n",
    "                if preceding_nan_gap_sufficient and succeeding_nan_gap_sufficient:\n",
    "                    # This island meets the criteria. Mark its samples for NaNing.\n",
    "                    # Get the actual indices from the original cleaned_df\n",
    "                    island_original_indices = cleaned_df[session_mask][\n",
    "                        (cleaned_df[session_mask].index >= current_block[\"start_index\"]) &\n",
    "                        (cleaned_df[session_mask].index <= current_block[\"end_index\"])\n",
    "                    ].index\n",
    "                    indices_to_nan_from_islands.extend(island_original_indices)\n",
    "                    print(f\"Session {session_id}: Marking island from {current_block['start_index']} to {current_block['end_index']} (Duration: {current_block['duration_samples']}) as NaN.\")\n",
    "\n",
    "\n",
    "# Apply the NaNing to the pupil series\n",
    "if indices_to_nan_from_islands:\n",
    "    # Use .loc with the list of original DataFrame indices\n",
    "    pupil_series_for_island_removal.loc[indices_to_nan_from_islands] = np.nan\n",
    "    # To update the main DataFrame:\n",
    "    cleaned_df.loc[indices_to_nan_from_islands, \"Pupil\"] = np.nan\n",
    "\n",
    "\n",
    "nans_after_island_removal = pupil_series_for_island_removal.isna().sum() # or cleaned_df[\"Pupil\"].isna().sum()\n",
    "print(f\"NaNs after island removal: {nans_after_island_removal}\")\n",
    "print(f\"Samples NaN'd due to island removal: {nans_after_island_removal - nans_before_island_removal}\")"
   ],
   "id": "d013faad85a9896a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 3.1: Interpolate over sufficiently small enough gaps:",
   "id": "68fa9ec13c46bb7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Step: Interpolate Missing Data (NaNs) ---\n",
    "print(\"\\n--- Interpolating Missing Pupil Data ---\")\n",
    "\n",
    "# Parameters for interpolation\n",
    "MAX_INTERPOLATION_GAP_MS = 250 # 250 based on paper, set to None to interpolate all gaps regardless of length.\n",
    "\n",
    "limit_samples_for_interpolation = MAX_INTERPOLATION_GAP_MS if MAX_INTERPOLATION_GAP_MS is not None else None\n",
    "\n",
    "nans_before_interpolation = cleaned_df[\"Pupil\"].isna().sum()\n",
    "print(f\"NaNs in 'Pupil' column before interpolation: {nans_before_interpolation}\")\n",
    "\n",
    "# Create a new column for interpolated data.\n",
    "# This keeps your 'Pupil' column (with NaNs from artifact rejection) intact for reference if needed.\n",
    "# We apply interpolation per session to avoid filling data across intended large gaps between sessions.\n",
    "cleaned_df[\"Pupil Int\"] = cleaned_df.groupby(\"Session\")[\"Pupil\"].transform( # Transform merges the subseries from groupby back to the original series shape\n",
    "    lambda x: x.interpolate(\n",
    "        method='linear',\n",
    "        limit_direction='both',  # Fills from both ends of a NaN sequence up to the limit\n",
    "        limit=limit_samples_for_interpolation  # Max number of consecutive NaNs to fill\n",
    "    )\n",
    ")\n",
    "\n",
    "nans_after_interpolation = cleaned_df[\"Pupil Int\"].isna().sum()\n",
    "print(f\"NaNs in 'Pupil Int' column after interpolation: {nans_after_interpolation}\")"
   ],
   "id": "56fe01e36a7058c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Visual Check (Optional but Recommended) ---\n",
    "# This is a basic plot. You can adapt your existing, more detailed plotting routines.\n",
    "# We try to find a segment that HAD NaNs and now doesn't, to illustrate.\n",
    "\n",
    "# Find indices where 'Pupil' was NaN but 'Pupil Int' is not\n",
    "interpolated_indices = cleaned_df[cleaned_df[\"Pupil\"].isna() & cleaned_df[\"Pupil Int\"].notna()].index\n",
    "\n",
    "if not interpolated_indices.empty:\n",
    "    num_plot_samples = min(3, len(interpolated_indices)) # Plot up to 3 examples\n",
    "    fig, axes = plt.subplots(num_plot_samples, 1, figsize=(15, 4 * num_plot_samples), sharex=False)\n",
    "    if num_plot_samples == 1: axes = [axes] # Make axes iterable\n",
    "\n",
    "    for i in range(num_plot_samples):\n",
    "        # Pick a random index from those that were successfully interpolated\n",
    "        center_idx = random.choice(interpolated_indices)\n",
    "\n",
    "        # Define a window around the center_idx for plotting\n",
    "        # Ensure window indices are valid and within the DataFrame bounds\n",
    "        plot_start_idx = cleaned_df.index.get_loc(center_idx) - 100\n",
    "        plot_end_idx = cleaned_df.index.get_loc(center_idx) + 100\n",
    "\n",
    "        # Clamp to valid DataFrame iloc range\n",
    "        plot_start_idx = max(0, plot_start_idx)\n",
    "        plot_end_idx = min(len(cleaned_df) -1, plot_end_idx)\n",
    "\n",
    "        # Get the segment using iloc for integer-based slicing\n",
    "        segment_df = cleaned_df.iloc[plot_start_idx:plot_end_idx + 1] # +1 because iloc end is exclusive\n",
    "\n",
    "        if not segment_df.empty:\n",
    "            ax = axes[i]\n",
    "            # Plot original data (with NaNs) for this segment\n",
    "            ax.plot(segment_df[\"Timestamp\"], segment_df[\"Pupil\"],\n",
    "                    marker='o', linestyle='-', color='skyblue', alpha=1, markersize=5, linewidth=5, label=\"Original Pupil (with NaNs)\")\n",
    "            # Plot interpolated data for this segment\n",
    "            ax.plot(segment_df[\"Timestamp\"], segment_df[\"Pupil Int\"],\n",
    "                    marker='.', linestyle='-', color='red', alpha=0.9, markersize=3, label=\"Pupil Int\")\n",
    "            ax.axvline(x=cleaned_df.loc[center_idx, 'Timestamp'], label=f\"Center Point, TS: {cleaned_df.loc[center_idx, 'Timestamp']}\", color=\"green\")\n",
    "\n",
    "            ax.set_title(f\"Interpolation Example (Index {center_idx}, Timestamp {cleaned_df.loc[center_idx, 'Timestamp']})\")\n",
    "            ax.set_xlabel(\"Timestamp\")\n",
    "            ax.set_ylabel(\"Pupil Size\")\n",
    "            ax.legend(loc=\"upper center\")\n",
    "            ax.grid(True)\n",
    "        else:\n",
    "            axes[i].text(0.5, 0.5, \"Could not get segment for plotting\", ha='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No segments found that were NaN before and interpolated after (e.g., all data was initially valid or all gaps were too long).\")"
   ],
   "id": "3128e8480b6a34ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3.2: Calculate Percentage of Trial That Contains Interpolated Data\n",
    "This allows us to later reject trials if they have data that is mostly 'inferred'."
   ],
   "id": "f4a65795d97b9d88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "investigate_percentage_threshold = 20 # Change this to affect the printout (pc trials containing interpolated data)\n",
    "\n",
    "trial_identifier_cols = [\"Run\", \"Session\", \"Block\", \"Trial\"]\n",
    "\n",
    "# Create a new column, initialized to NaN\n",
    "cleaned_df[\"Interpolation PC\"] = np.nan\n",
    "\n",
    "# Get unique trials\n",
    "# Sort them to process in a logical order, though not strictly necessary for this calculation\n",
    "unique_trials_df = cleaned_df[trial_identifier_cols].drop_duplicates().sort_values(by=trial_identifier_cols).reset_index(drop=True)\n",
    "\n",
    "print(f\"Found {len(unique_trials_df)} unique trials to process.\")\n",
    "\n",
    "# counter for num of trials with interpolated data:\n",
    "intp_trials = 0\n",
    "intp_trials_investigate = 0\n",
    "\n",
    "for index, trial_info in unique_trials_df.iterrows():\n",
    "    current_run = trial_info[\"Run\"]\n",
    "    current_session = trial_info[\"Session\"]\n",
    "    current_block = trial_info[\"Block\"]\n",
    "    current_trial_num = trial_info[\"Trial\"]\n",
    "\n",
    "    # Create a mask to select all rows for the current trial\n",
    "    trial_mask = (cleaned_df[\"Run\"] == current_run) & \\\n",
    "                 (cleaned_df[\"Session\"] == current_session) & \\\n",
    "                 (cleaned_df[\"Block\"] == current_block) & \\\n",
    "                 (cleaned_df[\"Trial\"] == current_trial_num)\n",
    "\n",
    "    # Get the segment of the DataFrame for the current trial\n",
    "    current_trial_data = cleaned_df[trial_mask]\n",
    "\n",
    "    if current_trial_data.empty:\n",
    "        print(f\"Warning: No data found for trial: Run {current_run}, Sess {current_session}, Block {current_block}, Trial {current_trial_num}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Total number of samples in this trial\n",
    "    total_samples_in_trial = len(current_trial_data)\n",
    "\n",
    "    # Number of samples that were originally NaN and have interpolated values for this trial\n",
    "    successfully_interpolated_samples = current_trial_data[\n",
    "        current_trial_data[\"Pupil\"].isna() & current_trial_data[\"Pupil Int\"].notna()\n",
    "    ].shape[0]\n",
    "\n",
    "    # Calculate percentage\n",
    "    if total_samples_in_trial > 0:\n",
    "        percent_interpolated = (successfully_interpolated_samples / total_samples_in_trial) * 100\n",
    "    else:\n",
    "        percent_interpolated = 0.0 # Or np.nan if you prefer for empty trials\n",
    "\n",
    "    # Add to trial count if trial contains interpolated data:\n",
    "    if percent_interpolated > 0:\n",
    "        intp_trials += 1\n",
    "    if percent_interpolated >= investigate_percentage_threshold:\n",
    "        intp_trials_investigate += 1\n",
    "\n",
    "    # Assign this percentage to all rows of the current trial in the main DataFrame\n",
    "    cleaned_df.loc[trial_mask, \"Interpolation PC\"] = percent_interpolated\n",
    "\n",
    "print(\"\\n--- Finished Calculating Interpolation Percentages ---\")\n",
    "\n",
    "# --- Verification (Optional) ---\n",
    "# Check if any trials still have NaN in \"Percent_Interpolated\" (shouldn't happen if all trials were processed)\n",
    "if cleaned_df[\"Interpolation PC\"].isna().any():\n",
    "    print(\"\\nWarning: Some trials have NaN for 'Interpolation PC'. This might indicate an issue.\")\n",
    "\n",
    "num_of_unique_trials = len(unique_trials_df)\n",
    "\n",
    "print(f\"Percentage of trials containing interpolated data: {intp_trials/num_of_unique_trials * 100:.2f}% ({intp_trials}/{num_of_unique_trials})\")\n",
    "print(f\"Percentage of trials containing more than {investigate_percentage_threshold}% interpolated data: {intp_trials_investigate/num_of_unique_trials * 100:.2f}% ({intp_trials_investigate}/{num_of_unique_trials})\")\n",
    "cleaned_df[\"Interpolation PC\"].describe()"
   ],
   "id": "481930704a254122",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3.3: Remove Trials That Still Contain NaNs\n",
    "If a trial still contains NaN data, then it was likely involved in an exceptionally long blink or artifact due to recording error. In that case, we'll flag those trials for exclusion, as they'll be both unreliable and will mess with the smoothing in Step 4:"
   ],
   "id": "6577a84343c58c9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "cleaned_df[(cleaned_df[\"Timestamp\"].between(2533052,2533251))]",
   "id": "cc2e434d645ed7aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Step: Identify and Handle Trials with Remaining NaNs After Interpolation ---\n",
    "print(\"\\n--- Identifying and Handling Trials with Remaining NaNs Before Filtering ---\")\n",
    "\n",
    "trial_identifier_cols = [\"Run\", \"Session\", \"Block\", \"Trial\"]\n",
    "\n",
    "# Add an 'Exclude Trial' column if it doesn't exist\n",
    "if \"Exclude Trial\" not in cleaned_df.columns:\n",
    "    cleaned_df[\"Exclude Trial\"] = False\n",
    "    print(\"Added 'Exclude Trial' column, initialized to False.\")\n",
    "\n",
    "# Get unique trials\n",
    "unique_trials_for_nan_check_df = cleaned_df[trial_identifier_cols].drop_duplicates()\n",
    "\n",
    "trials_with_nans_count = 0\n",
    "total_samples_in_nan_trials = 0\n",
    "\n",
    "for index, trial_info in unique_trials_for_nan_check_df.iterrows():\n",
    "    current_run = trial_info[\"Run\"]\n",
    "    current_session = trial_info[\"Session\"]\n",
    "    current_block = trial_info[\"Block\"]\n",
    "    current_trial_num = trial_info[\"Trial\"]\n",
    "\n",
    "    trial_mask = (cleaned_df[\"Run\"] == current_run) & \\\n",
    "                 (cleaned_df[\"Session\"] == current_session) & \\\n",
    "                 (cleaned_df[\"Block\"] == current_block) & \\\n",
    "                 (cleaned_df[\"Trial\"] == current_trial_num)\n",
    "\n",
    "    # Check if any NaNs exist in 'Pupil Int' for this trial\n",
    "    if cleaned_df.loc[trial_mask, \"Pupil Int\"].isna().any():\n",
    "        trials_with_nans_count += 1\n",
    "        total_samples_in_nan_trials += trial_mask.sum()\n",
    "\n",
    "        # Mark the trial for exclusion\n",
    "        cleaned_df.loc[trial_mask, \"Exclude Trial\"] = True\n",
    "\n",
    "\n",
    "if trials_with_nans_count > 0:\n",
    "    print(f\"Identified {trials_with_nans_count} trials with remaining NaNs in 'Pupil Int'.\")\n",
    "    print(f\"These {total_samples_in_nan_trials} samples in these trials will be marked for exclusion (or handled as per your choice).\")\n",
    "    print(\"Example trials marked for exclusion:\")\n",
    "    print(cleaned_df[cleaned_df[\"Exclude Trial\"] == True][trial_identifier_cols].drop_duplicates().head())\n",
    "else:\n",
    "    print(\"No trials found with remaining NaNs in 'Pupil Int'. All trials are ready for filtering.\")"
   ],
   "id": "e6e87b9a9f90a52b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 4: Smoothing the data\n",
    "The paper recommends running the data through a zero-phase low-pass filter with 4Hz cutoff:"
   ],
   "id": "b5bb381bfb2158b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "cleaned_df[(cleaned_df[\"Timestamp\"].between(2533052,2533251))]",
   "id": "231780d65434cb34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Step: Low-Pass Filtering (e.g., Butterworth) --- #\n",
    "print(\"\\n--- Applying Zero-Phase Low-Pass Butterworth Filter ---\")\n",
    "\n",
    "# Filter Parameters\n",
    "CUTOFF_FREQUENCY_HZ = 4.0\n",
    "FILTER_ORDER = 4\n",
    "\n",
    "# Design the Butterworth filter\n",
    "nyquist_freq = 0.5 * 1000 # * sampling rate\n",
    "normal_cutoff = CUTOFF_FREQUENCY_HZ / nyquist_freq\n",
    "b, a = butter(FILTER_ORDER, normal_cutoff, btype='low', analog=False)\n",
    "\n",
    "# The data to filter is 'Pupil Int'.\n",
    "# Trials with remaining NaNs in 'Pupil Int' are marked by 'Exclude Trial = True'\n",
    "# and we'll create 'Pupil Filtered' for all rows, setting it to NaN for these excluded trials\n",
    "\n",
    "# Create the 'Pupil Filtered' column, initialized with NaN data and later filled only for non-excluded trials.\n",
    "cleaned_df[\"Pupil Filtered\"] = np.nan\n",
    "\n",
    "# Create a temporary series for filtering that has NaNs for excluded trials to ensure filtfilt is only applied to valid trial data.\n",
    "temp_pupil_for_filtering = cleaned_df[\"Pupil Int\"].copy()\n",
    "if cleaned_df[\"Exclude Trial\"].any():\n",
    "    temp_pupil_for_filtering.loc[cleaned_df[\"Exclude Trial\"] == True] = np.nan\n",
    "    print(f\"Temporarily NaN'd Pupil Int for {cleaned_df['Exclude Trial'].sum()} samples from excluded trials before filtering.\")\n",
    "\n",
    "if temp_pupil_for_filtering.isna().any():\n",
    "    print(f\"Warning: {temp_pupil_for_filtering.isna().sum()} NaNs present in the data to be filtered (likely from excluded trials).\")\n",
    "    print(\"  The simplified filtfilt expects no NaNs within a processing segment.\")\n",
    "    print(\"  The apply_filtfilt_simplified_safe function will handle NaN segments by returning NaNs.\")\n",
    "\n",
    "else: # If no NaN vals, then all Pupil Int data is fair game:\n",
    "    temp_pupil_for_filtering = cleaned_df[\"Pupil Int\"]\n",
    "\n",
    "\n",
    "# Simplified safe application of filtfilt. It ensures that if a segment passed to it contains NaNs, it returns NaNs.\n",
    "MIN_SAMPLES_FOR_FILTER = FILTER_ORDER * 3 +1 # Padlen must be less than data length\n",
    "\n",
    "def apply_filtfilt_simplified_safe(series, b_coeffs, a_coeffs):\n",
    "    if series.isna().all(): # If the entire series segment (e.g., a session where all trials were excluded) is NaN\n",
    "        return series # Return the all-NaN series\n",
    "    if series.isna().any(): # If there are any NaNs mixed in (shouldn't happen if pre-processing is correct)\n",
    "        print(\"Error: apply_filtfilt_simplified_safe received a series with mixed NaNs and non-NaNs. This indicates an issue in pre-filtering NaN handling.\")\n",
    "        # Fallback: return NaNs for the whole series to avoid errors, but this is a sign of a problem.\n",
    "        return pd.Series(np.nan, index=series.index, dtype=series.dtype)\n",
    "    if len(series.dropna()) < MIN_SAMPLES_FOR_FILTER: # Check length of actual data\n",
    "        print(f\"Warning: Segment too short ({len(series.dropna())} non-NaN samples) for filtfilt. Returning original data for this segment.\")\n",
    "        return series\n",
    "    return filtfilt(b_coeffs, a_coeffs, series.dropna()) # Apply to non-NaN data; re-indexing might be needed if using on subset\n",
    "\n",
    "# Applying the filter using groupby.transform\n",
    "filtered_series_list = []\n",
    "for session_id, group_df in cleaned_df.groupby(\"Session\"):\n",
    "    session_data_to_filter = temp_pupil_for_filtering.loc[group_df.index]\n",
    "\n",
    "    # If all data for this session is NaN (e.g., all trials excluded)\n",
    "    if session_data_to_filter.isna().all():\n",
    "        filtered_segment = session_data_to_filter # Keep it all NaN\n",
    "    elif session_data_to_filter.isna().any():\n",
    "        # This case means some trials in the session are valid, some are NaN (excluded).\n",
    "        # We need to filter only the contiguous non-NaN parts.\n",
    "        filtered_segment = pd.Series(np.nan, index=session_data_to_filter.index, dtype=float)\n",
    "        not_nan_mask = session_data_to_filter.notna()\n",
    "\n",
    "        # Find change points to identify contiguous blocks of non-NaN data\n",
    "        change_points = not_nan_mask.diff().ne(0)\n",
    "        block_ids = change_points.cumsum()\n",
    "\n",
    "        for block_id, data_block in session_data_to_filter[not_nan_mask].groupby(block_ids[not_nan_mask]):\n",
    "            if len(data_block) >= MIN_SAMPLES_FOR_FILTER:\n",
    "                filtered_sub_segment = filtfilt(b, a, data_block)\n",
    "                filtered_segment.loc[data_block.index] = filtered_sub_segment\n",
    "            else:\n",
    "                print(f\"Info: Non-NaN sub-segment in session {session_id} too short ({len(data_block)} samples). Not filtering this part.\")\n",
    "                filtered_segment.loc[data_block.index] = data_block # Keep original for very short valid segments\n",
    "    else: # No NaNs in this session's data to filter\n",
    "        if len(session_data_to_filter) < MIN_SAMPLES_FOR_FILTER:\n",
    "            print(f\"Info: Entire session {session_id} data too short ({len(session_data_to_filter)} samples). Not filtering.\")\n",
    "            filtered_segment = session_data_to_filter\n",
    "        else:\n",
    "            filtered_segment = filtfilt(b, a, session_data_to_filter)\n",
    "\n",
    "    filtered_series_list.append(pd.Series(filtered_segment, index=group_df.index))\n",
    "\n",
    "if filtered_series_list:\n",
    "    cleaned_df[\"Pupil Filtered\"] = pd.concat(filtered_series_list).sort_index()\n",
    "else:\n",
    "    print(\"No data processed for filtering.\")\n",
    "\n",
    "\n",
    "nans_after_filtering = cleaned_df[\"Pupil Filtered\"].isna().sum()\n",
    "if \"Exclude Trial\" in cleaned_df.columns:\n",
    "    expected_nans_from_excluded = cleaned_df[\"Exclude Trial\"].sum()\n",
    "    if nans_after_filtering > 0 :\n",
    "        print(f\"NaNs in 'Pupil Filtered' after filtering: {nans_after_filtering} (expected around {expected_nans_from_excluded} from excluded trials).\")\n",
    "else:\n",
    "     if nans_after_filtering > 0:\n",
    "        print(f\"NaNs in 'Pupil Filtered' after filtering: {nans_after_filtering}.\")\n",
    "\n",
    "\n",
    "print(\"--- Butterworth Filtering Complete ---\")"
   ],
   "id": "4fd2a8a8083ded6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Visual Check ---\n",
    "# (Same visual check code as before, it will use Pupil Int and Pupil Filtered)\n",
    "if not cleaned_df.empty:\n",
    "    num_plot_samples_viz = min(3, len(cleaned_df[\"Session\"].unique()))\n",
    "    plot_indices_viz = []\n",
    "\n",
    "    # Try to pick segments from non-excluded trials if possible\n",
    "    valid_for_plot_df = cleaned_df\n",
    "    if \"Exclude Trial\" in cleaned_df.columns:\n",
    "        valid_for_plot_df = cleaned_df[cleaned_df[\"Exclude Trial\"] == False]\n",
    "\n",
    "    if valid_for_plot_df.empty and \"Exclude Trial\" in cleaned_df.columns:\n",
    "        print(\"No non-excluded trials to pick for visualization. Plotting from original df if available.\")\n",
    "        valid_for_plot_df = cleaned_df # Fallback to all data if no non-excluded\n",
    "\n",
    "    if not valid_for_plot_df.empty:\n",
    "        for session_id in valid_for_plot_df[\"Session\"].unique()[:num_plot_samples_viz]:\n",
    "            session_data = valid_for_plot_df[valid_for_plot_df[\"Session\"] == session_id]\n",
    "            if len(session_data) > 4000: # Ensure session is long enough for a good window\n",
    "                mid_point_iloc_viz = session_data.index.get_loc(session_data.index[len(session_data) // 2])\n",
    "                plot_indices_viz.append(cleaned_df.index[mid_point_iloc_viz]) # Get original df index\n",
    "            elif not session_data.empty:\n",
    "                plot_indices_viz.append(session_data.index[0])\n",
    "\n",
    "        if not plot_indices_viz:\n",
    "             if len(cleaned_df) > 4000:\n",
    "                plot_indices_viz.append(cleaned_df.index[len(cleaned_df)//2])\n",
    "             elif not cleaned_df.empty:\n",
    "                plot_indices_viz.append(cleaned_df.index[0])\n",
    "\n",
    "        fig, axes = plt.subplots(len(plot_indices_viz), 1, figsize=(15, 5 * len(plot_indices_viz)), sharex=False)\n",
    "        if len(plot_indices_viz) == 1: axes = [axes]\n",
    "\n",
    "        for i, center_idx in enumerate(plot_indices_viz):\n",
    "            ax = axes[i]\n",
    "            plot_start_iloc = cleaned_df.index.get_loc(center_idx) - 2000\n",
    "            plot_end_iloc = cleaned_df.index.get_loc(center_idx) + 2000\n",
    "            plot_start_iloc = max(0, plot_start_iloc)\n",
    "            plot_end_iloc = min(len(cleaned_df) - 1, plot_end_iloc)\n",
    "            segment_df_viz = cleaned_df.iloc[plot_start_iloc:plot_end_iloc + 1]\n",
    "\n",
    "            if not segment_df_viz.empty:\n",
    "                ax.plot(segment_df_viz[\"Timestamp\"], segment_df_viz[\"Pupil Int\"], label=\"Pupil Interpolated\", color=\"skyblue\", alpha=0.7)\n",
    "                ax.plot(segment_df_viz[\"Timestamp\"], segment_df_viz[\"Pupil Filtered\"], label=f\"Pupil Filtered ({CUTOFF_FREQUENCY_HZ}Hz)\", color=\"red\")\n",
    "                current_session_id_viz = segment_df_viz[\"Session\"].iloc[0]\n",
    "                ax.set_title(f\"Filtering Example (Session {current_session_id_viz}, around Timestamp {cleaned_df.loc[center_idx, 'Timestamp']})\")\n",
    "                ax.set_xlabel(\"Timestamp\")\n",
    "                ax.set_ylabel(\"Pupil Size\")\n",
    "                ax.legend(loc=\"lower center\")\n",
    "                ax.grid(True)\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, \"Could not get segment for plotting\", ha='center')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No suitable data for filter visualization.\")"
   ],
   "id": "5ef498ec42f27e37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n--- Plotting Original (Interpolated) vs. Filtered Pupil Data for the Entire Run ---\")\n",
    "\n",
    "if cleaned_df.empty:\n",
    "    print(\"DataFrame is empty. Cannot generate plot.\")\n",
    "else:\n",
    "    plt.figure(figsize=(20, 8)) # Adjust figure size as needed\n",
    "\n",
    "    # Plot 1: Pupil Int (data after interpolation, before this Butterworth filter)\n",
    "    # We plot this with some transparency to see the filtered line clearly.\n",
    "    # If there are still NaNs in Pupil Int (e.g., from very long gaps not interpolated),\n",
    "    # they will appear as breaks in this line.\n",
    "    plt.plot(cleaned_df[\"Timestamp\"], cleaned_df[\"Pupil Int\"],\n",
    "             label=\"Pupil Data (After Interpolation)\", color=\"skyblue\", alpha=0.7, linewidth=1.5)\n",
    "\n",
    "    # Plot 2: Pupil Filtered (data after Butterworth filter)\n",
    "    # This line should be smoother. NaNs here would correspond to:\n",
    "    #   - Trials excluded before filtering (their input was NaN).\n",
    "    #   - Very short valid segments that were not filtered.\n",
    "    plt.plot(cleaned_df[\"Timestamp\"], cleaned_df[\"Pupil Filtered\"],\n",
    "             label=f\"Pupil Data (Filtered - {CUTOFF_FREQUENCY_HZ}Hz Butterworth)\", color=\"red\", linewidth=1)\n",
    "\n",
    "    # Add vertical lines for session boundaries if you have multiple sessions in `cleaned_df`\n",
    "    # and want to visualize them.\n",
    "    if \"Session\" in cleaned_df.columns and cleaned_df[\"Session\"].nunique() > 1:\n",
    "        session_changes = cleaned_df[\"Session\"].diff().fillna(0) != 0\n",
    "        session_start_timestamps = cleaned_df.loc[session_changes, \"Timestamp\"]\n",
    "        for ts in session_start_timestamps:\n",
    "            if ts != cleaned_df[\"Timestamp\"].iloc[0]: # Don't draw for the very first sample\n",
    "                plt.axvline(x=ts, color='gray', linestyle='--', linewidth=1, label='Session Start' if ts == session_start_timestamps.iloc[1] else None) # Label only once\n",
    "\n",
    "\n",
    "    plt.title(f\"Pupil Data: Interpolated vs. Butterworth Filtered ({CUTOFF_FREQUENCY_HZ}Hz) - Entire Run\")\n",
    "    plt.xlabel(\"Timestamp (ms)\")\n",
    "    plt.ylabel(\"Pupil Size\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(True, linestyle=':', alpha=0.6)\n",
    "\n",
    "    # You might want to adjust x-axis limits if the run is extremely long\n",
    "    # to focus on specific parts, or let matplotlib auto-scale.\n",
    "    # Example: plt.xlim(cleaned_df[\"Timestamp\"].min(), cleaned_df[\"Timestamp\"].min() + 60000) # First minute\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Optional: Plotting each session separately for better detail ---\n",
    "if \"Session\" in cleaned_df.columns and cleaned_df[\"Session\"].nunique() > 1:\n",
    "    print(\"\\n--- Plotting each session separately ---\")\n",
    "    num_sessions = cleaned_df[\"Session\"].nunique()\n",
    "\n",
    "    # Determine number of rows and columns for subplots\n",
    "    # Aim for a somewhat square layout, or max 2 columns for readability\n",
    "    if num_sessions <= 2:\n",
    "        n_cols = num_sessions\n",
    "        n_rows = 1\n",
    "    elif num_sessions <= 4:\n",
    "        n_cols = 2\n",
    "        n_rows = int(np.ceil(num_sessions / 2.0))\n",
    "    else: # More than 4 sessions, might get crowded\n",
    "        n_cols = 2 # Or 3 if you prefer\n",
    "        n_rows = int(np.ceil(num_sessions / float(n_cols)))\n",
    "\n",
    "\n",
    "    fig_sessions, axes_sessions = plt.subplots(n_rows, n_cols, figsize=(10 * n_cols, 6 * n_rows), sharey=True, squeeze=False)\n",
    "    axes_sessions_flat = axes_sessions.flatten()\n",
    "\n",
    "    for i, (session_id, session_df) in enumerate(cleaned_df.groupby(\"Session\")):\n",
    "        if i >= len(axes_sessions_flat): # Should not happen with correct subplot calculation\n",
    "            break\n",
    "        ax = axes_sessions_flat[i]\n",
    "\n",
    "        ax.plot(session_df[\"Timestamp\"], session_df[\"Pupil Int\"],\n",
    "                label=\"Interpolated\", color=\"skyblue\", alpha=0.7, linewidth=1.5)\n",
    "        ax.plot(session_df[\"Timestamp\"], session_df[\"Pupil Filtered\"],\n",
    "                label=f\"Filtered ({CUTOFF_FREQUENCY_HZ}Hz)\", color=\"red\", linewidth=1)\n",
    "\n",
    "        ax.set_title(f\"Session {session_id}\")\n",
    "        ax.set_xlabel(\"Timestamp (ms)\")\n",
    "        ax.set_ylabel(\"Pupil Size\")\n",
    "        ax.legend(loc=\"upper right\")\n",
    "        ax.grid(True, linestyle=':', alpha=0.6)\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(axes_sessions_flat)):\n",
    "        fig_sessions.delaxes(axes_sessions_flat[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "elif \"Session\" not in cleaned_df.columns:\n",
    "    print(\"No 'Session' column found, cannot plot per session.\")"
   ],
   "id": "1c56dd5e3e0e59df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Create Baselines:\n",
    "Baselines will be needed to then standardise the pupil's responses over trials, thereby accounting and controlling for factors such as changes in light and environment which may skew the data when comparing over timecourses.\n",
    "\n",
    "I will take the final 200ms of the prior trial (-200ms relative to stimulus onset) for the trial's baseline. However, this does mean that the first trial of each session needs to be handled differently. I shall use the final 500ms of the adaptation period to form this baseline. I chose a longer period from which to average from in the hopes that it would be more consistent with the interpolated, smoothed pupil data, but honestly it's just a shot in the dark"
   ],
   "id": "41f40d3a0c3c5eaa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if PRODUCE_BASELINES:\n",
    "    # Parameters\n",
    "    BASELINE_DURATION_MS = 200 # Your desired duration (e.g., -200ms to -1ms from stim onset)\n",
    "\n",
    "    # Initialize new columns\n",
    "    cleaned_df[\"Baseline\"] = np.nan\n",
    "    cleaned_df[\"Pupil Normed\"] = np.nan      # For subtractive normalization\n",
    "    cleaned_df[\"Pupil Normed PC\"] = np.nan   # For percent change normalization\n",
    "\n",
    "    # 1. Load adaptation baselines (for the first trial of each session), which were saved by the EDF Reader notebook\n",
    "    with open(f\"data/processed_data/baselines/{PARTICIPANT_ID}/run_{run}_baselines.pkl\", 'rb') as f:\n",
    "        adaptation_baselines = pickle.load(f) # Expected format: {'Session 1': value, 'Session 2': value, ...}\n",
    "\n",
    "    trial_identifier_cols = [\"Run\", \"Session\", \"Block\", \"Trial\"]\n",
    "\n",
    "    # Get unique trials, sorted to process them chronologically. We also need 'Timestamp' to find trial onsets\n",
    "    unique_trials_df = cleaned_df[trial_identifier_cols + [\"Timestamp\"]].groupby(\n",
    "        trial_identifier_cols, as_index=False\n",
    "    ).agg(\n",
    "        Trial_Onset_Timestamp=('Timestamp', 'min') # Get the min timestamp for each trial as its onset\n",
    "    ).sort_values(by=trial_identifier_cols).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Found {len(unique_trials_df)} unique trials for baseline calculation.\")\n",
    "\n",
    "    for index, trial_info in unique_trials_df.iterrows():\n",
    "        current_run_num = trial_info[\"Run\"] # Using a different var name to avoid conflict with 'run' if it's global\n",
    "        current_session = trial_info[\"Session\"]\n",
    "        current_block = trial_info[\"Block\"]\n",
    "        current_trial_num = trial_info[\"Trial\"]\n",
    "        trial_onset_timestamp = trial_info[\"Trial_Onset_Timestamp\"]\n",
    "\n",
    "        # Create a mask to select all rows for the current trial in the main DataFrame\n",
    "        trial_mask_main_df = (cleaned_df[\"Run\"] == current_run_num) & \\\n",
    "                             (cleaned_df[\"Session\"] == current_session) & \\\n",
    "                             (cleaned_df[\"Block\"] == current_block) & \\\n",
    "                             (cleaned_df[\"Trial\"] == current_trial_num)\n",
    "\n",
    "        baseline_mean = np.nan\n",
    "\n",
    "        # Check if this trial was marked for exclusion (if you have that column)\n",
    "        is_excluded_trial = False\n",
    "        if \"Exclude_Trial\" in cleaned_df.columns:\n",
    "            # Check the first row of the trial for the exclusion flag\n",
    "            if not cleaned_df[trial_mask_main_df].empty:\n",
    "                is_excluded_trial = cleaned_df[trial_mask_main_df][\"Exclude_Trial\"].iloc[0]\n",
    "\n",
    "        if is_excluded_trial:\n",
    "            print(f\"Info: Trial Run {current_run_num}, Sess {current_session}, Block {current_block}, Trial {current_trial_num} is marked for exclusion. Baseline will be NaN or calculated based on available data if not fully NaN.\")\n",
    "            # If an excluded trial has all NaNs in Pupil Filtered, the baseline will be NaN.\n",
    "            # If it has some data, the baseline will be calculated, but the trial is still excluded.\n",
    "            pass\n",
    "\n",
    "\n",
    "        if current_block == 1 and current_trial_num == 1: # First trial of a session\n",
    "            baseline_key = f\"Session {current_session}\"\n",
    "            if baseline_key in adaptation_baselines:\n",
    "                baseline_mean = adaptation_baselines[baseline_key]\n",
    "            else:\n",
    "                print(f\"  Warning: No adaptation baseline found for '{baseline_key}'. Baseline for this trial will be NaN.\")\n",
    "        else:\n",
    "            # Baseline from the specified period before the current trial's onset timestamp\n",
    "            baseline_start_ts = trial_onset_timestamp - BASELINE_DURATION_MS\n",
    "            baseline_end_ts = trial_onset_timestamp - 1 # Up to, but NOT including, the trial onset\n",
    "\n",
    "            # Extract 'Pupil Filtered' data from this baseline period\n",
    "            baseline_period_data = cleaned_df[\n",
    "                (cleaned_df[\"Timestamp\"] >= baseline_start_ts) &\n",
    "                (cleaned_df[\"Timestamp\"] <= baseline_end_ts)\n",
    "            ][\"Pupil Filtered\"]\n",
    "\n",
    "            if not baseline_period_data.empty and not baseline_period_data.isna().all():\n",
    "                baseline_mean = baseline_period_data.mean()\n",
    "            elif baseline_period_data.empty:\n",
    "                print(f\"  Warning: Baseline period empty for S{current_session} B{current_block} T{current_trial_num} (Timestamps {baseline_start_ts}-{baseline_end_ts}). Baseline will be NaN.\")\n",
    "            else: # Not empty, but all NaNs\n",
    "                 print(f\"  Warning: Baseline period data is all NaNs for S{current_session} B{current_block} T{current_trial_num} (Timestamps {baseline_start_ts}-{baseline_end_ts}). Baseline will be NaN.\")\n",
    "\n",
    "\n",
    "        # If a baseline_mean was successfully determined (not NaN)\n",
    "        if pd.notna(baseline_mean):\n",
    "            cleaned_df.loc[trial_mask_main_df, \"Baseline\"] = baseline_mean\n",
    "\n",
    "            # Get the 'Pupil Filtered' data for the current trial\n",
    "            current_trial_Pupil_Filtered = cleaned_df.loc[trial_mask_main_df, \"Pupil Filtered\"]\n",
    "\n",
    "            # Subtractive Normalization\n",
    "            cleaned_df.loc[trial_mask_main_df, \"Pupil Normed\"] = current_trial_Pupil_Filtered - baseline_mean\n",
    "\n",
    "            # Percent Change Normalization\n",
    "            # Avoid division by zero or by very small baseline values\n",
    "            if baseline_mean != 0:\n",
    "                cleaned_df.loc[trial_mask_main_df, \"Pupil Normed PC\"] = ((current_trial_Pupil_Filtered - baseline_mean) / baseline_mean) * 100\n",
    "            else:\n",
    "                # Handle zero baseline (e.g., set PC to NaN or a large number, or 0 if pupil also 0)\n",
    "                cleaned_df.loc[trial_mask_main_df, \"Pupil Normed PC\"] = np.nan\n",
    "                if not is_excluded_trial: # Don't warn for trials that are already problematic\n",
    "                     print(f\"  Warning: Baseline is zero for S{current_session} B{current_block} T{current_trial_num}. 'Pupil Normed PC' set to NaN.\")\n",
    "        else:\n",
    "            # If baseline_mean is NaN, the normed columns will also remain NaN (as initialized)\n",
    "            if not is_excluded_trial: # Don't warn for trials that are already problematic\n",
    "                print(f\"  Failed to determine baseline for S{current_session} B{current_block} T{current_trial_num}. Normed values will be NaN.\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Baseline Calculation and Normalization Complete ---\")\n",
    "\n",
    "    # --- Verification (Optional) ---\n",
    "    print(\"\\nExample trials with baseline and normed values:\")\n",
    "    cols_to_show = trial_identifier_cols + [\"Baseline\", \"Pupil Filtered\", \"Pupil Normed\", \"Pupil Normed PC\"]\n",
    "    # Show a few rows from a couple of trials to see the effect\n",
    "    example_display_df = pd.concat([\n",
    "        cleaned_df[cleaned_df[\"Trial\"] == 1].head(3), # First trial (likely uses adaptation baseline)\n",
    "        cleaned_df[cleaned_df[\"Trial\"] == 2].head(3)  # Second trial (uses calculated baseline)\n",
    "    ]).drop_duplicates(subset=trial_identifier_cols, keep='first') # Show one representative from each\n",
    "\n",
    "    if not example_display_df.empty:\n",
    "        print(example_display_df[cols_to_show])\n",
    "    else:\n",
    "        # If the above specific trials don't exist, just show some head() data\n",
    "        print(cleaned_df[cols_to_show].head(10).drop_duplicates(subset=trial_identifier_cols, keep='first'))\n",
    "\n",
    "\n",
    "    # Check for any trials where baseline might still be NaN\n",
    "    nan_baseline_trials = cleaned_df[cleaned_df[\"Baseline\"].isna()][trial_identifier_cols].drop_duplicates()\n",
    "    if not nan_baseline_trials.empty:\n",
    "        print(f\"\\nWarning: {len(nan_baseline_trials)} unique trials still have NaN for 'Baseline'. Review warnings above.\")\n",
    "        # print(nan_baseline_trials.head())"
   ],
   "id": "3fac7459749f7feb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Reject Trials:",
   "id": "8939bcce9f3f8e71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def produce_pre_processed_csv(analysis_df, rejected_trials, filepath):\n",
    "    # rejected_trials is a set of tuples: (Run, Session, Block, Trial)\n",
    "    # analysis_df is your main cleaned_df at that point\n",
    "    print(f\"\\nTotal unique trials identified for potential rejection/flagging: {len(rejected_trials)}\")\n",
    "\n",
    "    # Start with a full copy. This will be modified.\n",
    "    output_df = analysis_df.copy()\n",
    "\n",
    "    if rejected_trials:\n",
    "        # Convert the set of rejected trial tuples into a DataFrame for merging\n",
    "        df_trials_to_flag_or_reject = pd.DataFrame(list(rejected_trials), columns=trial_identifier_cols)\n",
    "\n",
    "        # Merge analysis_df with the trials to flag/reject.\n",
    "        # This adds an indicator column '_merge'.\n",
    "        #   '_merge' == 'both': The trial was in df_trials_to_flag_or_reject.\n",
    "        #   '_merge' == 'left_only': The trial was only in analysis_df (i.e., not flagged for rejection).\n",
    "        merged_df = pd.merge(\n",
    "            output_df,\n",
    "            df_trials_to_flag_or_reject,\n",
    "            on=trial_identifier_cols,\n",
    "            how='left',  # Keep all rows from output_df\n",
    "            indicator=True\n",
    "        )\n",
    "\n",
    "        if not DECONVOLUTION_OUTPUT:\n",
    "            # Original behavior: Physically drop the rejected trials\n",
    "            # Keep only rows where '_merge' is 'left_only' (i.e., trials not in rejected_trials)\n",
    "            output_df = merged_df[merged_df['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "            action_taken = \"rejected and dropped\"\n",
    "        else:\n",
    "            # DECONVOLUTION_OUTPUT is True: Keep all trials, add a flag column\n",
    "            # The 'merged_df' already contains all original rows.\n",
    "            # We just need to create our flag based on the '_merge' column.\n",
    "            # Let's name the flag column clearly, e.g., 'Trial_Rejected_Least_Strict'\n",
    "            output_df = merged_df.copy() # Keep all rows from the merge\n",
    "            output_df['Trial_Rejected_Least_Strict'] = (output_df['_merge'] == 'both')\n",
    "            output_df = output_df.drop(columns=['_merge']) # Clean up the temporary indicator\n",
    "            action_taken = \"flagged in 'Trial_Rejected_Least_Strict' column\"\n",
    "\n",
    "        print(f\"  Trials matching criteria have been {action_taken}.\")\n",
    "    else:\n",
    "        print(\"  No trials matched rejection/flagging criteria.\")\n",
    "        if DECONVOLUTION_OUTPUT: # Ensure the flag column exists even if no trials are flagged\n",
    "             output_df['Trial_Rejected_Least_Strict'] = False\n",
    "\n",
    "\n",
    "    # --- Verify and Report Final Counts (of trials in the output_df) ---\n",
    "    # This part needs to be careful based on whether rows were dropped or not.\n",
    "    # For DECONVOLUTION_OUTPUT=True, total_trials_final should be same as total_trials_initial.\n",
    "\n",
    "    total_trials_initial = len(analysis_df[trial_identifier_cols].drop_duplicates())\n",
    "\n",
    "    if not DECONVOLUTION_OUTPUT: # If trials were dropped\n",
    "        total_trials_final = len(output_df[trial_identifier_cols].drop_duplicates())\n",
    "        print(f\"\\nInitial unique trials in input analysis_df: {total_trials_initial}\")\n",
    "        print(f\"Trials remaining in output_df after rejection: {total_trials_final}\")\n",
    "        if total_trials_initial > 0:\n",
    "            rejected_count = total_trials_initial - total_trials_final\n",
    "            print(f\"Number of unique trials rejected: {rejected_count}\")\n",
    "            print(f\"Percentage of trials rejected: {(rejected_count / total_trials_initial) * 100:.2f}%\")\n",
    "        else:\n",
    "            print(\"No trials to begin with in input analysis_df.\")\n",
    "    else: # If DECONVOLUTION_OUTPUT is True, trials are flagged, not dropped\n",
    "        total_trials_final = len(output_df[trial_identifier_cols].drop_duplicates())\n",
    "        flagged_trials_count = output_df[output_df['Trial_Rejected_Least_Strict'] == True][trial_identifier_cols].drop_duplicates().shape[0]\n",
    "        print(f\"\\nInitial unique trials in input analysis_df: {total_trials_initial}\")\n",
    "        print(f\"Total unique trials in output_df (all kept): {total_trials_final}\")\n",
    "        print(f\"Number of unique trials flagged as 'Trial_Rejected_Least_Strict': {flagged_trials_count}\")\n",
    "        if total_trials_initial > 0:\n",
    "             print(f\"Percentage of trials flagged: {(flagged_trials_count / total_trials_initial) * 100:.2f}%\")\n",
    "        else:\n",
    "            print(\"No trials to begin with in input analysis_df.\")\n",
    "\n",
    "    output_df.to_csv(filepath, index=(not isinstance(output_df.index, pd.RangeIndex))) # Save index if it's not default\n",
    "    print(f\"Output saved to: {filepath}\")"
   ],
   "id": "2b6144d49fb3cb2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# First, we need to add information about our epoch onsets and offsets to the dataframe for each trail:\n",
    "\n",
    "trial_identifier_cols = [\"Run\", \"Session\", \"Block\", \"Trial\"]\n",
    "\n",
    "# --- Get Main Stimulus Onset Timestamps for Each Trial ---\n",
    "# Make sure to include trial identifiers for merging\n",
    "main_stim_onsets_df = test_messages_df[test_messages_df[\"Message Type\"] == \"Main Stimulus Onset\"][trial_identifier_cols + [\"Timestamp\"]].copy()\n",
    "\n",
    "# Ensure one onset per trial (in case of duplicate messages)\n",
    "main_stim_onsets_df = main_stim_onsets_df.drop_duplicates(subset=trial_identifier_cols, keep=\"first\")\n",
    "main_stim_onsets_df = main_stim_onsets_df.rename(columns={\"Timestamp\": \"Trial Main Stim Onset\"})\n",
    "\n",
    "# --- Calculate Epoch Start and End Timestamps for Each Trial ---\n",
    "main_stim_onsets_df[\"Trial Epoch Start\"] = main_stim_onsets_df[\"Trial Main Stim Onset\"] + config.EPOCH_START\n",
    "main_stim_onsets_df[\"Trial Epoch End\"] = main_stim_onsets_df[\"Trial Main Stim Onset\"] + config.EPOCH_END\n",
    "\n",
    "# --- Merge Epoch Information into cleaned_df ---\n",
    "# This adds 'Trial Main Stim Onset', 'Trial Epoch Start', 'Trial Epoch End' to each sample based on its trial identifiers.\n",
    "cleaned_df = pd.merge(cleaned_df,\n",
    "                      main_stim_onsets_df[trial_identifier_cols + [\"Trial Main Stim Onset\", \"Trial Epoch Start\", \"Trial Epoch End\"]],\n",
    "                      on=trial_identifier_cols,\n",
    "                      how='left') # Use 'left' to keep all rows from cleaned_df\n",
    "\n",
    "\n",
    "# --- Create a Boolean Column 'Is In Epoch' ---\n",
    "# This flags each sample if its Timestamp falls within its trial's defined epoch\n",
    "cleaned_df[\"Is In Epoch\"] = (\n",
    "    (cleaned_df[\"Timestamp\"] >= cleaned_df[\"Trial Epoch Start\"]) &\n",
    "    (cleaned_df[\"Timestamp\"] <= cleaned_df[\"Trial Epoch End\"])\n",
    ")\n",
    "\n",
    "# --- Create a Boolean Column 'Is Before Epoch':\n",
    "cleaned_df[\"Is Before Epoch\"] = cleaned_df[\"Timestamp\"] < cleaned_df[\"Trial Epoch Start\"]\n",
    "\n",
    "# Handle cases where Trial Epoch Start might be NaN (if a trial somehow missed its onset message)\n",
    "cleaned_df[\"Is In Epoch\"] = cleaned_df[\"Is In Epoch\"].fillna(False)\n",
    "\n",
    "print(f\"Number of samples within an epoch: {cleaned_df[\"Is In Epoch\"].sum()}\")\n"
   ],
   "id": "c6373301f5dde85d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Next, we'll handle the rejection criteria that is essential, i.e. what I consider absolutely necessary:\n",
    "trials_to_reject_set = set()\n",
    "os.makedirs(f\"data/fully_preprocessed_data/{participant_id}\", exist_ok=True)\n",
    "\n",
    "# Reject trials which contained data that could not be interpolated:\n",
    "excluded_by_nan_trials = cleaned_df[cleaned_df[\"Exclude Trial\"] == True][trial_identifier_cols].drop_duplicates()\n",
    "for _, row in excluded_by_nan_trials.iterrows():\n",
    "    trials_to_reject_set.add(tuple(row))\n",
    "print(f\"Trials to reject due to un-interpolatable NaNs: {len(excluded_by_nan_trials)}\")\n",
    "\n",
    "# Reject trials if there was a blink during the main stimulus presentation period:\n",
    "blink_on_stim_trials = cleaned_df[cleaned_df[\"Blink On Main Stim\"] == True][trial_identifier_cols].drop_duplicates()\n",
    "for _, row in blink_on_stim_trials.iterrows():\n",
    "    trials_to_reject_set.add(tuple(row))\n",
    "print(f\"Trials to reject due to blink on main stimulus: {len(blink_on_stim_trials)}\")\n",
    "\n",
    "# Reject trials above the interpolation allowed threshold (if a trial contains more than x% interpolated data, reject it):\n",
    "high_interp_trials = cleaned_df[cleaned_df[\"Interpolation PC\"] >= config.INTERPOLATION_PC_THRESHOLD][trial_identifier_cols].drop_duplicates()\n",
    "for _, row in high_interp_trials.iterrows():\n",
    "    trials_to_reject_set.add(tuple(row))\n",
    "print(f\"Trials to reject due to high interpolation ({config.INTERPOLATION_PC_THRESHOLD}%+): {len(high_interp_trials)}\")\n",
    "\n",
    "if not DECONVOLUTION_OUTPUT:\n",
    "    produce_pre_processed_csv(\n",
    "        analysis_df=cleaned_df,\n",
    "        rejected_trials=trials_to_reject_set, filepath=f\"data/fully_preprocessed_data/{participant_id}/{run}_least_strict.csv\"\n",
    "    )\n",
    "else:\n",
    "    produce_pre_processed_csv(\n",
    "        analysis_df=cleaned_df,\n",
    "        rejected_trials=trials_to_reject_set, filepath=f\"data/fully_preprocessed_data/{participant_id}/{run}_deconvolution_input.csv\"\n",
    "    )\n",
    "    test_messages_df.to_csv(f\"data/fully_preprocessed_data/{participant_id}/{run}_messages.csv\")"
   ],
   "id": "212d39aa83cd0237",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not DECONVOLUTION_OUTPUT:\n",
    "    # Next, we'll handle the rejection criteria that is probably a good idea to add, but not strictly necessary:\n",
    "\n",
    "    # Reject trials if there was a saccade during the main stimulus presentation period:\n",
    "    saccade_on_stim_trials = cleaned_df[cleaned_df[\"Saccade On Main Stim\"] == True][trial_identifier_cols].drop_duplicates()\n",
    "    for _, row in saccade_on_stim_trials.iterrows():\n",
    "        trials_to_reject_set.add(tuple(row))\n",
    "    print(f\"Trials to reject due to saccade on main stimulus: {len(saccade_on_stim_trials)}\")\n",
    "\n",
    "    # Reject trials where a target appeared before or during the epoch of interest:\n",
    "    samples_in_epoch_df = cleaned_df[(cleaned_df[\"Is In Epoch\"] == True) | (cleaned_df[\"Is Before Epoch\"] == True)]\n",
    "    target_in_epoch_trials_df = samples_in_epoch_df[samples_in_epoch_df[\"Target Status\"] == True][trial_identifier_cols].drop_duplicates()\n",
    "    for _, row in target_in_epoch_trials_df.iterrows():\n",
    "        trials_to_reject_set.add(tuple(row))\n",
    "    print(f\"Trials to reject due to a target appearing before or within the epoch ({config.EPOCH_START}-{config.EPOCH_END}ms from onset): {len(target_in_epoch_trials_df)}\")\n",
    "\n",
    "    produce_pre_processed_csv(analysis_df=cleaned_df, rejected_trials=trials_to_reject_set, filepath=f\"data/fully_preprocessed_data/{participant_id}/{run}_mid_strict.csv\")"
   ],
   "id": "81297099e7ac8a92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not DECONVOLUTION_OUTPUT:\n",
    "    # Finally, we'll reject trials based on our most strictest requirements - things that can be argued are not necessary:\n",
    "\n",
    "    # Reject trials where a blink has occurred before or during the epoch of interest:\n",
    "    blink_in_epoch_trials_df = samples_in_epoch_df[samples_in_epoch_df[\"Blink\"] == True][trial_identifier_cols].drop_duplicates()\n",
    "    for _, row in blink_in_epoch_trials_df.iterrows():\n",
    "        trials_to_reject_set.add(tuple(row))\n",
    "    print(f\"Trials to reject due to any blink occurring before or within the epoch ({config.EPOCH_START}-{config.EPOCH_END}ms from onset): {len(blink_in_epoch_trials_df)}\")\n",
    "\n",
    "    # Reject trials where a saccade has occurred before or during the epoch of interest:\n",
    "    saccade_in_epoch_trials_df = samples_in_epoch_df[samples_in_epoch_df[\"Saccade\"] == True][trial_identifier_cols].drop_duplicates()\n",
    "    for _, row in saccade_in_epoch_trials_df.iterrows():\n",
    "        trials_to_reject_set.add(tuple(row))\n",
    "    print(f\"Trials to reject due to any saccade occurring before or within the epoch ({config.EPOCH_START}-{config.EPOCH_END}ms from onset): {len(saccade_in_epoch_trials_df)}\")\n",
    "\n",
    "    produce_pre_processed_csv(analysis_df=cleaned_df, rejected_trials=trials_to_reject_set, filepath=f\"data/fully_preprocessed_data/{participant_id}/{run}_most_strict.csv\")"
   ],
   "id": "7dafc45069ba395d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "cleaned_df.columns",
   "id": "80e20b0eb077e093",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
